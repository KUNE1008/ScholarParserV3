{
    "title": "intrinsicnerf: learning intrinsic neural radiance fields for editable novel view synthesis",
    "id": 5,
    "valid_pdf_number": "21/24",
    "matched_pdf_number": "12/21",
    "matched_rate": 0.5714285714285714,
    "citations": {
        "Palettenerf: Palette-based appearance editing of neural radiance fields": {
            "authors": [
                "Zhengfei Kuang",
                "Fujun Luan",
                "Sai Bi",
                "Zhixin Shu",
                "Gordon Wetzstein",
                "Kalyan Sunkavalli"
            ],
            "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Kuang_PaletteNeRF_Palette-Based_Appearance_Editing_of_Neural_Radiance_Fields_CVPR_2023_paper.pdf",
            "ref_texts": "[38] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. arXiv preprint arXiv:2210.00647, 2022. 2[39] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. CoRR, abs/2112.05131, 2021. 1,2",
            "ref_ids": [
                "38",
                "39"
            ],
            "1": "Introduction Neural Radiance Fields (NeRF) [23] and its variants [8, 25, 27, 39] have received increasing attention in recent years for their ability to robustly reconstruct real-world 3D scenes from 2D images and enable high-quality, photorealistic novel view synthesis.",
            "2": "Many recent works [8, 25,36,39,44] propose to speed up the training and improve the performance of the models by applying a combination of light-weight MLPs and neural feature maps or volumes.",
            "3": "[38] introduces a NeRF-based intrinsic decomposition model which enables 3D intuitive recoloring, but it does not support palette-based editing.",
            "4": "2[39] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa."
        },
        "PVO: Panoptic visual odometry": {
            "authors": [
                "Weicai Ye",
                "Xinyue Lan",
                "Shuo Chen",
                "Yuhang Ming",
                "Xingyuan Yu",
                "Hujun Bao",
                "Zhaopeng Cui",
                "Guofeng Zhang"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Ye_PVO_Panoptic_Visual_Odometry_CVPR_2023_paper.pdf",
            "ref_texts": ""
        },
        "Nero: Neural geometry and brdf reconstruction of reflective objects from multiview images": {
            "authors": [
                "Y Liu",
                "P Wang",
                "C Lin",
                "X Long",
                "J Wang",
                "L Liu"
            ],
            "url": "https://arxiv.org/pdf/2305.17398",
            "ref_texts": "2022b. S3-NeRF: Neural Reflectance Field from Shading and Shadow under a Single Viewpoint. In NeurIPS . Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. 2018. MVSNet: Depth inference for unstructured multi-view stereo. In ECCV . Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. 2022. NeILF: Neural incident light field for physically-based material estimation. In ECCV . Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. 2021. Volume rendering of neural implicit surfaces. In NeurIPS . Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. 2020. Multiview neural surface reconstruction by disentangling geometry and appearance. In NeurIPS . Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. 2022. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. arXiv preprint arXiv:2210.00647 (2022). Ye Yu, Abhimitra Meka, Mohamed Elgharib, Hans-Peter Seidel, Christian Theobalt, and William AP Smith. 2020. Self-supervised outdoor scene relighting. In ECCV . Ye Yu and William AP Smith. 2019. Inverserendernet: Learning single image inverse rendering. In CVPR . Jason Zhang, Gengshan Yang, Shubham Tulsiani, and Deva Ramanan. 2021c. NeRS: Neural reflectance surfaces for sparse-view 3d reconstruction in the wild. In NeurIPS . Kai Zhang, Fujun Luan, Zhengqi Li, and Noah Snavely. 2022a. IRON: Inverse Rendering by Optimizing Neural SDFs and Materials from Photometric Images. In CVPR . Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. 2021a. PhySG: Inverse rendering with spherical gaussians for physics-based material editing and relighting. In CVPR . Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. 2020. Nerf++: Analyzing and improving neural radiance fields. arXiv preprint arXiv:2010.07492 (2020). Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR . Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul Debevec, William T Freeman, and Jonathan T Barron. 2021b. NeRFactor: Neural factorization of shape and reflectance under an unknown illumination. In SIGGRAPH . Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, and Xiaowei Zhou."
        },
        "Clean-NeRF: Reformulating NeRF to account for View-Dependent Observations": {
            "authors": [
                "X Liu",
                "YW Tai",
                "CK Tang"
            ],
            "url": "https://arxiv.org/pdf/2303.14707",
            "ref_texts": "[66] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. arXiv preprint arXiv:2210.00647 , 2022. 3",
            "ref_ids": [
                "66"
            ],
            "1": "IntrinsicNeRF [66] introduces intrinsic decomposition to the NeRF-based neural rendering method, which allows for editable novel view synthesis in room-scale scenes."
        },
        "PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF": {
            "authors": [
                "Y Feng",
                "Y Shang",
                "X Li",
                "T Shao",
                "C Jiang"
            ],
            "url": "https://arxiv.org/pdf/2311.13099",
            "ref_texts": "[84] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 339\u2013351, 2023. 2",
            "ref_ids": [
                "84"
            ],
            "1": "These include semantic-driven editing [3, 13, 24, 45, 66, 73], shading-driven adjustments (like relighting and texturing) [21, 43, 64, 68, 78, 84], scene modifications (such as object addition or removal) [35, 36, 76, 83, 90], face editing [27, 31, 70, 89], physics based editing from video[25, 62], and multi-purpose editing [30, 75, 82]."
        },
        "GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image": {
            "authors": [
                "C Bao",
                "Y Zhang",
                "Y Li",
                "X Zhang",
                "B Yang",
                "H Bao"
            ],
            "url": "https://arxiv.org/pdf/2404.02152",
            "ref_texts": "[63] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 339\u2013351, 2023. 2",
            "ref_ids": [
                "63"
            ],
            "1": "Neural Radiance Field [33] has exhibited great reconstruction and rendering qualities in SLAM [62, 73], scene editing [5, 58\u201360, 64] and relighting [63, 66, 67], especially promoting the emergence of many 3D avatar reconstruction [4, 16, 53, 68, 69, 76] and generation [50, 52, 54]."
        },
        "Semantically-aware Neural Radiance Fields for Visual Scene Understanding: A Comprehensive Review": {
            "authors": [
                "TAQ Nguyen",
                "A Bourki",
                "M Macudzinski"
            ],
            "url": "https://arxiv.org/pdf/2402.11141",
            "ref_texts": "[277] Jingbo Zhang et al. \u201cFdnerf: Few-shot dynamic neural radiance fields for face reconstruction and expression editing\u201d. In: SIGGRAPH Asia 2022 Conference Papers . 2022, pp. 1\u20139.",
            "ref_ids": [
                "277"
            ],
            "1": "Users can manipulate facial attributes effectively by providing simple expression codes [47, 277], mask 16\n(a)\n (b) Fig.",
            "2": "[277] Jingbo Zhang et al."
        },
        "Few-Shot Neural Radiance Fields under Unconstrained Illumination": {
            "authors": [
                "Yeong Lee",
                "Yong Choi",
                "Seungryong Kim",
                "Jae Kim",
                "Junghyun Cho"
            ],
            "url": "https://ojs.aaai.org/index.php/AAAI/article/view/28075/28156",
            "ref_texts": "12901. Rudnev, V.; Elgharib, M.; Smith, W.; Liu, L.; Golyanik, V.; and Theobalt, C. 2022. Nerf for outdoor scene relighting. In ECCV, 615\u2013631. Springer. Sch\u00a8onberger, J. L.; Zheng, E.; Pollefeys, M.; and Frahm, J.M. 2016. Pixelwise View Selection for Unstructured MultiView Stereo. In ECCV. Sitzmann, V.; Zollh \u00a8ofer, M.; and Wetzstein, G. 2019. Scene representation networks: Continuous 3d-structureaware neural scene representations. NeurIPS, 32. Snavely, N.; Seitz, S. M.; and Szeliski, R. 2006. Photo tourism: exploring photo collections in 3D. In ACM SIGGRAPH, 835\u2013846. Toschi, M.; De Matteo, R.; Spezialetti, R.; De Gregorio, D.; Di Stefano, L.; and Salti, S. 2023. ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects. In CVPR, 20762\u201320772. Wang, C.; Chai, M.; He, M.; Chen, D.; and Liao, J. 2022. CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields. In CVPR, 3835\u20133844. Wang, Q.; Wang, Z.; Genova, K.; Srinivasan, P. P.; Zhou, H.; Barron, J. T.; Martin-Brualla, R.; Snavely, N.; and Funkhouser, T. 2021. Ibrnet: Learning multi-view imagebased rendering. In CVPR, 4690\u20134699. Watson, D.; Chan, W.; Martin-Brualla, R.; Ho, J.; Tagliasacchi, A.; and Norouzi, M. 2022. Novel view synthesis with diffusion models. arXiv preprint arXiv:2210.04628. Wynn, J.; and Turmukhambetov, D. 2023. Diffusionerf: Regularizing neural radiance fields with denoising diffusion models. In CVPR, 4180\u20134189. Xu, D.; Jiang, Y.; Wang, P.; Fan, Z.; Shi, H.; and Wang, Z. 2022. SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single Image. arXiv preprint arXiv:2204.00928. Yang, J.; Pavone, M.; and Wang, Y. 2023. FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization. In CVPR, 8254\u20138263. Yang, S.; Cui, X.; Zhu, Y.; Tang, J.; Li, S.; Yu, Z.; and Shi, B. 2023. Complementary Intrinsics From Neural Radiance Fields and CNNs for Outdoor Scene Relighting. In CVPR, 16600\u201316609. Ye, W.; Chen, S.; Bao, C.; Bao, H.; Pollefeys, M.; Cui, Z.; and Zhang, G. 2022. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. arXiv preprint arXiv:2210.00647.Yu, A.; Ye, V.; Tancik, M.; and Kanazawa, A. 2021. pixelnerf: Neural radiance fields from one or few images. In CVPR, 4578\u20134587. Yuan, Y.-J.; Lai, Y.-K.; Huang, Y.-H.; Kobbelt, L.; and Gao, L. 2022a. Neural Radiance Fields from Sparse RGB-D Images for High-Quality View Synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1\u201316. Yuan, Y.-J.; Sun, Y.-T.; Lai, Y.-K.; Ma, Y.; Jia, R.; and Gao, L. 2022b. Nerf-editing: geometry editing of neural radiance fields. In CVPR, 18353\u201318364. The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)",
            "ref_ids": [
                "12901"
            ]
        },
        "SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild": {
            "authors": [
                "A Engelhardt",
                "A Raj",
                "M Boss",
                "Y Zhang",
                "A Kar"
            ],
            "url": "https://arxiv.org/pdf/2401.10171",
            "ref_texts": "[15] Yue Chen, Xingyu Chen, Xuan Wang, Qi Zhang, Yu Guo, Ying Shan, and Fei Wang. Local-to-global registration forbundle-adjusting neural radiance fields. CVPR , pages 8264\u2013",
            "ref_ids": [
                "15"
            ],
            "1": "Other recent methods rely on rough initialization of the camera, global alignment, or a template shape for joint optimization [15,44,77,84].",
            "2": "2, 3, 4, 5, 6, 7, 12, 13, 14, 15, 16\n[15] Yue Chen, Xingyu Chen, Xuan Wang, Qi Zhang, Yu Guo, Ying Shan, and Fei Wang."
        },
        "NeRF: Multi-Modal Decomposition NeRF with 3D Feature Fields": {
            "authors": [
                "N Wang",
                "L Zhang",
                "AX Chang"
            ],
            "url": "https://arxiv.org/pdf/2405.05010",
            "ref_texts": "59. Ye, W., Chen, S., Bao, C., Bao, H., Pollefeys, M., Cui, Z., Zhang, G.: Intrinsicnerf: Learningintrinsicneuralradiancefieldsforeditablenovelviewsynthesis.In:ICCV. pp. 339\u2013351 (2023) 4",
            "ref_ids": [
                "59"
            ],
            "1": "Neural rendering has spurred an exploration into implicit and hybrid representations, offering various approaches for 3D editing, such as changing global appearance [7,29], intrinsic decomposition [59,64], per-object decomposition [54, 56], geometry and texture editing [1,55,61], 3D inpainting [36,52], and others [20,39,50]."
        },
        "NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth Supervision for Indoor Multi-View 3D Detection": {
            "authors": [
                "C Huang",
                "Y Hou",
                "W Ye",
                "D Huang",
                "X Huang"
            ],
            "url": "https://arxiv.org/pdf/2402.14464",
            "ref_texts": "[Yeet al. , 2023 ]Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 2023.",
            "ref_ids": [
                "Yeet al\\. , 2023 "
            ]
        },
        "Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation": {
            "authors": [
                "Y Wang",
                "A Gao",
                "Y Gong",
                "Y Zeng"
            ],
            "url": "https://arxiv.org/pdf/2404.05236"
        },
        "ExtremeNeRF: Few-shot Neural Radiance Fields Under Unconstrained Illumination": {
            "authors": [
                "SY Lee",
                "JY Choi",
                "S Kim",
                "IJ Kim",
                "J Cho"
            ],
            "url": "https://arxiv.org/pdf/2303.11728",
            "ref_texts": "12901. Rudnev, V .; Elgharib, M.; Smith, W.; Liu, L.; Golyanik, V .; and Theobalt, C. 2022. Nerf for outdoor scene relighting. In ECCV , 615\u2013631. Springer. Sch\u00a8onberger, J. L.; Zheng, E.; Pollefeys, M.; and Frahm, J.M. 2016. Pixelwise View Selection for Unstructured MultiView Stereo. In ECCV . Sitzmann, V .; Zollh \u00a8ofer, M.; and Wetzstein, G. 2019. Scene representation networks: Continuous 3d-structureaware neural scene representations. NeurIPS , 32. Snavely, N.; Seitz, S. M.; and Szeliski, R. 2006. Photo tourism: exploring photo collections in 3D. In ACM SIGGRAPH , 835\u2013846. Toschi, M.; De Matteo, R.; Spezialetti, R.; De Gregorio, D.; Di Stefano, L.; and Salti, S. 2023. ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects. In CVPR , 20762\u201320772. Wang, C.; Chai, M.; He, M.; Chen, D.; and Liao, J. 2022. CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields. In CVPR , 3835\u20133844. Wang, Q.; Wang, Z.; Genova, K.; Srinivasan, P. P.; Zhou, H.; Barron, J. T.; Martin-Brualla, R.; Snavely, N.; and Funkhouser, T. 2021. Ibrnet: Learning multi-view imagebased rendering. In CVPR , 4690\u20134699. Watson, D.; Chan, W.; Martin-Brualla, R.; Ho, J.; Tagliasacchi, A.; and Norouzi, M. 2022. Novel view synthesis with diffusion models. arXiv preprint arXiv:2210.04628 . Wynn, J.; and Turmukhambetov, D. 2023. Diffusionerf: Regularizing neural radiance fields with denoising diffusion models. In CVPR , 4180\u20134189. Xu, D.; Jiang, Y .; Wang, P.; Fan, Z.; Shi, H.; and Wang, Z. 2022. SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single Image. arXiv preprint arXiv:2204.00928 .Yang, J.; Pavone, M.; and Wang, Y . 2023. FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization. In CVPR , 8254\u20138263. Yang, S.; Cui, X.; Zhu, Y .; Tang, J.; Li, S.; Yu, Z.; and Shi, B. 2023. Complementary Intrinsics From Neural Radiance Fields and CNNs for Outdoor Scene Relighting. In CVPR , 16600\u201316609. Ye, W.; Chen, S.; Bao, C.; Bao, H.; Pollefeys, M.; Cui, Z.; and Zhang, G. 2022. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. arXiv preprint arXiv:2210.00647 . Yu, A.; Ye, V .; Tancik, M.; and Kanazawa, A. 2021. pixelnerf: Neural radiance fields from one or few images. In CVPR , 4578\u20134587. Yuan, Y .-J.; Lai, Y .-K.; Huang, Y .-H.; Kobbelt, L.; and Gao, L. 2022a. Neural Radiance Fields from Sparse RGB-D Images for High-Quality View Synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence , 1\u201316. Yuan, Y .-J.; Sun, Y .-T.; Lai, Y .-K.; Ma, Y .; Jia, R.; and Gao, L. 2022b. Nerf-editing: geometry editing of neural radiance fields. In CVPR , 18353\u201318364. Supplementary Material In this section, we provide further details on the experiments and datasets, followed by additional ablation studies and experimental results. More Details on Experiments Experimental Details Comparisons on Phototourism F3.A subset for fewshot view synthesis was created by selecting 3 input views with similar depth bounds and frontal-facing poses. The image IDs are (185, 45, 1066), (964, 34, 478), and (82, 312, 803) for \u2018Brandenburg Gate\u2019, \u2018Sacre Coeur\u2019, and \u2018Trevi Fountain\u2019, respectively. Fig. 7 shows image samples of the dataset. Each scene has about eight test images to evaluate the performance. For the depth map comparison using Abs Rel, the original ground truth depth maps provided by Phototourism (Snavely, Seitz, and Szeliski 2006) are very noisy. Following the publicly available instructions of the dataset, we used the clean versions of the depth maps with the background masks. As a result, reported Abs Rel excludes background regions for the evaluation. More detailed information about the proposed datasets can be found in the next section. Comparisons on NeRF Extreme. For evaluating the fewshot view synthesis performance on NeRF Extreme, image IDs (0, 14, 29) were used as inputs for each scene. Given that our proposed multi-view consistency takes into account complex scene geometry, including occlusions, the optimal model and parameters might vary based on the specific scene characteristics. However, experimental results in Tab. 4of the paper were based on our final model. Ablation studies. In this paragraph, we provide detailed information about the ablation studies, especially for ablation 1-3, with albedo MLP. Ablations other than 1-3, were conducted with straightforward implementation with and without proposed consistency regularization. In the case of 1-3, we implemented the multi-view albedo consistency by directly synthesizing the albedo map using the Multi-Layered Perceptron (MLP) of NeRF. Given the viewindependent nature of albedo, we configured the MLP to output albedo as density. By doing so, the proposed framework may be free from the computational costs that come from continuous patch-wise sampling. However, as shown in Tab. 5 in the paper, the model with albedo MLP shows sub-optimal results, which has almost the same results as the model without albedo consistency (1-1). Implementation Details Neural radiance fields. Our framework is based on JAX (Bradbury et al. 2018) implementation of RegNeRF (Niemeyer et al. 2022), while partially adopting a frequency regularization mask of FreeNeRF (Yang, Pavone, and Wang 2023) when constructing MLP. Detailed algorithms of the proposed loss functions are provided in Alg. 1 and 2. Figure 7: Examples of inputs sampled from Phototourism F3.Sampled frontal-facing scenes with varying illumination from the \u2018Brandenburg Gate\u2019, \u2018Sacre Coeur\u2019, and \u2018Trevi Fountain\u2019, respectively. Intrinsic decomposition network. Building upon the concept of integrating an offline intrinsic decomposition network, our PIDNet adopts the architecture of the chosen FIDNet. Given that our ultimate model employs IIDWW (Li and Snavely 2018) as the FIDNet to provide pseudo-albedo ground truth, our PIDNet shares a similar architecture with IIDWW, albeit in a shallower configuration. However, it\u2019s worth highlighting that any intrinsic decomposition network demonstrating superior performance can substitute IIDWW as the FIDNet with a paired PIDNet that has a similar, and shallower architecture. Additional Losses In addition to the losses suggested in the main paper, we incorporates several losses to better optimize the PIDNet as described below. For Lcolor andLds, they were part of the baseline and showed a performance decrease upon removal. Edge-preserving loss. Motivated by (Godard, Mac Aodha, and Brostow 2017), we used the gradient-based edge-preserving loss, to enforce the input and the novel view patches to preserve geometric properties. Using a weight term, \u03c9(x), which already has been discussed in the paper, our edge-preserving loss on the predicted albedo can be formulated as: Ledge=X x\u2032\u2208P\u2032\u03c9(x)\u2225\u2202(\u02c6a(x)\u2212\u02c6a(x\u2032))\u22252,(12) where \u2202denotes the partial derivatives of the vertical and the horizontal directions, and P\u2032denotes all the pixels in the target image. Chromaticity consistency loss. Similar to (Ye et al.",
            "ref_ids": [
                "12901"
            ]
        },
        "DreamMat: High-quality PBR Material Generation with Geometry-and Light-aware Diffusion Models": {
            "authors": [
                "Y Zhang",
                "Y Liu",
                "Z Xie",
                "L Yang",
                "Z Liu",
                "M Yang"
            ],
            "url": "https://arxiv.org/pdf/2405.17176",
            "ref_texts": "(2023). Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. 2022. Neilf: Neural incident light field for physically-based material estimation. In ECCV . Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. 2020. Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance. In NeurIPS . Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. 2023. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. In ICCV . Jounathan Young. 2021. xatlas. https://github.com/jpcy/xatlas.git Kim Youwang, Tae-Hyun Oh, and Gerard Pons-Moll. 2023. Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering. arXiv preprint arXiv:2312.11360 (2023). Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, and Xiaojuan Qi. 2023a. Texture Generation on 3D Meshes with Point-UV Diffusion. In ICCV . Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-Hai Zhang, and Xiaojuan Qi."
        },
        "Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo": {
            "authors": [
                "Z Li",
                "Z Lu",
                "H Yan",
                "B Shi",
                "G Pan",
                "Q Zheng"
            ],
            "url": "https://arxiv.org/pdf/2404.01612",
            "ref_texts": "[30] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. IntrinsicNeRF: Learning intrinsic neural radiance fields for editable novel view synthesis. Proc. International Conference on Computer Vision (ICCV) , 2023. 5",
            "ref_ids": [
                "30"
            ],
            "1": "Similar to [30], the normalized color loss calculated as \u2225Nor(A)\u2212Nor(I)\u2225is implemented to help Spin-UP learn a better albedo representation, where Nor(."
        },
        "Neural Implicit Field Editing Considering Object-environment Interaction": {
            "authors": [
                "Z Zeng",
                "Z Wang",
                "Y Zhang",
                "W Cai",
                "Z Cao"
            ],
            "url": "https://arxiv.org/pdf/2311.00425",
            "ref_texts": "[36] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. 2023. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 339\u2013351.",
            "ref_ids": [
                "36"
            ],
            "1": "Intrinsic NeRF [36] performs unsupervised clustering of image color labels in training process to reconstruct the albedo and shading scene colors.",
            "2": "[36].",
            "3": "To verify the effectiveness of the intrinsic decomposition method we used, we compared the albedo images obtained by Intrinsic NeRF [36] with our own results, which showed our separation in shadow areas were superior to existing methods.",
            "4": "The main method Intrinsic NeRF [36], which did not release code and datasets, mentioned the reference of preprocessed Replica indoor scene dataset provided by Semantic NeRF [40].",
            "5": "Intrinsic NeRF [36] is evaluated their albedo images according to the similarity of ground truth provided by earlier methods such as PhySG [37].",
            "6": "Replica-room_0 Config PSNR \u2193SSIM\u2191LPIPS\u2193 Intrinsic NeRF [36] 30."
        },
        "Neural Radiance Field-based Visual Rendering: A Comprehensive Review": {
            "authors": [
                "M Yao",
                "Y Huo",
                "Y Ran",
                "Q Tian",
                "R Wang"
            ],
            "url": "https://arxiv.org/pdf/2404.00714",
            "ref_texts": "[111] W. Ye, S. Chen, C. Bao, H. Bao, M. Pollefeys, Z. Cui, and G. Zhang, \u201cIntrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision , 2023, pp. 339\u2013351.",
            "ref_ids": [
                "111"
            ],
            "1": "Others SparseNeRF [110](depth prior), NeRF-Det [111](end-to-end detection), IntrinsicNeRF [112](Unsupervised), etc.",
            "2": "20 IntrinsicNeRF [111] (2023) possesses the capability to break down a static scene\u2019s multi-view image into consistent elements like reflectance, shading, and residual layers, achieved through intrinsic decomposition in a NeRF-based neural rendering technique.",
            "3": "[111] W."
        },
        "3D Scene Creation and Rendering via Rough Meshes: A Lighting Transfer Avenue": {
            "authors": [
                "Y Li",
                "B Cai",
                "Y Liang",
                "R Jia",
                "B Zhao",
                "M Gong"
            ],
            "url": "https://arxiv.org/pdf/2211.14823",
            "ref_texts": "[54] W. Ye, S. Chen, C. Bao, H. Bao, M. Pollefeys, Z. Cui, and G. Zhang, \u201cIntrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis,\u201d arXiv preprint arXiv:2210.00647 , 2022.",
            "ref_ids": [
                "54"
            ],
            "1": "There are several works [53], [54], [55] that have also exploited free scene lighting editing.",
            "2": "[54] W."
        },
        "Light Source Estimation via Intrinsic Decomposition for Novel View Synthesis": {
            "authors": [
                "DS Tetruashvili"
            ],
            "url": "https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/645817/Tetruashvili_David.pdf?sequence=1",
            "ref_texts": ""
        },
        "Clean-NeRF: Defogging using Ray Statistics Prior in Natural NeRFs": {
            "authors": [
                "X Liu",
                "YW Tai",
                "CK Tang"
            ],
            "url": "https://openreview.net/pdf?id=YHqEWF5gt8",
            "ref_texts": "612, 2004. Frederik Warburg, Ethan Weber, Matthew Tancik, Aleksander Ho\u0142y \u00b4nski, and Angjoo Kanazawa. Nerfbusters: Removing ghostly artifacts from casually captured nerfs. 2023. Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, and Cengiz Oztireli. D\u02c6 2nerf: Self-supervised decoupling of dynamic and static objects from a monocular video. Advances in Neural Information Processing Systems (NeurIPS) , 35:32653\u201332666, 2022. Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. In IEEE/CVF International Conference on Computer Vision (ICCV) , 2023. Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for realtime rendering of neural radiance fields. In IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 5752\u20135761, 2021. Ye Yu and William AP Smith. Inverserendernet: Learning single image inverse rendering. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 3155\u20133164, 2019. Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu, Yingliang Zhang, Lan Xu, and Jingyi Yu. Editable free-viewpoint video using a layered neural representation. ACM Transactions on Graphics (TOG) , 40(4):1\u201318, 2021a. Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. Physg: Inverse rendering with spherical gaussians for physics-based material editing and relighting. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 5453\u20135462, 2021b. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 586\u2013595, 2018. Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang Zhang, Jingyi Yu, and Lan Xu. Humannerf: Efficiently generated human radiance field from sparse inputs. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 7743\u20137753, 2022. Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and Andrew J Davison. In-place scene labelling and understanding with implicit scene representation. In IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 15838\u201315847, 2021. Rui Zhu, Zhengqin Li, Janarbek Matai, Fatih Porikli, and Manmohan Chandraker. Irisformer: Dense vision transformers for single-image inverse rendering in indoor scenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 2822\u20132831, 2022."
        },
        "Learning Relighting and Intrinsic Decomposition in Neural Radiance Fields": {
            "authors": [
                "Y Yang",
                "S Hu",
                "H Wu",
                "R Baldrich",
                "D Samaras",
                "M Vanrell"
            ],
            "url": "https://neural-rendering.com/papers/22.pdf",
            "ref_texts": "[37] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. In Proceedings oftheIEEE/CVF International Conference onComputer Vision, 2023. 1, 2, 3, 4",
            "ref_ids": [
                "37"
            ],
            "1": "Concurrently, there has been an exploration towards scene editing [35], such as recoloring [37] and relighting [21, 38].",
            "2": "The second approach [37], based on intrinsic decomposition[2], aims to provide an interpretable representation of a scene (in terms of reflectance and shading) suitable for image editing.",
            "3": "While IntrinsicNeRF [37] has pioneered the integration of intrinsic decomposition within NeRF, they have not utilized relighting or fully leveraged the 3D information available through neural rendering.",
            "4": "IntrinsicNeRF [37] has been a pioneer in applying intrinsic decomposition to neural rendering.",
            "5": "However, real-world scenes often require a residual term to account for discrepancies [11, 37].",
            "6": "As demonstrated in [37], the diffuse components dominate the scene, so it is crucial to prevent the training from converging to undesirable local minima (R= 0, S=\n0, Re=I).",
            "7": "1194 IntrinsicNeRF*[37] 25.",
            "8": "[4]) and the state-of-the-art neural rendering approach (IntrinsicNeRF [37]).",
            "9": "The other neural rendering method, IntrinsicNeRF [37], also fails to achieve correct decomposition, primarily attributed to the failure in distinguishing intrinsic components and also the difficulty in scene reconstruction."
        }
    }
}