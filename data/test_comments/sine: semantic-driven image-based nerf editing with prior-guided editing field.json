{
    "title": "sine: semantic-driven image-based nerf editing with prior-guided editing field",
    "id": 6,
    "valid_pdf_number": "56/60",
    "matched_pdf_number": "38/56",
    "matched_rate": 0.6785714285714286,
    "citations": {
        "Shap-e: Generating conditional 3d implicit functions": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.02463",
            "ref_texts": "[3]Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. arXiv:2303.13277 , 2023.",
            "ref_ids": [
                "3"
            ],
            "1": "Since they are end-to-end differentiable, INRs also enable various downstream applications such as style transfer [72] and differentiable shape editing [3]."
        },
        "Dreameditor: Text-driven 3d scene editing with neural fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2306.13455",
            "ref_texts": "Omri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for textdriven editing of natural images. In CVPR 2022 . 18208\u201318218. Chong Bao, Yinda Zhang, and Bangbang et al. Yang. 2023. Sine: Semantic-driven imagebased nerf editing with prior-guided editing field. In CVPR 2023 . 20919\u201320929. Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2022. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800 (2022). Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Xu Jia, and Huchuan Lu. 2021. Animatable neural radiance fields from monocular rgb videos. arXiv preprint arXiv:2106.13629 (2021). Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023. Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. arXiv preprint arXiv:2303.13873 (2023). Yongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, and Kui Jia. 2022. Tango: Textdriven photorealistic and robust 3d stylization via lighting decomposition. arXiv preprint arXiv:2210.11277 (2022). Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. 2022. Diffedit: Diffusion-based semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427 (2022). Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2022a. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022). Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2022b. StyleGAN-NADA: CLIP-guided domain adaptation of image generators. ACM Transactions on Graphics (TOG) 41, 4 (2022), 1\u201313. William Gao, Noam Aigerman, Thibault Groueix, Vladimir G Kim, and Rana Hanocka."
        },
        "Multimodal image synthesis and editing: A survey": {
            "authors": [],
            "url": "https://pure.mpg.de/rest/items/item_3487306/component/file_3487307/content",
            "ref_texts": ""
        },
        "Blended-nerf: Zero-shot object generation and blending in existing neural radiance fields": {
            "authors": [
                "Ori Gordon",
                "Omri Avrahami",
                "Dani Lischinski"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2023W/AI3DCC/papers/Gordon_Blended-NeRF_Zero-Shot_Object_Generation_and_Blending_in_Existing_Neural_Radiance_ICCVW_2023_paper.pdf",
            "ref_texts": ""
        },
        "Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis": {
            "authors": [
                "Weicai Ye",
                "Shuo Chen",
                "Chong Bao",
                "Hujun Bao",
                "Marc Pollefeys",
                "Zhaopeng Cui",
                "Guofeng Zhang"
            ],
            "url": "https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_IntrinsicNeRF_Learning_Intrinsic_Neural_Radiance_Fields_for_Editable_Novel_View_ICCV_2023_paper.pdf",
            "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field. In Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference , 2023.",
            "ref_ids": [
                "1"
            ],
            "1": "Given the high degree of integration of our approach with NeRF, NeRF extensions can be seamlessly incorporated into our IntrinsicNeRF, such as NeRF in the wild [12, 46, 59], NeRF in dynamic environments [33, 51, 52, 69], fast NeRF [48, 18, 10, 71], NeRF with generalization [11, 64, 72, 27], generative NeRF [55, 62], NeRF with panoptic segmentation [26, 68], NeRFbased SLAM [47, 58, 82], Geometry and Texture Editing with NeRF [1, 14] etc, which will be helpful to the comOriginal Original Recoloring RecoloringFigure 11: Recoloring on Synthetic/Real-World Data."
        },
        "Gaussianeditor: Editing 3d gaussians delicately with text instructions": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2311.16037",
            "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023. 3",
            "ref_ids": [
                "1"
            ],
            "1": "3D Scene Editing of Radiance Fields 3D Scene Editing of Radiance Fields has gained significant popularity as a recent research direction [1, 10, 15, 20, 22\u201325, 28, 34, 47, 48, 52\u201354]."
        },
        "Or-nerf: Object removing from 3d scenes guided by multiview segmentation with neural radiance fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2305.10503",
            "ref_texts": "[30] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field,\u201d 2023. [Online]. Available: http: //arxiv.org/abs/2303.13277",
            "ref_ids": [
                "30",
                "Online"
            ],
            "1": "Scene Object Removal NeRF has greatly facilitated the area of 3D scene editing and research [30], [31], [32], [33] focuses on various editing typesemerging in large numbers.",
            "2": "[Online].",
            "3": "[Online].",
            "4": "[Online].",
            "5": "[Online].",
            "6": "[Online].",
            "7": "[Online].",
            "8": "[Online].",
            "9": "[Online].",
            "10": "[Online].",
            "11": "[Online].",
            "12": "[Online].",
            "13": "[Online].",
            "14": "[Online].",
            "15": "[Online].",
            "16": "[Online].",
            "17": "[Online].",
            "18": "15224\n[30] C.",
            "19": "[Online].",
            "20": "[Online].",
            "21": "[Online].",
            "22": "[Online].",
            "23": "[Online].",
            "24": "[Online].",
            "25": "[Online].",
            "26": "[Online].",
            "27": "[Online].",
            "28": "[Online].",
            "29": "[Online].",
            "30": "[Online].",
            "31": "[Online].",
            "32": "[Online].",
            "33": "[Online].",
            "34": "[Online].",
            "35": "[Online].",
            "36": "[Online].",
            "37": "[Online].",
            "38": "[Online]."
        },
        "Gaussianeditor: Swift and controllable 3d editing with gaussian splatting": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2311.14521",
            "ref_texts": "[1]Chong Bao, Yinda Zhang, and Bangbang et al. Yang. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In CVPR 2023 , pages 20919\u201320929, 2023. 3",
            "ref_ids": [
                "1"
            ],
            "1": "Additionally, some works [1,10,45,46] leverage CLIP models to facilitate editing through the use of text prompts or reference images.",
            "2": "[1]Chong Bao, Yinda Zhang, and Bangbang et al.",
            "3": "Specifically, when the user clicks a point on the screen, we back-project this point into a spatial point based on the intrinsic and extrinsic parameters of the current viewpoint camera: [x, y, z ]T= [R|t]z(p)K\u22121[px, py,1]T, (9) where [R|t]andKdenote the extrinsic and intrinsic of the current camera, p,z(p)and[x, y, z ]Trefer to the userclicked pixel, its corresponding depth, and the spatial point, respectively."
        },
        "Dreamspace: Dreaming your room space with text-driven panoramic texture propagation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2310.13119.pdf?!%5B%E5%9B%BE%E7%89%87%5D(https://mmbiz.qpic.cn/sz_mmbiz_png/tGynVEPiakb9lruS9sv1HdDZ7vhDqdSHTglAfA3BTYFnjkbjPq1ScXWEdvTr7zziboby5kzsWghbScUOPKSziag0g/640?wx_fmt=png)",
            "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 2, 3",
            "ref_ids": [
                "2"
            ],
            "1": ", by giving text prompts, and automatically transferring textures of our living room with enchanting and meaningful details? Over the past few years, enormous efforts have been paid in the field of scene stylization (or texture synthesis) [2, 5, 16, 18, 22, 40, 56].",
            "2": ", imitating Van Gogh\u2019s paintings instead of generating recognizable visual elements [22,56]), or focus on texture editing [2,18] on 3D objects with NeRF representation [31] but struggle to generate high-fidelity textures for the whole space and achieve real-time rendering on HMD devices.",
            "3": ", CLIP model [39]) for style transfer (or editing) [2,18], which achieves stylized results that also follow human language prompts, but these works mainly cannot be scaled to large indoor scenes that allow immersive room touring.",
            "4": "Therefore, existing works for scene-level stylization either are not applicable for immersive indoor scenescale scenarios with affordable computation on HMD devices [2,18], cannot support semantic meaningful style generation [6, 7, 11, 22, 23, 56], or require well-structured CAD model instead of real-world reconstruction [49]."
        },
        "ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields": {
            "authors": [],
            "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf",
            "ref_texts": "[29] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. SINE: Semantic-driven image-based NeRF editing with prior-guided editing field. In CVPR , 2023.",
            "ref_ids": [
                "29"
            ]
        },
        "Advances in 3D Generation: A Survey": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2401.17807",
            "ref_texts": ""
        },
        "Dyn-e: Local appearance editing of dynamic neural radiance fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2307.12909",
            "ref_texts": "Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. 2023. SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field. In CVPR . Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. 2022. Text2live: Text-driven layered image and video editing. In ECCV . Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo MartinBrualla, and Pratul P Srinivasan. 2021. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In ICCV\u2018\u2019 . Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman."
        },
        "Dynvideo-e: Harnessing dynamic nerf for large-scale motion-and view-change human-centric video editing": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2310.10624",
            "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 3",
            "ref_ids": [
                "1"
            ],
            "1": "SINE [1] supports editing a local region of static NeRF from a single view by delivering edited contents to multi-views through pretrained NeRF priors."
        },
        "ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2310.02712",
            "ref_texts": "Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 20919\u201320929, 2023. Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. InProceedings of the IEEE/CVF International Conference on Computer Vision , pp. 5855\u20135864, 2021. Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision , pp. 333\u2013350. Springer, 2022. Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 5501\u20135510, 2022. Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clipguided domain adaptation of image generators. arXiv preprint arXiv:2108.00946 , 2021. Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. arXiv preprint arXiv:2303.12789 , 2023. Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. arXiv preprint arXiv:2304.07090 , 2023. Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 867\u2013876, 2022. Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy Mitra. Relu fields: The little nonlinearity that could. In ACM SIGGRAPH 2022 Conference Proceedings , pp. 1\u20139, 2022. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643 , 2023."
        },
        "Mirror-NeRF: Learning Neural Radiance Fields for Mirrors with Whitted-Style Ray Tracing": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2308.03280",
            "ref_texts": "[2]Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. 2023. SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field. arXiv preprint arXiv:2303.13277",
            "ref_ids": [
                "2"
            ],
            "1": "Several extensions and improvements have been proposed to apply NeRF to more challenging problems, such as scene reconstruction [1,8,13,29,30,32,36,38,39,44,48], generalization [24,33], novel view extrapolation [35,45], scene manipulation [2,28,40\u201342], SLAM [23,54], segmentation [20,53], human body [18,31] and so on."
        },
        "NeRF in Robotics: A Survey": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2405.01333",
            "ref_texts": "[90] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with priorguided editing field,\u201d in CVPR , 2023, pp. 20 919\u201320 929.",
            "ref_ids": [
                "90"
            ],
            "1": "SINE\n[90] employs a prior-guided editing field to adjust spatial point coordinates and colours for semantic-driven editing.",
            "2": "[90] C."
        },
        "PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2311.13099",
            "ref_texts": "[3] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 2",
            "ref_ids": [
                "3"
            ],
            "1": "These include semantic-driven editing [3, 13, 24, 45, 66, 73], shading-driven adjustments (like relighting and texturing) [21, 43, 64, 68, 78, 84], scene modifications (such as object addition or removal) [35, 36, 76, 83, 90], face editing [27, 31, 70, 89], physics based editing from video[25, 62], and multi-purpose editing [30, 75, 82]."
        },
        "GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2404.02152",
            "ref_texts": "[5] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 2",
            "ref_ids": [
                "5"
            ],
            "1": "Neural Radiance Field [33] has exhibited great reconstruction and rendering qualities in SLAM [62, 73], scene editing [5, 58\u201360, 64] and relighting [63, 66, 67], especially promoting the emergence of many 3D avatar reconstruction [4, 16, 53, 68, 69, 76] and generation [50, 52, 54]."
        },
        "Coarf: Controllable 3d artistic style transfer for radiance fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2404.14967",
            "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023. 3",
            "ref_ids": [
                "1"
            ],
            "1": "CLIPNeRF and SINE [1, 51] enable text-driven editing, whereas [21] distills the 2D semantic feature from LSeg [26] to train 3D semantic feature using volumetric rendering, enabling editing including colorization, translation, deletion, and text-driven editing.",
            "2": "Our Semantic Aware Nearest Neighbor Feature Matching (SANNFM) function performs nearest neighbor matching between content and style features in both VGG (FV GG r,FV GG s ) and LSeg (FLSeg r,FLSeg s ) spaces for each specific pixel x, y with label m: SANNFM (x, y, m ) =argminx\u2032,y\u2032\u2208SDsannfm, (9) where S={x\u2032, y\u2032|Ms(x\u2032, y\u2032) =m}, and the distance functionDsannfm is defined as a weighted average of the VGG cosine distance and LSeg cosine distance: Dsannfm =\u03b1\u00b7D(FV GG r(x, y),FV GG s(x\u2032, y\u2032))\n+ (1\u2212\u03b1)\u00b7D(FLSeg r(x, y),FLSeg s(x\u2032, y\u2032)),(10) where \u03b1\u2208[0,1]is a hyperparameter to control the weight of VGG and LSeg features and D(."
        },
        "Aprf: Anti-aliasing projection representation field for inverse problem in imaging": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2307.05270",
            "ref_texts": "[27] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with priorguided editing field,\u201d in Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023.",
            "ref_ids": [
                "27"
            ],
            "1": "INR techniques have yielded impressive advances in image reconstruction for numerous tasks: single image super-resolution [19], video super-resolution [22], novel view synthesis [18], generative modeling [23]\u2013[25], and editing [26], [27].",
            "2": "[27] C."
        },
        "Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2312.01663",
            "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In CVPR , 2023. 3, 6",
            "ref_ids": [
                "2"
            ],
            "1": "CLIP-NeRF [42] and SINE [2] leverage prior models to optimize the geometry and texture of NeRF based on text descriptions or exemplar images.",
            "2": "For image-driven editing, due to the absence of existing methods for this setting, we modified several existing works to serve as baselines for comparison with our method, including: (1) Ours+Splice Loss: Splice loss [41] is proposed to disentangle structure and appearance information from an image for image editing, which is further demonstrated effective to transfer the texture from an exemplar image for NeRF editing in SINE [2]."
        },
        ": A 3D Neural Additive Model for Interpretable Shape Representation": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=wg8NPfeMF9",
            "ref_texts": "40\u201349. PMLR, 2018. Rishabh Agarwal, Nicholas Frosst, Xuezhou Zhang, Rich Caruana, and Geoffrey E Hinton. Neural additive models: Interpretable machine learning with neural nets. arXiv preprint arXiv:2004.13912 , 2020. Sercan \u00d6 Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35, pp. 6679\u20136687, 2021. K Somani Arun, Thomas S Huang, and Steven D Blostein. Least-squares fitting of two 3-d point sets. IEEE Transactions on pattern analysis and machine intelligence , (5):698\u2013700, 1987. Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp."
        },
        "S2RF: Semantically Stylized Radiance Fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2309.01252",
            "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023.",
            "ref_ids": [
                "2"
            ],
            "1": "Similar to previous methods [25, 12, 2, 26, 8, 6], addressing style transfer in 3D, we adopted an optimization-based approach.",
            "2": "An interesting work Sine [2], requires one image from a scene edited by the user and can generate a 3D view of the scene with the edited objects."
        },
        "Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2404.01168",
            "ref_texts": "1. Bao, C., Zhang, Y., Yang, B., Fan, T., Yang, Z., Bao, H., Zhang, G., Cui, Z.: Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition (CVPR) (2023)",
            "ref_ids": [
                "1"
            ],
            "1": "These endeavors have been geared towards refining reconstruction quality [2\u20134,15,38,43], enhancing computational efficiency [13,28,30,33,37], enabling advanced editing functionalities [1,23,40,44,45], and progressing dynamic scene representation [8,10,12,22,29].",
            "2": "We add a learnable mirror attribute m\u2208[0,1]for each Gaussian in the original 3DGS, representing the probability of it being a mirror.",
            "3": "(28) We apply SINE and Sigmoid activation function to normalize the opacity and mirror properties respectively, both of which are bounded within the range of [0, 1], while keeping all other experimental variables constant."
        },
        "ED-NeRF: Efficient Text-Guided Editing of 3D Scene With Latent Space NeRF": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=9DvDRTTdlu",
            "ref_texts": "Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 20919\u201320929, 2023. Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. InProceedings of the IEEE/CVF International Conference on Computer Vision , pp. 5855\u20135864, 2021. Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision , pp. 333\u2013350. Springer, 2022. Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 5501\u20135510, 2022. Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clipguided domain adaptation of image generators. arXiv preprint arXiv:2108.00946 , 2021. Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. arXiv preprint arXiv:2303.12789 , 2023. Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. arXiv preprint arXiv:2304.07090 , 2023. Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 867\u2013876, 2022. Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy Mitra. Relu fields: The little nonlinearity that could. In ACM SIGGRAPH 2022 Conference Proceedings , pp. 1\u20139, 2022. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643 , 2023."
        },
        "SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2312.09246",
            "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In CVPR , 2023. 2",
            "ref_ids": [
                "1"
            ],
            "1": "Approaches that followed include 3D editing from just a single edited view [1], or via 2D sketches [44], keypoints [87], attributes [25], meshes [22, 52, 76, 78, 80] or point clouds [5]."
        },
        "CaesarNeRF: Calibrated Semantic Representation for Few-shot Generalizable Neural Rendering": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2311.15510",
            "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In CVPR , pages 20919\u201320929, 2023. 2",
            "ref_ids": [
                "1"
            ],
            "1": "In recent years, NeRF has witnessed improvements in a wide range of applications, such as photo-realistic novel view synthesis for large-scale scenes [34, 60, 71], dynamic scene decomposition and deformation [21, 27, 31, 40\u2013\n42, 44, 75, 76], occupancy or depth estimation [58, 62, 74], scene generation and editing [1, 20, 28, 30, 35, 43, 63, 64, 70], and so on."
        },
        "Neural Rendering and Its Hardware Acceleration: A Review": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2402.00028",
            "ref_texts": "[8] C. Bao, Y. Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023.",
            "ref_ids": [
                "8"
            ],
            "1": "NeRF [62] and related works [60]\u2013 [8] have demonstrated good performance in learning scene representation from multi-view input data using volume rendering, which can be utilized in neural rendering-based inverse rendering frameworks.",
            "2": "[8] C."
        },
        "LLMs Meet Multimodal Generation and Editing: A Survey": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2405.19334",
            "ref_texts": "[358] C. Bao, Y. Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with prior-guided editing field,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023, pp.",
            "ref_ids": [
                "358"
            ],
            "1": "Sine [358] presented a prior-guided editing field that encodes finegrained geometric and texture modifications.",
            "2": "5 CLIP for 3D editing CLIP-NeRF [138] CVPR 2022 CLIP Loss NeRF CLIP Blended-NeRF [356] ICCVW 2023 CLIP Loss NeRF CLIP SKED [359] ICCV 2023 Score Distillation NeRF SD DreamEditor [360] SIGGRAPH Asia 2023 Score Distillation NeRF SD Instruct-NeRF2NeRF [361] SIGGRAPH Asia 2023 Score Distillation NeRF SD TextDeformer [357] TVCG 2022 Score Distillation Mesh SD SINE [358] CVPR 2023 Score Distillation NeRF SD Blending-NeRF [378] ICCV2023 CLIP Loss NeRF CLIP CustomNeRF [379] CVPR 2024 Score Distillation NeRF SD Paint3D [380] arXiv 2023 Mesh SD\n3D Paintbrush [362] arXiv 2023 Score Distillation NeRF SD\n20 TABLE 8: Audio datasets that can be adopted for language-based audio research.",
            "3": "[358] C."
        },
        "TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2401.14828",
            "ref_texts": "Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. 2023. Break-a-scene: Extracting multiple concepts from a single image. In SIGGRAPH Asia 2023 Conference Papers . 1\u201312. Omri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for text-driven editing of natural images. In CVPR 2022 . 18208\u201318218. Chong Bao, Yinda Zhang, and Bangbang et al. Yang. 2023. SINE: Semantic-driven imagebased nerf editing with prior-guided editing field. In CVPR 2023 . 20919\u201320929. Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2022. InstructPix2Pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800 (2022). de Charette Raoul Cao, Anh-Quan. 2023. SceneRF: Self-supervised monocular 3D scene reconstruction with radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 9387\u20139398. Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. 2023. Segment Anything in 3D with NeRFs. In NeurIPS . Jun-Kun Chen, Jipeng Lyu, and Yu-Xiong Wang. 2023c. Neuraleditor: Editing neural radiance fields via manipulating point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 12439\u201312448. Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023b. Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. arXiv preprint arXiv:2303.13873 (2023). Yiwen Chen, Zilong Chen, and Chi .eta Zhang. 2023a. GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting. arXiv preprint arXiv:2311.14521"
        },
        "Consolidating Attention Features for Multi-view Image Editing": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2402.14792",
            "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023. 3",
            "ref_ids": [
                "2"
            ],
            "1": "Recent works employ advances in text-based image editing to edit an implicit 3D representation with text [2, 20, 47, 54, 55]."
        },
        "Coin3D: Controllable and Interactive 3D Assets Generation with Proxy-Guided Conditioning": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2405.08054",
            "ref_texts": "Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. 2018. Learning representations and generative models for 3d point clouds. In International conference on machine learning . PMLR, 40\u201349. Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, and Zhaopeng Cui. 2024. GeneAvatar: Generic ExpressionAware Volumetric Head Avatar Editing from a Single Image. arXiv preprint arXiv:2404.02152 (2024). Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. 2023. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 20919\u201320929. Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. 2023. Multidiffusion: Fusing diffusion paths for controlled image generation. (2023). Shariq Farooq Bhat, Niloy J Mitra, and Peter Wonka. 2023. LooseControl: Lifting ControlNet for Generalized Depth Conditioning. arXiv preprint arXiv:2312.03079"
        },
        "RHINO: Regularizing the Hash-based Implicit Neural Representation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2309.12642",
            "ref_texts": "(2023) Sine: Semantic-driven image-based nerf editing with priorguided editing field. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp 20919\u201320929 Barron JT, Mildenhall B, Tancik M, Hedman P, Martin-Brualla R, Srinivasan PP (2021) Mip-nerf: A multiscale representation for antialiasing neural radiance fields. In: Proceedings of the IEEE/CVF International Conference on Computer Vision , pp 5855\u20135864 Cao A, Johnson J (2023) Hexplane: A fast representation for dynamic scenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp 130\u2013141 Chabra R, Lenssen JE, Ilg E, Schmidt T, Straub J, Lovegrove S, Newcombe R (2020) Deep local shapes: Learning local sdf priors for detailed 3d reconstruction. In: European Conference on Computer Vision , Springer, pp 608\u2013625 Chan ER, Lin CZ, Chan MA, Nagano K, Pan B, De Mello S, Gallo O, Guibas LJ, Tremblay J, Khamis S, et al. (2022) Efficient geometryaware 3d generative adversarial networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 16123\u201316133 Chen A, Xu Z, Zhao F, Zhang X, Xiang F, Yu J, Su H (2021) MVSNeRF: Fast generalizable radiance field reconstruction from multiview stereo. In: Proceedings of the IEEE/CVF International Conference on Computer Vision , pp 14124\u201314133 Chen A, Xu Z, Geiger A, Yu J, Su H (2022) Tensorf: Tensorial radiance fields. In: European Conference on Computer Vision , Springer, pp 333\u2013350 Chen Y, Lu L, Karniadakis GE, Dal Negro L (2020) Physics-informed neural networks for inverse problems in nano-optics and metamaterials. Optics express 28(8):11618\u201311633 Dupont E, Goli \u00b4nski A, Alizadeh M, Teh YW, Doucet A (2021) Coin: Compression with implicit neural representations. arXiv preprint arXiv:210303123 Fang J, Yi T, Wang X, Xie L, Zhang X, Liu W, Nie\u00dfner M, Tian Q"
        },
        "Advances in 3D Neural Stylization: A Survey": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2311.18328"
        },
        "StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2403.07807",
            "ref_texts": "1. Bao, C., Zhang, Y., Yang, B., Fan, T., Yang, Z., Bao, H., Zhang, G., Cui, Z.: Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition. pp. 20919\u201320929 (2023) 4",
            "ref_ids": [
                "1"
            ],
            "1": "Consequently, previous studies have resorted to learning-based methods for editing radiance fields [1,6,7,14,23\u201325, 34,42,45,46,50,52,57], guided by images [1,7,24,53], text [6,14,45,46,57], or other forms of user input [6,25,50], encompassing modifications such as deformation [34,50,52], appearance changes [6,14,24,45,46,57], removal [6], relighting [42], and inpainting [23,28]."
        },
        "DATENeRF: Depth-Aware Text-based Editing of NeRFs": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2404.04526",
            "ref_texts": "2. Bao, C., Zhang, Y., Yang, B., Fan, T., Yang, Z., Bao, H., Zhang, G., Cui, Z.: Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition. pp. 20919\u201320929 (2023) 3",
            "ref_ids": [
                "2"
            ],
            "1": "SINE [2] transfers edits from a single edited image across the entire scene using a ViT model [6] as a semantic texture prior."
        },
        "NeRF: Multi-Modal Decomposition NeRF with 3D Feature Fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2405.05010",
            "ref_texts": "1. Bao, C., Zhang, Y., Yang, B., Fan, T., Yang, Z., Bao, H., Zhang, G., Cui, Z.: Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In: CVPR. pp. 20919\u201320929 (2023) 4",
            "ref_ids": [
                "1"
            ],
            "1": "Neural rendering has spurred an exploration into implicit and hybrid representations, offering various approaches for 3D editing, such as changing global appearance [7,29], intrinsic decomposition [59,64], per-object decomposition [54, 56], geometry and texture editing [1,55,61], 3D inpainting [36,52], and others [20,39,50]."
        },
        "Semantically-aware Neural Radiance Fields for Visual Scene Understanding: A Comprehensive Review": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2402.11141",
            "ref_texts": ""
        },
        "SIGNeRF: Scene Integrated Generation for Neural Radiance Fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2401.01647",
            "ref_texts": "[2]Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. pages 20919\u201320929, 2023. 3",
            "ref_ids": [
                "2"
            ],
            "1": "On the other hand, SINE [2] allows direct NeRF editing by changing a reference image in 2D space."
        },
        "Noise-NeRF: Hide Information in Neural Radiance Fields using Trainable Noise": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2401.01216",
            "ref_texts": "[11] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui, \u201cSine: Semanticdriven image-based nerf editing with prior-guided editing field,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023, pp. 20919\u201320929.",
            "ref_ids": [
                "11"
            ],
            "1": "There are currently many improvements and application research on NeRF, including accelerated training of NeRF [8]\u2013[10], editing research on NeRF [11]\u2013\n[13], generalization research on NeRF [14]\u2013[17], and largescale scenes [18], [19], etc."
        },
        "4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance Fields via 4D Semantic Segmentation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2310.16858",
            "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023. 2",
            "ref_ids": [
                "2"
            ],
            "1": "Additionally, researchers have proposed blending editing methods [2, 14], which combine an auxiliary editing field with original NeRF to support creative editing."
        },
        "Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2405.17811",
            "ref_texts": "1. Bao, C., Zhang, Y., Yang, B., Fan, T., Yang, Z., Bao, H., Zhang, G., Cui, Z.: Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition. pp. 20919\u201320929 (2023) 4",
            "ref_ids": [
                "1"
            ],
            "1": "Some other work [1,32,34,44] edit the NeRF in texture level which is not the focus of this paper."
        },
        "NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2402.08622",
            "ref_texts": "[4] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023. 2",
            "ref_ids": [
                "4"
            ],
            "1": "Most of the aforementioned methods, however, ignore semantic similarity while performing stylization or appearance editing, with the exception of [4, 30, 50], who perform region-based stylization or appearance-editing of NeRFs, but do not change geometry.",
            "2": ", excluding topological changes [4, 30, 68])."
        },
        "SERF: Fine-Grained Interactive 3D Segmentation and Editing with Radiance Fields": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2312.15856",
            "ref_texts": "[3] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 1, 9",
            "ref_ids": [
                "3"
            ],
            "1": "In contrast, approaches like Get3D [17] and SINE [3] offer direct editing in the feature space, deviating from the high-resolution 2D feature maps common in 2D editing.",
            "2": "Subsequently, we leverage Du and{eto estimate the scene flow \u222beby applying the scene flow MLP proposed in DynPoint [75] and SINE [3]."
        },
        "Survey on controlable image synthesis with deep learning": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2307.10275",
            "ref_texts": "[171] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with priorguided editing field,\u201d in The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023.",
            "ref_ids": [
                "171"
            ],
            "1": "[171] proposed SINE, a novel approach for editing a neural radiance field (NeRF) with a single image or text prompts.",
            "2": "[171] C."
        },
        "Three-Dimensional-Consistent Scene Inpainting via Uncertainty-Aware Neural Radiance Field": {
            "authors": [
                "Meng Wang",
                "Qinkang Yu",
                "Haipeng Liu"
            ],
            "url": "https://www.mdpi.com/2079-9292/13/2/448/pdf",
            "ref_texts": "9. Bao, C.; Zhang, Y.; Yang, B.; Fan, T.; Yang, Z.; Bao, H.; Zhang, G.; Cui, Z. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Vancouver, BC, Canada, 17\u201324 June 2023; pp. 20919\u201320929.",
            "ref_ids": [
                "9"
            ],
            "1": "Numerous endeavors have aimed at augmenting their performance and expanding their applicability via, for instance, improving the training speed [3\u20135], reducing view input requirements [6,7], facilitating scene editing [8,9], and extending their functionality to dynamic scenes [10,11].",
            "2": "Driven by the needs of practical applications, NeRf editing methods [9,12\u201319] have emerged as a focal point of current research.",
            "3": "Furthermore, research related to interactive NeRF editing has focused more on interactive target selection [16] or semantic editing [9]."
        },
        "SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2402.16366",
            "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 2",
            "ref_ids": [
                "2"
            ],
            "1": "Neural Radiance Field Neural radiance field [28] has shown great ability in 3D reconstruction, and motivated massive follow-up works, such as editing [2, 43] and speeding-up [7, 29]."
        },
        "3D StreetUnveiler with Semantic-Aware 2DGS": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2405.18416",
            "ref_texts": "[2]Chong Bao, Yinda Zhang, and Bangbang et al. Yang. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In CVPR , pages 20919\u201320929, 2023.",
            "ref_ids": [
                "2"
            ],
            "1": "With the rapid development of Neural Scene Representation, editing a 3D scene has been explored by lots of works [10,88,75,81,2,23,19,40].",
            "2": "Subsequent works [2,23,19,40] utilized CLIP models to provide editing guidance from text prompts or reference images.",
            "3": "[2]Chong Bao, Yinda Zhang, and Bangbang et al."
        },
        "Semantic-Human: Neural Rendering of Humans from Monocular Video with Human Parsing": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2308.09894",
            "ref_texts": "Bao, C.; Zhang, Y .; Yang, B.; Fan, T.; Yang, Z.; Bao, H.; Zhang, G.; and Cui, Z. 2023. Sine: Semantic-driven imagebased nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 20919\u201320929. Barron, J. T.; Mildenhall, B.; Tancik, M.; Hedman, P.; Martin-Brualla, R.; and Srinivasan, P. P. 2021. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 5855\u20135864. Cao, A.; and Johnson, J. 2023. Hexplane: A fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 130\u2013141. Chen, Y .; Wang, X.; Chen, X.; Zhang, Q.; Li, X.; Guo, Y .; Wang, J.; and Wang, F. 2023. UV V olumes for real-time rendering of editable free-view human performance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 16621\u201316631. Cheng, W.; Xu, S.; Piao, J.; Qian, C.; Wu, W.; Lin, K.-Y .; and Li, H. 2022. Generalizable neural performer: Learning robust radiance fields for human novel view synthesis. arXiv preprint arXiv:2204.11798 . Deng, C. L.; and Tartaglione, E. 2023. Compressing explicit voxel grid representations: fast nerfs become also small. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , 1236\u20131245. Fridovich-Keil, S.; Meanti, G.; Warburg, F. R.; Recht, B.; and Kanazawa, A. 2023. K-planes: Explicit radiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 12479\u201312488. Gong, K.; Liang, X.; Li, Y .; Chen, Y .; Yang, M.; and Lin, L."
        },
        "VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System in Virtual Reality": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2401.16663",
            "ref_texts": "(2009), 114\u2013123. Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. 2023. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 20919\u201320929. Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Xu Jia, and Huchuan Lu. 2021. Animatable neural radiance fields from monocular rgb videos. arXiv preprint arXiv:2106.13629 (2021). Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. 2023. Gaussianeditor: Swift and controllable 3d editing with gaussian splatting. arXiv preprint arXiv:2311.14521"
        },
        "DragGaussian: Enabling Drag-style Manipulation on 3D Gaussian Representation": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2405.05800"
        },
        "LIVE: LaTex Interactive Visual Editing": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2405.06762",
            "ref_texts": "[33] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with priorguided editing field,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023, pp. 20 919\u201320 929.",
            "ref_ids": [
                "33"
            ],
            "1": "2021\n|{z}2022\n|{z}2023\n|{z}\n\u22c6\u2192 \u22c6\u22c6\u2192 \u22c6 \u22c6 \u22c6\u2192\n\u2191 \u2191 \u2191\n[13]NeRF\n[28]pixelnerf [29]NeRF -[15]Mip -nerf [14]D-nerf[30]Headnerf [21]Block -nerf [31]Mip -nerf360[32]Nope -nerf [33]Sine The main function of Gitem FlowGraph is automatically generating a time sequence or other developing sequence flow interactive graph to describe the development of one issue.",
            "2": "[33] C."
        },
        "Plasticine3D: Non-rigid 3D editting with text guidance": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2312.10111",
            "ref_texts": "[3] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 1, 3",
            "ref_ids": [
                "3"
            ],
            "1": "(4) Directly perform a controllable non-rigid transformation with a modified 3D NeRF representation[3], however they often require an additional guidance (e.",
            "2": "SINE[3] on the other hand, realizes controllable non-rigid editing by a modified 3D NeRF representation."
        },
        "ViFu: Multiple 360 Objects Reconstruction with Clean Background via Visible Part Fusion": {
            "authors": [],
            "url": "https://arxiv.org/pdf/2404.09426",
            "ref_texts": "[27] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with priorguided editing field,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2023.",
            "ref_ids": [
                "27"
            ],
            "1": "Another direction explores object-level manipulations on scene content, enabling editing to object appearance [26], [27] or geometry [4], [5].",
            "2": "[27] C."
        },
        "ViFu: Visible Part Fusion for Multiple Scene Radiance Fields": {
            "authors": [],
            "url": "https://openreview.net/pdf?id=C3msSjudA7",
            "ref_texts": "Kaxlamangla S. Arun, T. S. Huang, and Steven D. Blostein. Least-squares fitting of two 3-d point sets. IEEE Transactions on Pattern Analysis and Machine Intelligence , PAMI-9:698\u2013700, 1987. Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 20919\u201320929, 2023. Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M Dollar. The ycb object and model set: Towards common benchmarks for manipulation research. In 2015 international conference on advanced robotics (ICAR) , pp. 510\u2013517. IEEE, 2015. Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision , 2022. Blender Online Community. Blender a 3D modelling and rendering package . Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. URL http://www.blender.org . Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Michael Hickman, Krista Reymann, Thomas Barlow McHugh, and Vincent Vanhoucke. Google scanned objects: A highquality dataset of 3d scanned household items. 2022 International Conference on Robotics and Automation (ICRA) , pp. 2553\u20132560, 2022. Martin Ester, Hans-Peter Kriegel, J \u00a8org Sander, and Xiaowei Xu. A density-based algorithm for discovering clusters in large spatial databases with noise. In Knowledge Discovery and Data Mining , 1996. Jiading Fang, Shengjie Lin, Igor Vasiljevic, Vitor Guizilini, Rares Ambrus, Adrien Gaidon, Gregory Shakhnarovich, and Matthew R Walter. Nerfuser: Large-scale scene representation by nerf fusion. arXiv preprint arXiv:2305.13307 , 2023. Martin A. Fischler and Robert C. Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Commun. ACM , 24:381\u2013395, 1981. Michelle Guo, Alireza Fathi, Jiajun Wu, and Thomas Funkhouser. Object-centric neural scene rendering. arXiv preprint arXiv:2012.08503 , 2020. Won Jun Jang and Lourdes de Agapito. Codenerf: Disentangled neural radiance fields for object categories. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 12929\u2013"
        },
        "Blended-NeRF: Zero-Shot Object Generation and Blending In Existing Neural Radiance Fields (Supplementary Material)": {
            "authors": [],
            "url": "https://openaccess.thecvf.com/content/ICCV2023W/AI3DCC/supplemental/Gordon_Blended-NeRF_Zero-Shot_Object_ICCVW_2023_supplemental.pdf",
            "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. arXiv preprint arXiv:2303.13277 , 2023. 5",
            "ref_ids": [
                "1"
            ],
            "1": "After sampling a camera pose, we recenter its rays around the ROI by moving its center location according to the center of mass inside the ROI (tracked by exponential moving average during training), but allow with a probability p\u2208[0,1](hyperparameter, set to 0.",
            "2": "In SINE [1], they suggest a method for editing NeRF scene by only editing a single view, and than apply the edit to the entire scene."
        }
    }
}