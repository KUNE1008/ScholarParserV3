{
    "title": "mobile3drecon: real-time monocular 3d reconstruction on a mobile phone",
    "id": 3,
    "valid_pdf_number": "32/44",
    "matched_pdf_number": "23/32",
    "matched_rate": 0.71875,
    "citations": {
        "Neuralrecon: Real-time coherent 3d reconstruction from monocular video": {
            "authors": [
                "Jiaming Sun",
                "Yiming Xie",
                "Linghao Chen",
                "Xiaowei Zhou",
                "Hujun Bao"
            ],
            "url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Sun_NeuralRecon_Real-Time_Coherent_3D_Reconstruction_From_Monocular_Video_CVPR_2021_paper.pdf",
            "ref_texts": "[51] Xingbin Yang, L. Zhou, Hanqing Jiang, Z. Tang, Yuanbo Wang, H. Bao, and Guofeng Zhang. Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone. IEEE TVCG , 2020. 2",
            "ref_ids": [
                "51"
            ],
            "1": "[46,51] optimize this line of research towards low power consumption on mobile platforms.",
            "2": "2\n[51] Xingbin Yang, L."
        },
        "Depthformer: Exploiting long-range correlation and local information for accurate monocular depth estimation": {
            "authors": [
                "Z Li",
                "Z Chen",
                "X Liu",
                "J Jiang"
            ],
            "url": "https://link.springer.com/content/pdf/10.1007/s11633-023-1458-0.pdf",
            "ref_texts": "\u00a0X.\u00a0B.\u00a0Yang,\u00a0 L.\u00a0Y.\u00a0Zhou,\u00a0 H.\u00a0Q.\u00a0Jiang,\u00a0 Z.\u00a0L.\u00a0Tang,\u00a0 Y.\u00a0B. Wang,\u00a0 H.\u00a0J.\u00a0Bao,\u00a0 G.\u00a0F.\u00a0Zhang.\u00a0 Mobile3DRecon:\u00a0 Real-time monocular\u00a0 3D\u00a0reconstruction\u00a0 on\u00a0a\u00a0mobile\u00a0 phone.\u00a0 IEEE Transactions on Visualization and Computer Graphics , vol.\u00a026,\u00a0no.\u00a012,\u00a0pp.\u00a03446\u20133456,\u00a0 2020.\u00a0 DOI:\u00a0 10.1109/TVCG."
        },
        "Simplerecon: 3d reconstruction without 3d convolutions": {
            "authors": [
                "M Sayed",
                "J Gibson",
                "J Watson",
                "V Prisacariu"
            ],
            "url": "https://arxiv.org/pdf/2208.14743",
            "ref_texts": "77. Yang, X., Zhou, L., Jiang, H., Tang, Z., Wang, Y., Bao, H., Zhang, G.: Mobile3DRecon: Real-time monocular 3D reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics (2020)",
            "ref_ids": [
                "77"
            ],
            "1": "io/simplerecon 1 Introduction Generating 3D reconstructions of a scene is a challenging problem in computer vision which is useful for tasks such as robotic navigation, autonomous driving, content placement for augmented reality and historical preservation [47,77]."
        },
        "What's the Situation With Intelligent Mesh Generation: A Survey and Perspectives": {
            "authors": [
                "N Lei",
                "Z Li",
                "Z Xu",
                "Y Li",
                "X Gu"
            ],
            "url": "https://arxiv.org/pdf/2211.06009",
            "ref_texts": "[73] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang, \u201cMobile3drecon: real-time monocular 3d reconstruction on a mobile phone,\u201d IEEE TVCG , vol. 26, no. 12, pp. 3446\u2013",
            "ref_ids": [
                "73"
            ],
            "1": "[57] \u2713 \u2713 \u2713 U-Net Normal, depth maps DMC [58] \u2713 \u2713 \u2713 \u2713 \u2713 DMC Occupancy and vertex displacement CoMA [59] \u2713 \u2713\u2713 \u2713 Autoencoder Point position 3D-CFCN [60] \u2713 \u2713 \u2713 OctNet-based U-Net Truncated signed distance field MGN [11] \u2713 \u2713 \u2713 \u2713 \u2713 MLP Vertices position 3DN [35] \u2713 \u2713 \u2713 \u2713 \u2713 PointNet/VGG Vertices position TMN [12] \u2713 \u2713 \u2713 \u2713 ResNet/MLP Vertices position and errors ONet [61] \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 ResNet/PointNet Grid occupancy N3DMM [62] \u2713 \u2713\u2713 \u2713 Spiral-Conv GAN Vertices position PGAN [63] \u2713 \u2713\u2713 WGAN Geometry image HumanMeshNet [13] \u2713 \u2713 \u2713 \u2713 Resnet-18 Vertices position DISN [64] \u2713 \u2713 \u2713 \u2713 VGG-16 Signed distance field IM-Net [65] \u2713\u2713 \u2713\u2713 \u2713 IM-Net Signed distance field Scan2Mesh [66] \u2713 \u2713 \u2713 \u2713 3D-Conv GNN Mesh face DGP [67] \u2713 \u2713 \u2713 \u2713 MLP Local parametrization Mesh R-CNN [14] \u2713 \u2713 \u2713 \u2713 Mesh R-CNN Occupancy and point position DeepSDF [68] \u2713 \u2713 \u2713 MLP Signed distance field Pixel2mesh++ [15] \u2713 \u2713 \u2713 \u2713 VGG; GCN Vertex position PQ-Net [69] \u2713\u2713 \u2713 \u2713\u2713 Seq2Seq Autoencoder Signed distance field BCNet [16] \u2713 \u2713 \u2713 \u2713 ResNet; GAT; Spiral-Conv SMPL parameters; vertices position PolyGen [70] \u2713 \u2713\u2713 \u2713 \u2713 Transformer-based Predict vertices and faces sequentially DGTS [71] \u2713 \u2713 \u2713 \u2713 GCN Displacement vector per face Neural Subdivision [72] \u2713 \u2713 \u2713 \u2713 MLP Predict vertex position Mobile3drecon [73] \u2713 \u2713 \u2713 Res-UNet Depth map Sal [74] \u2713 \u2713 \u2713 MLP Unsigned distance field Pixel2mesh2 [17] \u2713 \u2713 \u2713 \u2713 GCN;G-Resnet Vertex position Voxel2mesh [40] \u2713 \u2713 \u2713 \u2713 GCN-3D Vertex position X-ray2shape [18] \u2713 \u2713 \u2713 \u2713 CNN;GCN Vertex position Liet al.",
            "2": "According to the different selected implicit functions, these IMG methods are divided into four categories: radial basis functions (RBFs) [52], occupancy fields [32], [54], [57], [58], [61], [73], [78], [84], [88], [91], [110], [119], [121], signed distance functions (SDFs) [39], [60], [64], [65], [69], [76], [77], [80], [87], [90], [92], [94]\u2013[97], [99], [101], [103], [107]\u2013[109], [113], [117], [120], and unsigned distance function (UDFs) [74], [86], [89], [100].",
            "3": "Currently, various occupancy-based methods with different network architectures and training strategies have been proposed to continuously improve the efficiency, robustness, and accuracy [32], [57], [73], [78], [84], [88], [91], [110], [119], [121].",
            "4": "83 Mobile3drecon [73] Real-time mesh generation Real-time dense mesh reconstruction Poorly maintained sharp features 3.",
            "5": "Existing methodologies [13], [28], [73], [90], [91], [93], [95], [110], [119], however, fall short of satisfying these criteria simultaneously, resulting in scene meshes that tend to be overly smooth and lacking in detail.",
            "6": "[73] X."
        },
        "Transformr: Pose-aware object substitution for composing alternate mixed realities": {
            "authors": [
                "M Kari",
                "T Grosse-Puppendahl"
            ],
            "url": "https://mkari.de/pubs/ismar2021-transformr.pdf",
            "ref_texts": "[63] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang. Mobile3drecon: Real-time monocular 3D reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics , 26(12):3446\u20133456, 2020. doi: 10.1109/TVCG.2020.3023634",
            "ref_ids": [
                "63"
            ],
            "1": "Index Terms: Human-centered computing\u2014Mixed / augmented reality\u2014;\u2014\n1 I NTRODUCTION Continuous advances in geometric scene understanding have contributed to the physical coherence of virtual objects in mixed reality scenes, for example through improvements in mesh reconstruction [63], occlusion shading [7], visual-inertial odometry [20,57], or light source estimation [59].",
            "2": "3347875\n[63] X."
        },
        "GR-PSN: learning to estimate surface normal and reconstruct photometric stereo images": {
            "authors": [
                "Y Ju",
                "B Shi",
                "Y Chen",
                "H Zhou",
                "J Dong"
            ],
            "url": "https://figshare.le.ac.uk/articles/journal_contribution/GR-PSN_Learning_to_Estimate_Surface_Normal_and_Reconstruct_Photometric_Stereo_Images/24463159/1/files/42980308.pdf",
            "ref_texts": "[3] Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, and Guofeng Zhang, \u201cMobile3drecon: real-time monocular 3d reconstruction on a mobile phone,\u201d IEEE Transactions on Visualization and Computer Graphics, vol. 26, no. 12, pp. 3446\u20133456, 2020.",
            "ref_ids": [
                "3"
            ],
            "1": "\u2726\n1 I NTRODUCTION RECOVERING the 3D shape of an object is a pivotal problem in many computer graphics and vision applications because it can further improve the understanding of images and scenes [1], [2], [3], [4]."
        },
        "MobiDepth: real-time depth estimation using on-device dual cameras": {
            "authors": [
                "Jinrui Zhang",
                "Huan Yang",
                "Ju Ren",
                "Deyu Zhang",
                "Bangwen He",
                "Ting Cao",
                "Yuanchun Li",
                "Yaoxue Zhang",
                "Yunxin Liu"
            ],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3495243.3560517",
            "ref_texts": "[54] Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, and Guofeng Zhang. 2020. Mobile3DRecon: real-time monocular 3D reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics 26, 12 (2020), 3446\u20133456.",
            "ref_ids": [
                "54"
            ],
            "1": "3560517\n1 INTRODUCTION In recent years, the mobile industry and research community have significantly invested in augmented reality (AR) and virtual reality (VR) applications for mobile devices [8,11,20,37,53,54].",
            "2": "[54] propose a keyframe-based real-time surface mesh generation approach to reconstruct 3D objects from single RGB image."
        },
        "Mononeuralfusion: Online monocular neural 3d reconstruction with geometric priors": {
            "authors": [
                "ZX Zou",
                "SS Huang",
                "YP Cao",
                "TJ Mu",
                "Y Shan"
            ],
            "url": "https://arxiv.org/pdf/2209.15153",
            "ref_texts": "[18] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang, \u201cMobile3drecon: real-time monocular 3d reconstruction on a mobile phone,\u201d IEEE Transactions on Visualization and Computer Graphics , vol. 26, no. 12, pp. 3446\u20133456, 2020.[19] A. Gordon, H. Li, R. Jonschkowski, and A. Angelova, \u201cDepth from videos in the wild: Unsupervised monocular depth learning from unknown cameras,\u201d in IEEE ICCV , 2019, pp. 8977\u20138986.",
            "ref_ids": [
                "18",
                "19"
            ],
            "1": "With the progress of deep learning, some pioneering works [16], [17], [18] adopt the single-view depth estimation to monoc\u2022Zi-Xin Zou and Tai-Jiang Mu are with BNRist, the Department of Computer Science and Technology, Tsinghua University, Beijing, China.",
            "2": "However, given effective deep learning based monocular depth estimation approaches [19], [20], [21], it is still challenging to generate consistent depth estimation across different views, making it difficult to build coherent 3D reconstruction for large-scale VA/AR applications.",
            "3": "Mobile3DRecon [18] uses a multi-view semi-global matching method followed by a depth refinement post-processing for robust monocular depth estimation.",
            "4": "[18] X.",
            "5": "[19] A."
        },
        "FarfetchFusion: Towards Fully Mobile Live 3D Telepresence Platform": {
            "authors": [
                "Kyungjin Lee"
            ],
            "url": "https://scholar.archive.org/work/q4ljkmkzpjcgrc77uzr33txllu/access/wayback/https://dl.acm.org/doi/pdf/10.1145/3570361.3592525",
            "ref_texts": "[56] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang. Mobile3drecon: Real-time monocular 3d reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics , 26(12):3446\u20133456, 2020.",
            "ref_ids": [
                "56"
            ],
            "1": "Existing mobile 3D reconstruction systems that leverages TSDF-based volumetric fusion focus on reconstructing static 3D scenes [38,42,56].",
            "2": "[56] X."
        },
        "The robodepth challenge: Methods and advancements towards robust depth estimation": {
            "authors": [
                "L Kong",
                "Y Niu",
                "S Xie",
                "H Hu",
                "LX Ng"
            ],
            "url": "https://arxiv.org/pdf/2307.15061",
            "ref_texts": "[119] Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, and Guofeng Zhang. Mobile3drecon: real-time monocular 3d reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics (TVCG) , 26(12):3446\u20133456, 2020. 9",
            "ref_ids": [
                "119"
            ],
            "1": "1 Overview Depth estimation is a fundamental task in 3D vision with vital applications, such as autonomous driving [93], augmented reality [123], virtual reality [59], and 3D reconstruction [119]."
        },
        "Fmgs: Foundation model embedded 3d gaussian splatting for holistic 3d scene understanding": {
            "authors": [
                "X Zuo",
                "P Samangouei",
                "Y Zhou",
                "Y Di",
                "M Li"
            ],
            "url": "https://arxiv.org/pdf/2401.01970",
            "ref_texts": "[59] Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, and Guofeng Zhang. Mobile3drecon: realtime monocular 3d reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics , 26(12):3446\u2013",
            "ref_ids": [
                "59"
            ],
            "1": "To estimate the dense 3d voxel cells, probabilistic fusion methods were firstly [22] used and researchers also developed end-to-end learn-able methods [50], by using either depth sensors [22] or monocular camera systems [59]."
        },
        "Efficient view path planning for autonomous implicit reconstruction": {
            "authors": [
                "J Zeng",
                "Y Li",
                "Y Ran",
                "S Li",
                "F Gao",
                "L Li"
            ],
            "url": "https://arxiv.org/pdf/2209.13159"
        },
        "Real-time globally consistent 3D reconstruction with semantic priors": {
            "authors": [
                "SS Huang",
                "H Chen",
                "J Huang",
                "H Fu"
            ],
            "url": "https://shishenghuang.github.io/index/Papers/semanticfusion/paper.pdf",
            "ref_texts": "[4] Y. Zhang, W. Xu, Y. Tong, and K. Zhou, \u201cOnline structure analysis for real-time indoor scene reconstruction,\u201d ACM Trans. Graphics , vol. 34, no. 5, pp. 159:1\u2013159:13, 2015.",
            "ref_ids": [
                "4"
            ],
            "1": "Most of early works have focused on either 3D reconstruction [1], [2], [3], [4], [5], [6], [7], [8], [9] or 3D semantic segmentation [10], [11], [12], [13], [14], [15], [16] separately.",
            "2": "[4] Y."
        },
        "Coli-ba: Compact linearization based solver for bundle adjustment": {
            "authors": [
                "Zhichao Ye",
                "Guanglin Li",
                "Haomin Liu",
                "Zhaopeng Cui",
                "Hujun Bao",
                "Guofeng Zhang"
            ],
            "url": "http://www.cad.zju.edu.cn/home/gfzhang/papers/CoLi/CoLi.pdf",
            "ref_texts": "[45] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang. Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone. IEEE Transactions on Visualization and Computer Graphics, 26(12):3446\u20133456, 2020.",
            "ref_ids": [
                "45"
            ],
            "1": "Facing large scenes, the convergence becomes extremely slow, resulting in many offline reconstruction systems timecosting [3,35, 45].",
            "2": "[45] X."
        },
        "Robust real-time AUV self-localization based on stereo vision-inertia": {
            "authors": [
                "Y Wang",
                "D Gu",
                "X Ma",
                "J Wang"
            ],
            "url": "https://repository.essex.ac.uk/34811/1/Robust_Real_Time_AUV_Self_Localization_Based_on_Stereo_Vision_Inertia__final_version.pdf",
            "ref_texts": "[36] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang, \u201cMobile3DRecon: Real-time monocular 3D reconstruction on a mobile phone,\u201d IEEE Trans. Vis. Comput. Graphics , vol. 26, no. 12, pp. 3446\u2013",
            "ref_ids": [
                "36"
            ],
            "1": "In the literature, some kinds of visionbased methods have been applied to the localization of unmanned equipment [33], [34], [35], [36].",
            "2": "[36] X.",
            "3": ": DeepSLAM: A ROBUST MONOCULAR SLAM SYSTEM WITH UNSUPERVISED DL 3587\n[36] R."
        },
        "Time-Distributed Framework for 3D Reconstruction Integrating Fringe Projection with Deep Learning": {
            "authors": [
                "Hieu Nguyen",
                "Zhaoyang Wang"
            ],
            "url": "https://www.mdpi.com/1424-8220/23/16/7284/pdf",
            "ref_texts": "32. Yang, X.; Zhuo, L.; Jiang, H.; Tang, Z.; Wang, Y.; Bao, H.; Zhang, G. Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone. IEEE Trans. Vis. Comput. Graph. 2020 ,26, 3446\u20133456. [CrossRef]",
            "ref_ids": [
                "32"
            ]
        },
        "Multi-sensor fusion self-supervised deep odometry and depth estimation": {
            "authors": [
                "Yingcai Wan",
                "Qiankun Zhao",
                "Cheng Guo",
                "Chenlong Xu",
                "Lijing Fang"
            ],
            "url": "https://www.mdpi.com/2072-4292/14/5/1228/pdf",
            "ref_texts": "3. Yang, X.; Zhou, L.; Jiang, H.; Tang, Z.; Wang, Y.; Bao, H.; Zhang, G. Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone. IEEE Trans. Vis. Comput. Graph. 2020 ,26, 3446\u20133456. [CrossRef] [PubMed]",
            "ref_ids": [
                "3"
            ],
            "1": "Introduction Dense depth estimation from an RGB image is the fundamental issue for 3D scene reconstruction that is useful for computer vision applications, such as automatic driving [1], simultaneous localization and mapping (SLAM) [2], and 3D scene understanding [3]."
        },
        "SimpleMapping: Real-time visual-inertial dense mapping with deep multi-view stereo": {
            "authors": [
                "Y Xin",
                "X Zuo",
                "D Lu"
            ],
            "url": "https://arxiv.org/pdf/2306.08648",
            "ref_texts": "[61] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang. Mobile3DRecon: real-time monocular 3D reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics , 26(12):3446\u20133456, 2020.",
            "ref_ids": [
                "61"
            ],
            "1": "Mobile3DRecon [61] employs a multi-view semi-global matching method to recover a dense depth map, which is subsequently refined by a lightweight CNN-based single-view depth refinement neural network.",
            "2": "[61] X."
        },
        "Monocular Depth Estimation: Lightweight Convolutional and Matrix Capsule Feature-Fusion Network": {
            "authors": [
                "Yinchu Wang",
                "Haijiang Zhu"
            ],
            "url": "https://www.mdpi.com/1424-8220/22/17/6344/pdf",
            "ref_texts": "5. Yang, X.; Zhou, L.; Jiang, H.; Tang, Z.; Wang, Y.; Bao, H.; Zhang, G. Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone. IEEE Trans. Vis. Comput. Graph. 2020 ,26, 3446\u20133456. [CrossRef] [PubMed]",
            "ref_ids": [
                "5"
            ],
            "1": "Obtaining a depth image of a real-world scene through depth estimation provides data that can serve as the basis for many applications, such as robots [1], autonomous driving [2], SLAM [3], augmented reality [4], 3D reconstruction [5], and segmentation [6]."
        },
        "Attention-enhanced cross-modal localization between 360 images and point clouds": {
            "authors": [
                "Z Zhao",
                "H Yu",
                "C Lyv",
                "W Yang",
                "S Scherer"
            ],
            "url": "https://arxiv.org/pdf/2212.02757",
            "ref_texts": "[2] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang, \u201cMobile3DRecon: real-time monocular 3d reconstruction on a mobile phone,\u201d IEEE Transactions on Visualization and Computer Graphics , vol. 26, no. 12, pp. 3446\u20133456, 2020.",
            "ref_ids": [
                "2"
            ],
            "1": "I NTRODUCTION Locating the position of an image inthe point cloud map is of great importance for mobile robots and autonomous vehicles with numerous applications such as Simultaneous Localization and Mapping (SLAM) [1] and Virtual Reality [2].",
            "2": "[2] X."
        },
        "A Comprehensive Review of Vision-Based 3D Reconstruction Methods": {
            "authors": [
                "Linglong Zhou",
                "Guoxin Wu",
                "Yunbo Zuo",
                "Xuanyu Chen",
                "Hongle Hu"
            ],
            "url": "https://www.mdpi.com/1424-8220/24/7/2314/pdf",
            "ref_texts": "321. Yang, X.; Zhou, L.; Jiang, H.; Tang, Z.; Wang, Y.; Bao, H.; Zhang, G. Mobile3DRecon: Real-time monocular 3D reconstruction on a mobile phone. IEEE Trans. Vis. Comput. Graph. 2020 ,26, 3446\u20133456. [CrossRef] [PubMed] Disclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.",
            "ref_ids": [
                "321"
            ],
            "1": "The popularity of mobile devices enables users to easily conduct image-based 3D scanning and 3D reconstruction [321]."
        },
        "Depth Completion with Multiple Balanced Bases and Confidence for Dense Monocular SLAM": {
            "authors": [
                "W Xie",
                "G Chu",
                "Q Qian",
                "Y Yu",
                "H Li",
                "D Chen"
            ],
            "url": "https://arxiv.org/pdf/2309.04145",
            "ref_texts": "[60] Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, and Guofeng Zhang. Mobile3DRecon: real-time monocular 3d reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics, 26(12):3446\u20133456, 2020.",
            "ref_ids": [
                "60"
            ],
            "1": "Our fusion method is based on an online incremental mesh generation method Mobile3DRecon [60]."
        },
        "Real-time hybrid mapping of populated indoor scenes using a low-cost monocular uav": {
            "authors": [
                "S Golodetz",
                "M Vankadari",
                "A Everitt"
            ],
            "url": "https://arxiv.org/pdf/2203.02453",
            "ref_texts": "[42] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang, \u201cMobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone,\u201d TVCG , 2020.",
            "ref_ids": [
                "42"
            ],
            "1": "We select the keyframe that maximises the following score (a simplified version of that in [42]): Sf(k) =I[\u2206f t(k)\u2265\u03c4tand\u2206f \u03b8(k)\u2264\u03c4\u03b8]e(\u2212(\u2206f t(k)\u2212\u03b4t)2/\u03c32 t)(3) In this, Idenotes the binary indicator function, fdenotes the frame,kdenotes a keyframe, \u2206f tand\u2206f \u03b8respectively denote the baseline (m) and angle (\u25e6) betweenfandk,\u03b4t= 0.",
            "2": "[42] X."
        },
        "Method for automated data collection for 3d reconstruction": {
            "authors": [
                "M Zaslavskiy",
                "R Shestopalov"
            ],
            "url": "https://fruct.org/publications/volume-32/fruct32/files/Zas.pdf"
        },
        "MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth Smooth Regularization": {
            "authors": [
                "P Zhu",
                "Y Zhuang",
                "B Chen",
                "L Li",
                "C Wu",
                "Z Liu"
            ],
            "url": "https://arxiv.org/pdf/2405.06241",
            "ref_texts": "[12] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang, \u201cMobile3drecon: Real-time monocular 3d reconstruction on a mobile phone,\u201d IEEE Transactions on Visualization and Computer Graphics , vol. 26, no. 12, pp. 3446\u20133456, 2020.",
            "ref_ids": [
                "12"
            ],
            "1": "Another study [11], [12] combines a real-time VO/SLAM system with a Multi-View Stereo (MVS) network for parallel tracking and dense depth estimation, and then the Truncated Signed Distance Function (TSDF) [13] is used to fuse depth maps and extract mesh.",
            "2": "[12] X."
        },
        "DINO-SD: Champion Solution for ICRA 2024 RoboDepth Challenge": {
            "authors": [
                "Y Mao",
                "M Li",
                "J Liu",
                "J Liu",
                "Z Qin",
                "C Chu",
                "J Xu"
            ],
            "url": "https://arxiv.org/pdf/2405.17102"
        },
        "The present and future of mixed reality in China": {
            "authors": [
                "G Zhang",
                "X Zhou",
                "F Tian",
                "H Zha",
                "Y Wang"
            ],
            "url": "https://dl.acm.org/doi/pdf/10.1145/3481619",
            "ref_texts": "25. Yang, X., Zhou, L., Jiang, H. Tang, Z., Wang, Y., Bao, H., Zhang, G. Mobile3DRecon: real-time monocular ",
            "ref_ids": [
                "25"
            ]
        },
        "Parallel Implementation of 3D Model Reconstruction of Monocular Video Frames in a Dynamic Environment.": {
            "authors": [
                "GM Fathy",
                "HA Hassan",
                "WM Sheta",
                "F Omara"
            ],
            "url": "https://inass.org/wp-content/uploads/2022/05/2022083153-2.pdf",
            "ref_texts": "[20] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang , \u201cMobile3drecon: real time monocular 3d reconstruction on a mobile phone \u201d, IEEE Transactions on Visual -Ization and Computer Graphics , Vol. 26, No. 12, pp. ",
            "ref_ids": [
                "20"
            ],
            "1": "Scene Methods Device Time \n(T/F) \n[19] Single Static object Monocular SLAM NVIDIA GeForce TITAN X without CUDA 21 ms \n[21] Single Dynamic object Markless \n3D human motion capture GeForce RTX 2070 without CUDA 40 ms \n[22] Single Dynamic object GCN network Nvidia GeForce RTX 2080Ti \n-without CUDA 23 ms \n[20] Full Static scene Online incremental mesh generation a single CPU thread third -party library OpenCV 2 57.",
            "2": "[20] X."
        },
        "A portable V-SLAM based solution for advanced visual 3D mobile mapping": {
            "authors": [
                "A Torresani"
            ],
            "url": "https://iris.unitn.it/bitstream/11572/362031/1/PhD_thesis_final.pdf",
            "ref_texts": "[100] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang, \u201cMobile3drecon: real-time monocular 3d reconstruction on a mobile phone,\u201dIEEE Transactions on Visualization and Computer Graphics , vol. 26, no. 12, pp. 3446\u20133456, 2020.",
            "ref_ids": [
                "100"
            ],
            "1": "Despite in the latest years some works were able to achieve it [48, 100], important compromises on the image resolutions, maximum scene size and reconstruction accuracy are still necessary.",
            "2": "[100] X."
        },
        "\u0421\u0440\u0430\u0432\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u0441\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0442\u0440\u0435\u0445\u043c\u0435\u0440\u043d\u043e\u0439 \u0440\u0435\u043a\u043e\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u0438": {
            "authors": [
                "\u0420\u041f \u0428\u0435\u0441\u0442\u043e\u043f\u0430\u043b\u043e\u0432",
                "\u041c\u041c \u0417\u0430\u0441\u043b\u0430\u0432\u0441\u043a\u0438\u0439"
            ],
            "url": "https://etu.ru/assets/files/Faculty-FKTI/MO/sbornik-2022-moevm.pdf#page=16",
            "ref_texts": ""
        },
        "On-Device 3D Foot Reconstruction for Digital Sizing": {
            "authors": [
                "N Hassan"
            ],
            "url": "https://tspace.library.utoronto.ca/bitstream/1807/129715/2/Hassan_Najah_202211_MAS_thesis.pdf",
            "ref_texts": ""
        },
        "Isovist computation of outdoor environment with semi-dense line SLAM and monocular camera": {
            "authors": [
                "T Le Jan",
                "M Servi\u00e8res",
                "T Leduc",
                "V Tourre"
            ],
            "url": "https://hal.science/hal-03368466/document",
            "ref_texts": ""
        }
    }
}