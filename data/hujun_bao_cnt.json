{
    "dp-mvs: detail preserving multi-view surface reconstruction of large-scale scenes": {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "DP-MVS: Detail preserving multi-view surface reconstruction of large-scale scenes",
            "author": "Zhou, Liyang and Zhang, Zhuang and Jiang, Hanqing and Sun, Han and Bao, Hujun and Zhang, Guofeng",
            "pub_year": "2021",
            "venue": "Remote Sensing",
            "abstract": "This paper presents an accurate and robust dense 3D reconstruction system for detail preserving surface modeling of large-scale scenes from multi-view images, which we named DP-MVS. Our system performs high-quality large-scale dense reconstruction, which preserves geometric details for thin structures, especially for linear objects. Our framework begins with a sparse reconstruction carried out by an incremental Structure-from-Motion. Based on the reconstructed sparse map, a novel detail preserving PatchMatch approach is",
            "publisher": "MDPI",
            "pages": "4569",
            "number": "22",
            "volume": "13",
            "journal": "Remote Sensing",
            "pub_type": "article",
            "bib_id": "zhou2021dp"
        },
        "filled": true,
        "gsrank": 1,
        "pub_url": "https:\/\/www.mdpi.com\/2072-4292\/13\/22\/4569",
        "author_id": [
            "",
            "",
            "4K67pwIAAAAJ",
            "",
            "",
            "F0xfpXAAAAAJ"
        ],
        "url_scholarbib": "\/scholar?hl=en&q=info:sLqQTF4QfcIJ:scholar.google.com\/&output=cite&scirp=0&hl=en",
        "url_add_sclib": "\/citations?hl=en&xsrf=&continue=\/scholar%3Fq%3Ddp-mvs:%2Bdetail%2Bpreserving%2Bmulti-view%2Bsurface%2Breconstruction%2Bof%2Blarge-scale%2Bscenes%26hl%3Den%26as_sdt%3D0,5&citilm=1&update_op=library_add&info=sLqQTF4QfcIJ&ei=buBmZsbLDb3Hy9YPl4uV4AQ&json=",
        "num_citations": 17,
        "citedby_url": "\/scholar?cites=14014375612644375216&as_sdt=2005&sciodt=0,5&hl=en",
        "url_related_articles": "\/scholar?q=related:sLqQTF4QfcIJ:scholar.google.com\/&scioq=dp-mvs:+detail+preserving+multi-view+surface+reconstruction+of+large-scale+scenes&hl=en&as_sdt=0,5",
        "eprint_url": "https:\/\/www.mdpi.com\/2072-4292\/13\/22\/4569",
        "self-cite": 2,
        "other-cite": 15,
        "total-cite": 17,
        "has-interesting-author": true
    },
    "vox-surf: voxel-based implicit surface representation": {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Vox-surf: Voxel-based implicit surface representation",
            "author": "Li, Hai and Yang, Xingrui and Zhai, Hongjia and Liu, Yuqian and Bao, Hujun and Zhang, Guofeng",
            "pub_year": "2022",
            "venue": "IEEE Transactions on …",
            "abstract": "Virtual content creation and interaction play an important role in modern 3D applications. Recovering detailed 3D models from real scenes can significantly expand the scope of its applications and has been studied for decades in the computer vision and computer graphics community. In this work, we propose Vox-Surf, a voxel-based implicit surface representation. Our Vox-Surf divides the space into finite sparse voxels, where each voxel is a basic geometry unit that stores geometry and appearance information on its corner",
            "publisher": "IEEE",
            "journal": "IEEE Transactions on Visualization and Computer Graphics",
            "pub_type": "article",
            "bib_id": "li2022vox"
        },
        "filled": true,
        "gsrank": 1,
        "pub_url": "https:\/\/ieeexplore.ieee.org\/abstract\/document\/9969571\/",
        "author_id": [
            "vn89ztQAAAAJ",
            "",
            "alXpF8wAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "\/scholar?hl=en&q=info:ZqB920z_5UcJ:scholar.google.com\/&output=cite&scirp=0&hl=en",
        "url_add_sclib": "\/citations?hl=en&xsrf=&continue=\/scholar%3Fq%3Dvox-surf:%2Bvoxel-based%2Bimplicit%2Bsurface%2Brepresentation%26hl%3Den%26as_sdt%3D0,5&citilm=1&update_op=library_add&info=ZqB920z_5UcJ&ei=feBmZrO7G53Iy9YP-N-YsAI&json=",
        "num_citations": 33,
        "citedby_url": "\/scholar?cites=5180827651924664422&as_sdt=2005&sciodt=0,5&hl=en",
        "url_related_articles": "\/scholar?q=related:ZqB920z_5UcJ:scholar.google.com\/&scioq=vox-surf:+voxel-based+implicit+surface+representation&hl=en&as_sdt=0,5",
        "eprint_url": "https:\/\/arxiv.org\/pdf\/2208.10925",
        "self-cite": 8,
        "other-cite": 25,
        "total-cite": 33,
        "has-interesting-author": true
    },
    "mobile3dscanner: an online 3d scanner for high-quality object reconstruction with a mobile device": {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Mobile3dscanner: An online 3d scanner for high-quality object reconstruction with a mobile device",
            "author": "Xiang, Xiaojun and Jiang, Hanqing and Zhang, Guofeng and Yu, Yihao and Li, Chenchen and Yang, Xingbin and Chen, Danpeng and Bao, Hujun",
            "pub_year": "2021",
            "venue": "… on Visualization and …",
            "abstract": "We present a novel online 3D scanning system for high-quality object reconstruction with a mobile device, called Mobile3DScanner. Using a mobile device equipped with an embedded RGBD camera, our system provides online 3D object reconstruction capability for users to acquire high-quality textured 3D object models. Starting with a simultaneous pose tracking and TSDF fusion module, our system allows users to scan an object with a mobile device to get a 3D model for real-time preview. After the real-time scanning process is",
            "publisher": "IEEE",
            "pages": "4245--4255",
            "number": "11",
            "volume": "27",
            "journal": "IEEE Transactions on Visualization and Computer Graphics",
            "pub_type": "article",
            "bib_id": "xiang2021mobile3dscanner"
        },
        "filled": true,
        "gsrank": 1,
        "pub_url": "https:\/\/ieeexplore.ieee.org\/abstract\/document\/9523840\/",
        "author_id": [
            "4Pu4JaIAAAAJ",
            "4K67pwIAAAAJ",
            "F0xfpXAAAAAJ",
            "",
            ""
        ],
        "url_scholarbib": "\/scholar?hl=en&q=info:4carFzyWXwQJ:scholar.google.com\/&output=cite&scirp=0&hl=en",
        "url_add_sclib": "\/citations?hl=en&xsrf=&continue=\/scholar%3Fq%3Dmobile3dscanner:%2Ban%2Bonline%2B3d%2Bscanner%2Bfor%2Bhigh-quality%2Bobject%2Breconstruction%2Bwith%2Ba%2Bmobile%2Bdevice%26hl%3Den%26as_sdt%3D0,5&citilm=1&update_op=library_add&info=4carFzyWXwQJ&ei=ieBmZqGJO--Dy9YPj_S4UA&json=",
        "num_citations": 9,
        "citedby_url": "\/scholar?cites=315135683778561761&as_sdt=2005&sciodt=0,5&hl=en",
        "url_related_articles": "\/scholar?q=related:4carFzyWXwQJ:scholar.google.com\/&scioq=mobile3dscanner:+an+online+3d+scanner+for+high-quality+object+reconstruction+with+a+mobile+device&hl=en&as_sdt=0,5",
        "self-cite": 1,
        "other-cite": 8,
        "total-cite": 9,
        "has-interesting-author": true
    },
    "mobile3drecon: real-time monocular 3d reconstruction on a mobile phone": {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Mobile3DRecon: Real-time monocular 3D reconstruction on a mobile phone",
            "author": "Yang, Xingbin and Zhou, Liyang and Jiang, Hanqing and Tang, Zhongliang and Wang, Yuanbo and Bao, Hujun and Zhang, Guofeng",
            "pub_year": "2020",
            "venue": "… on Visualization and …",
            "abstract": "We present a real-time monocular 3D reconstruction system on a mobile phone, called Mobile3DRecon. Using an embedded monocular camera, our system provides an online mesh generation capability on back end together with real-time 6DoF pose tracking on front end for users to achieve realistic AR effects and interactions on mobile phones. Unlike most existing state-of-the-art systems which produce only point cloud based 3D models online or surface mesh offline, we propose a novel online incremental mesh generation approach to",
            "publisher": "IEEE",
            "pages": "3446--3456",
            "number": "12",
            "volume": "26",
            "journal": "IEEE Transactions on Visualization and Computer Graphics",
            "pub_type": "article",
            "bib_id": "yang2020mobile3drecon"
        },
        "filled": true,
        "gsrank": 1,
        "pub_url": "https:\/\/ieeexplore.ieee.org\/abstract\/document\/9201064\/",
        "author_id": [
            "",
            "",
            "4K67pwIAAAAJ",
            ""
        ],
        "url_scholarbib": "\/scholar?hl=en&q=info:mVziGz3RwtAJ:scholar.google.com\/&output=cite&scirp=0&hl=en",
        "url_add_sclib": "\/citations?hl=en&xsrf=&continue=\/scholar%3Fq%3Dmobile3drecon:%2Breal-time%2Bmonocular%2B3d%2Breconstruction%2Bon%2Ba%2Bmobile%2Bphone%26hl%3Den%26as_sdt%3D0,5&citilm=1&update_op=library_add&info=mVziGz3RwtAJ&ei=muBmZpH3H6aty9YPybqgQA&json=",
        "num_citations": 67,
        "citedby_url": "\/scholar?cites=15042815765761907865&as_sdt=2005&sciodt=0,5&hl=en",
        "url_related_articles": "\/scholar?q=related:mVziGz3RwtAJ:scholar.google.com\/&scioq=mobile3drecon:+real-time+monocular+3d+reconstruction+on+a+mobile+phone&hl=en&as_sdt=0,5",
        "eprint_url": "http:\/\/www.cad.zju.edu.cn\/home\/gfzhang\/papers\/Mobile3DRecon\/mobile-3d-recon.pdf",
        "self-cite": 8,
        "other-cite": 59,
        "total-cite": 67,
        "has-interesting-author": true
    },
    "vox-fusion: dense tracking and mapping with voxel-based neural implicit representation": {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation",
            "author": "Yang, Xingrui and Li, Hai and Zhai, Hongjia and Ming, Yuhang and Liu, Yuqian and Zhang, Guofeng",
            "pub_year": "2022",
            "venue": "2022 IEEE International …",
            "abstract": "In this work, we present a dense tracking and mapping system named Vox-Fusion, which seamlessly fuses neural implicit representations with traditional volumetric fusion methods. Our approach is inspired by the recently developed implicit mapping and positioning system and further extends the idea so that it can be freely applied to practical scenarios. Specifically, we leverage a voxel-based neural implicit surface representation to encode and optimize the scene inside each voxel. Furthermore, we adopt an octree-based structure to",
            "organization": "IEEE",
            "pages": "499--507",
            "booktitle": "2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)",
            "pub_type": "inproceedings",
            "bib_id": "yang2022vox"
        },
        "filled": true,
        "gsrank": 1,
        "pub_url": "https:\/\/ieeexplore.ieee.org\/abstract\/document\/9995035\/",
        "author_id": [
            "",
            "vn89ztQAAAAJ",
            "alXpF8wAAAAJ",
            "Q91BzegAAAAJ",
            ""
        ],
        "url_scholarbib": "\/scholar?hl=en&q=info:lEtd6DL0bv0J:scholar.google.com\/&output=cite&scirp=0&hl=en",
        "url_add_sclib": "\/citations?hl=en&xsrf=&continue=\/scholar%3Fq%3Dvox-fusion:%2Bdense%2Btracking%2Band%2Bmapping%2Bwith%2Bvoxel-based%2Bneural%2Bimplicit%2Brepresentation%26hl%3Den%26as_sdt%3D0,5&citilm=1&update_op=library_add&info=lEtd6DL0bv0J&ei=qOBmZs-dCJ3Iy9YP-N-YsAI&json=",
        "num_citations": 90,
        "citedby_url": "\/scholar?cites=18261802038517910420&as_sdt=2005&sciodt=0,5&hl=en",
        "url_related_articles": "\/scholar?q=related:lEtd6DL0bv0J:scholar.google.com\/&scioq=vox-fusion:+dense+tracking+and+mapping+with+voxel-based+neural+implicit+representation&hl=en&as_sdt=0,5",
        "eprint_url": "https:\/\/arxiv.org\/pdf\/2210.15858",
        "self-cite": -1,
        "other-cite": -1,
        "total-cite": -1,
        "has-interesting-author": false
    },
    "intrinsicnerf: learning intrinsic neural radiance fields for editable novel view synthesis": {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis",
            "author": "Ye, Weicai and Chen, Shuo and Bao, Chong and Bao, Hujun and Pollefeys, Marc and Cui, Zhaopeng and Zhang, Guofeng",
            "pub_year": "2023",
            "venue": "Proceedings of the …",
            "abstract": "Existing inverse rendering combined with neural rendering methods can only perform editable novel view synthesis on object-specific scenes, while we present intrinsic neural radiance fields, dubbed IntrinsicNeRF, which introduce intrinsic decomposition into the NeRF-based neural rendering method and can extend its application to room-scale scenes. Since intrinsic decomposition is a fundamentally under-constrained inverse problem, we propose a novel distance-aware point sampling and adaptive reflectance iterative clustering",
            "pages": "339--351",
            "booktitle": "Proceedings of the IEEE\/CVF International Conference on Computer Vision",
            "pub_type": "inproceedings",
            "bib_id": "ye2023intrinsicnerf"
        },
        "filled": true,
        "gsrank": 1,
        "pub_url": "http:\/\/openaccess.thecvf.com\/content\/ICCV2023\/html\/Ye_IntrinsicNeRF_Learning_Intrinsic_Neural_Radiance_Fields_for_Editable_Novel_View_ICCV_2023_paper.html",
        "author_id": [
            "qsMRsnsAAAAJ",
            "vlu_3ksAAAAJ",
            "HRHCYq0AAAAJ",
            ""
        ],
        "url_scholarbib": "\/scholar?hl=en&q=info:smeYHnmG5MsJ:scholar.google.com\/&output=cite&scirp=0&hl=en",
        "url_add_sclib": "\/citations?hl=en&xsrf=&continue=\/scholar%3Fq%3Dintrinsicnerf:%2Blearning%2Bintrinsic%2Bneural%2Bradiance%2Bfields%2Bfor%2Beditable%2Bnovel%2Bview%2Bsynthesis%26hl%3Den%26as_sdt%3D0,5&citilm=1&update_op=library_add&info=smeYHnmG5MsJ&ei=s-BmZrjMKp--6rQP8OOGwAY&json=",
        "num_citations": 24,
        "citedby_url": "\/scholar?cites=14692015739151869874&as_sdt=2005&sciodt=0,5&hl=en",
        "url_related_articles": "\/scholar?q=related:smeYHnmG5MsJ:scholar.google.com\/&scioq=intrinsicnerf:+learning+intrinsic+neural+radiance+fields+for+editable+novel+view+synthesis&hl=en&as_sdt=0,5",
        "eprint_url": "https:\/\/openaccess.thecvf.com\/content\/ICCV2023\/papers\/Ye_IntrinsicNeRF_Learning_Intrinsic_Neural_Radiance_Fields_for_Editable_Novel_View_ICCV_2023_paper.pdf",
        "self-cite": 3,
        "other-cite": 21,
        "total-cite": 24,
        "has-interesting-author": true
    },
    "sine: semantic-driven image-based nerf editing with prior-guided editing field": {
        "container_type": "Publication",
        "source": "PUBLICATION_SEARCH_SNIPPET",
        "bib": {
            "title": "Sine: Semantic-driven image-based nerf editing with prior-guided editing field",
            "author": "Bao, Chong and Zhang, Yinda and Yang, Bangbang and Fan, Tianxing and Yang, Zesong and Bao, Hujun and Zhang, Guofeng and Cui, Zhaopeng",
            "pub_year": "2023",
            "venue": "Proceedings of the …",
            "abstract": "Despite the great success in 2D editing using user-friendly tools, such as Photoshop, semantic strokes, or even text prompts, similar capabilities in 3D areas are still limited, either relying on 3D modeling skills or allowing editing within only a few categories. In this paper, we present a novel semantic-driven NeRF editing approach, which enables users to edit a neural radiance field with a single image, and faithfully delivers edited novel views with high fidelity and multi-view consistency. To achieve this goal, we propose a prior-guided editing",
            "pages": "20919--20929",
            "booktitle": "Proceedings of the IEEE\/CVF Conference on Computer Vision and Pattern Recognition",
            "pub_type": "inproceedings",
            "bib_id": "bao2023sine"
        },
        "filled": true,
        "gsrank": 1,
        "pub_url": "http:\/\/openaccess.thecvf.com\/content\/CVPR2023\/html\/Bao_SINE_Semantic-Driven_Image-Based_NeRF_Editing_With_Prior-Guided_Editing_Field_CVPR_2023_paper.html",
        "author_id": [
            "HRHCYq0AAAAJ",
            "F15ODMIAAAAJ",
            "b03C_YgAAAAJ",
            "siv1RXUAAAAJ"
        ],
        "url_scholarbib": "\/scholar?hl=en&q=info:xZDPxp4AYiEJ:scholar.google.com\/&output=cite&scirp=0&hl=en",
        "url_add_sclib": "\/citations?hl=en&xsrf=&continue=\/scholar%3Fq%3Dsine:%2Bsemantic-driven%2Bimage-based%2Bnerf%2Bediting%2Bwith%2Bprior-guided%2Bediting%2Bfield%26hl%3Den%26as_sdt%3D0,5&citilm=1&update_op=library_add&info=xZDPxp4AYiEJ&ei=xOBmZqXMGqOZy9YPjry9-AY&json=",
        "num_citations": 65,
        "citedby_url": "\/scholar?cites=2405485832909590725&as_sdt=2005&sciodt=0,5&hl=en",
        "url_related_articles": "\/scholar?q=related:xZDPxp4AYiEJ:scholar.google.com\/&scioq=sine:+semantic-driven+image-based+nerf+editing+with+prior-guided+editing+field&hl=en&as_sdt=0,5",
        "eprint_url": "https:\/\/openaccess.thecvf.com\/content\/CVPR2023\/papers\/Bao_SINE_Semantic-Driven_Image-Based_NeRF_Editing_With_Prior-Guided_Editing_Field_CVPR_2023_paper.pdf",
        "self-cite": 7,
        "other-cite": 58,
        "total-cite": 65,
        "has-interesting-author": true
    }
}