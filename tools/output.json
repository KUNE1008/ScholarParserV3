[
  {
    "title": "vox-surf: voxel-based implicit surface representation",
    "id": 1,
    "valid_pdf_number": "27/28",
    "matched_pdf_number": "22/27",
    "matched_rate": 0.8148148148148148,
    "citations": {
      "Neuralangelo: High-fidelity neural surface reconstruction": {
        "authors": [
          "Zhaoshuo Li",
          "Thomas Muller",
          "Alex Evans",
          "Russell H. Taylor",
          "Mathias Unberath",
          "Yu Liu",
          "Hsuan Lin"
        ],
        "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Li_Neuralangelo_High-Fidelity_Neural_Surface_Reconstruction_CVPR_2023_paper.pdf",
        "ref_texts": "[18] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022. 2",
        "ref_ids": [
          "18"
        ],
        "1": "Follow-up works extend the above approaches to realtime at the cost of surface fidelity [18, 37], while others [3, 5, 44] use auxiliary information to enhance the reconstruction results."
      },
      "Sine: Semantic-driven image-based nerf editing with prior-guided editing field": {
        "authors": [
          "Chong Bao",
          "Yinda Zhang",
          "Bangbang Yang",
          "Tianxing Fan",
          "Zesong Yang",
          "Hujun Bao",
          "Guofeng Zhang",
          "Zhaopeng Cui"
        ],
        "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Bao_SINE_Semantic-Driven_Image-Based_NeRF_Editing_With_Prior-Guided_Editing_Field_CVPR_2023_paper.pdf",
        "ref_texts": "[31] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022. 2",
        "ref_ids": [
          "31"
        ],
        "1": "Recently, NeRF [42] achieves photo-realistic rendering with volume rendering and inspires many works, including surface reconstruction [31,65, 73], scene editing [4, 18, 67, 70, 71] and generation [22, 51], inverse rendering [5, 76], SLAM [72, 77], etc."
      },
      "Co-slam: Joint coordinate and sparse parametric encodings for neural real-time slam": {
        "authors": [
          "Hengyi Wang",
          "Jingwen Wang",
          "Lourdes Agapito"
        ],
        "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Co-SLAM_Joint_Coordinate_and_Sparse_Parametric_Encodings_for_Neural_Real-Time_CVPR_2023_paper.pdf",
        "ref_texts": "[12] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-Surf: V oxel-based Implicit SurfaceRepresentation. arXiv preprint arXiv:2208.10925 , 2022. 2, 3",
        "ref_ids": [
          "12"
        ],
        "1": "Recent efforts focus on sparse alternatives to these parametric embeddings such as octrees [28], tri-plane [2], hash-grid [15] or sparse voxel grid [12, 13] to improve the memory efficiency of dense grids.",
        "2": "To improve the memory efficiency of parametric encoding-based methods, sparse parametric encodings, such as Octree [28], Tri-plane [2], or sparse voxel grid [12, 13, 15], have been proposed."
      },
      "Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation": {
        "authors": [
          "X Yang",
          "H Li",
          "H Zhai",
          "Y Ming",
          "Y Liu"
        ],
        "url": "https://arxiv.org/pdf/2210.15858",
        "ref_texts": "[13] H. Li, X. Yang, H. Zhai, Y . Liu, H. Bao, and G. Zhang. V ox-surf: V oxelbased implicit surface representation. arXiv preprint arXiv:2208.10925 , 2022.",
        "ref_ids": [
          "13"
        ],
        "1": "Our hybrid scene representation is inspired by recent works that use voxel-based neural implicit representations [13, 14] .",
        "2": "More specifically, we share a similar structure with V ox-Surf [13] which encodes 3D scenes with neural networks and local embeddings.",
        "3": "Unlike prior works where the limit is heuristically specified [13, 14], we dynamically change it according to the specified maximum sampling distance Dmax.",
        "4": "[13] H."
      },
      "Neat: Learning neural implicit surfaces with arbitrary topologies from multi-view images": {
        "authors": [
          "Xiaoxu Meng",
          "Weikai Chen",
          "Bo Yang"
        ],
        "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Meng_NeAT_Learning_Neural_Implicit_Surfaces_With_Arbitrary_Topologies_From_Multi-View_CVPR_2023_paper.pdf",
        "ref_texts": "[23] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , pages 1\u201312, 2022. 2",
        "ref_ids": [
          "23"
        ],
        "1": "Related Work 3D Geometric Representation A 3D surface can be represented explicitly with voxels [5, 11, 23, 32, 44], pointclouds [1, 12, 24, 31, 53], and meshes [16, 47, 50], or can be represented implicitly with neural implicit functions, which have gained popularity for their continuity and the arbitraryresolution property."
      },
      "Gridpull: Towards scalability in learning implicit representations from 3d point clouds": {
        "authors": [
          "Chao Chen",
          "Shen Liu",
          "Zhizhong Han"
        ],
        "url": "https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_GridPull_Towards_Scalability_in_Learning_Implicit_Representations_from_3D_Point_ICCV_2023_paper.pdf",
        "ref_texts": "[38] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-Surf: V oxel-based implicit surface representation. CoRR, abs/2208.10925, 2022. 2",
        "ref_ids": [
          "38"
        ],
        "1": "Some methods [70, 75,96,38] inferred a discrete radiance field defined on grids to speed up the learning of a radiance field, while they cared more about the quality of synthesized views than the underlying geometry."
      },
      "Towards unbiased volume rendering of neural implicit surfaces with geometry priors": {
        "authors": [
          "Yongqiang Zhang",
          "Zhipeng Hu",
          "Haoqian Wu",
          "Minda Zhao",
          "Lincheng Li",
          "Zhengxia Zou",
          "Changjie Fan"
        ],
        "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Towards_Unbiased_Volume_Rendering_of_Neural_Implicit_Surfaces_With_Geometry_CVPR_2023_paper.pdf",
        "ref_texts": "[10] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022. 3",
        "ref_ids": [
          "10"
        ],
        "1": "Others aim to enhance training efficiency via voxel-based representation, such as V oxurf [26] and V ox-Surf [10]\n3."
      },
      "Cp-slam: Collaborative neural point-based slam system": {
        "authors": [
          "J Hu",
          "M Mao",
          "H Bao",
          "G Zhang"
        ],
        "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/7c10e259c7e56fa218ee03d9ae7d728e-Paper-Conference.pdf",
        "ref_texts": "[21] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022.",
        "ref_ids": [
          "21"
        ],
        "1": "Inspired by different representations of neural field including voxel grid [21] and point cloud [43], NICE-SLAM [50] and V ox-Fusion [44] chose voxel grid to perform tracking and mapping instead of a single neural network which is limited by expression ability and forgetting problem."
      },
      "Streetsurf: Extending multi-view implicit surface reconstruction to street views": {
        "authors": [
          "J Guo",
          "N Deng",
          "X Li",
          "Y Bai",
          "B Shi",
          "C Wang"
        ],
        "url": "https://arxiv.org/pdf/2306.04988",
        "ref_texts": "[18] H. Li, X. Yang, H. Zhai, Y . Liu, H. Bao, and G. Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022.",
        "ref_ids": [
          "18"
        ],
        "1": "To improve spatial efficiency, voxel-pruning [10,18,24,48,66], hash-indexing [32], tensordecomposition [4,5], or multi-scale voxels [27] are introduced.",
        "2": "Follow-up works replace the MLP network with local implicit grids [41,56], sparse voxels [18], MLP blocks [8] or displacement fields [58] for better efficiency or local details.",
        "3": "This occupancy information is periodically updated from the close-range SDF network in a bootstrap manner that does not require reconstructing the scene in advance with COLMAP [42] as [46] or feeding LiDAR pointclouds to pre-compute occupied voxels as V oxSurf [18].",
        "4": "[18] H.",
        "5": "2 Optional sky masks Previous research studying multi-view neural reconstruction [18,46] or novel view synthesis [40,49] on outdoor scenes typically require mask annotation or segmentation that marks sky pixels in order to penalize non-occupied areas.",
        "6": "Previous approach studying multi-view reconstruction of outdoor scenes require either reconstructing the scene in advance [46] with COLMAP [42] or feeding LiDAR pointclouds to pre-compute occupied voxels [18]."
      },
      "Mips-fusion: Multi-implicit-submaps for scalable and robust online neural rgb-d reconstruction": {
        "authors": [
          "Y Tang",
          "J Zhang",
          "Z Yu",
          "H Wang",
          "K Xu"
        ],
        "url": "https://arxiv.org/pdf/2308.08741",
        "ref_texts": ""
      },
      "Nero: Neural geometry and brdf reconstruction of reflective objects from multiview images": {
        "authors": [
          "Y Liu",
          "P Wang",
          "C Lin",
          "X Long",
          "J Wang",
          "L Liu"
        ],
        "url": "https://arxiv.org/pdf/2305.17398",
        "ref_texts": ""
      },
      "Neusg: Neural implicit surface reconstruction with 3d gaussian splatting guidance": {
        "authors": [
          "H Chen",
          "C Li",
          "GH Lee"
        ],
        "url": "https://arxiv.org/pdf/2312.00846",
        "ref_texts": "[22] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022. 3",
        "ref_ids": [
          "22"
        ],
        "1": "Further research has aimed to adapt these methods for real-time applications, but compromising on surface accuracy as reported in recent studies [22, 49]."
      },
      "Swift-Mapping: Online Neural Implicit Dense Mapping in Urban Scenes": {
        "authors": [
          "Ke Wu",
          "Kaizhao Zhang",
          "Mingzhe Gao",
          "Jieru Zhao",
          "Zhongxue Gan",
          "Wenchao Ding"
        ],
        "url": "https://ojs.aaai.org/index.php/AAAI/article/download/28420/28820",
        "ref_texts": "2022. Vox-surf: Voxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics.Li, Z.; Li, L.; and Zhu, J. 2023. Read: Large-scale neural scene rendering for autonomous driving. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, 1522\u20131529. Lin, J.; and Zhang, F. 2022. R 3 LIVE: A Robust, Real-time, RGB-colored, LiDAR-Inertial-Visual tightly-coupled state Estimation and mapping package. In 2022 International Conference on Robotics and Automation (ICRA), 10672\u2013",
        "ref_ids": [
          "2022"
        ]
      },
      "Mirror-NeRF: Learning Neural Radiance Fields for Mirrors with Whitted-Style Ray Tracing": {
        "authors": [
          "J Zeng",
          "C Bao",
          "R Chen",
          "Z Dong",
          "G Zhang"
        ],
        "url": "https://arxiv.org/pdf/2308.03280",
        "ref_texts": ""
      },
      "Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview": {
        "authors": [
          "Y Ming",
          "X Yang",
          "W Wang",
          "Z Chen",
          "J Feng"
        ],
        "url": "https://arxiv.org/pdf/2405.05526",
        "ref_texts": "[48] H. Li, X. Yang, H. Zhai, Y. Liu, H. Bao, G. Zhang, Vox-surf: Voxel-based implicit surface representation, IEEE Transactions on Visualization and Computer Graphics (2022) 1\u201312.",
        "ref_ids": [
          "48"
        ],
        "1": "InsteadofencodingtheentirescenewiththeMLPs, Vox-Surf [48] employs a hybrid architecture that consists of an explicit dense voxel grid with the neural implicit surface representation.",
        "2": "55 Vox-Surf[48] 0.",
        "3": "[48] H."
      },
      "PSDF: Prior-Driven Neural Implicit Surface Learning for Multi-view Reconstruction": {
        "authors": [
          "W Su",
          "C Zhang",
          "Q Xu",
          "W Tao"
        ],
        "url": "https://arxiv.org/pdf/2401.12751",
        "ref_texts": "[1] H. Li, X. Yang, H. Zhai, Y. Liu, H. Bao, and G. Zhang, \u201cVox-surf: Voxel-based implicit surface representation,\u201d IEEE Transactions on Visualization and Computer Graphics , 2022.",
        "ref_ids": [
          "1"
        ],
        "1": "Index Terms \u2014Surface reconstruction, volume rendering, surface rendering, multi-view stereo \u2726\n1 I NTRODUCTION SURFACE reconstruction from posed multi-view images is one of the fundamental problems in 3D computer vision [1], [2].",
        "2": "[1] H."
      },
      "Point-NeuS: Point-Guided Neural Implicit Surface Reconstruction by Volume Rendering": {
        "authors": [
          "C Zhang",
          "W Su",
          "W Tao"
        ],
        "url": "https://arxiv.org/pdf/2310.07997",
        "ref_texts": "[16] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022. 2",
        "ref_ids": [
          "16"
        ],
        "1": "Impressively, some methods use voxelbased representation to enhance training efficiency, such as V oxurf [32] and V ox-Surf [16], making significant strides in achieving this equilibrium."
      },
      "Sur2f: A Hybrid Representation for High-Quality and Efficient Surface Reconstruction from Multi-view Images": {
        "authors": [
          "Z Huang",
          "Z Liang",
          "H Zhang",
          "Y Lin",
          "K Jia"
        ],
        "url": "https://arxiv.org/pdf/2401.03704",
        "ref_texts": "[20] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022. 3",
        "ref_ids": [
          "20"
        ],
        "1": "And their follow-up works [3, 9, 10, 20, 22, 46\u201348, 50] make efforts to improve the quality and/or efficiency."
      },
      "LGSDF: Continual Global Learning of Signed Distance Fields Aided by Local Updating": {
        "authors": [
          "Y Yue",
          "Y Deng",
          "J Wang",
          "Y Yang"
        ],
        "url": "https://arxiv.org/pdf/2404.05187"
      },
      "InstantAvatar: Efficient 3D Head Reconstruction via Surface Rendering": {
        "authors": [
          "A Canela",
          "P Caselles",
          "I Malik",
          "GT Garces"
        ],
        "url": "https://arxiv.org/pdf/2308.04868",
        "ref_texts": "[20] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. arXiv preprint arXiv:2208.10925 , 2022. 2",
        "ref_ids": [
          "20"
        ],
        "1": "Still, the convergence time for these approaches remains in the tens of minutes [20], which is prohibitively high for many applications.",
        "2": "Methods based on multi-resolution feature grids have been successfully used in combination with differentiable volume rendering to obtain 3D reconstructions [20, 61]."
      },
      "3QFP: Efficient neural implicit surface reconstruction using Tri-Quadtrees and Fourier feature Positional encoding": {
        "authors": [
          "S Sun",
          "M Mielle",
          "AJ Lilienthal",
          "M Magnusson"
        ],
        "url": "https://arxiv.org/pdf/2401.07164",
        "ref_texts": "[24] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. \u201cV ox-Surf: V oxelBased Implicit Surface Representation\u201d. In: IEEE Transactions on Visualization and Computer Graphics (2022), pp. 1\u201312. DOI:10.1109/TVCG.2022.",
        "ref_ids": [
          "24"
        ],
        "1": "Instead of storing features in 3D voxel grids [20, 23, 24] or dense feature planes [21], we use three planar quadtrees to represent surfaces.",
        "2": "Accounting for the large memory footprint when applying dense feature voxel grids, several techniques have been proposed to reduce memory usage, such as hash-tables [35], octree-trees [16]; these compact data structures have been leveraged in recent robotic applications [20, 18, 19, 26, 24, 23, 36].",
        "3": "To avoid storing unnecessary features in free space, prior work [20, 23, 16, 24] employs octree to store features only within voxel grids where surface points are located."
      },
      "OmniSDF: Scene Reconstruction using Omnidirectional Signed Distance Functions and Adaptive Binoctrees": {
        "authors": [
          "H Kim",
          "A Meuleman",
          "H Jang",
          "J Tompkin"
        ],
        "url": "https://arxiv.org/pdf/2404.00678",
        "ref_texts": "[12] Hai Li, Xingrui Yang, Hongjia Zhai, Yuqian Liu, Hujun Bao, and Guofeng Zhang. V ox-surf: V oxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics , 2022. 2",
        "ref_ids": [
          "12"
        ],
        "1": "Neural implicit representations using SDFs show potential for image-based 3D reconstruction through several works [12, 19, 28, 30, 31]."
      },
      "ImTooth: Neural Implicit Tooth for Dental Augmented Reality": {
        "authors": [
          "Hai Li",
          "Hongjia Zhai",
          "Xingrui Yang",
          "Zhirong Wu",
          "Yihao Zheng",
          "Haofan Wang",
          "Jianchao Wu",
          "Hujun Bao",
          "Guofeng Zhang"
        ],
        "url": "http://www.cad.zju.edu.cn/home/gfzhang/papers/VR-TVCG-2023-ImTooth/ImTooth.pdf",
        "ref_texts": "[27] H. Li, X. Yang, H. Zhai, Y. Liu, H. Bao, and G. Zhang. Vox-Surf: Voxelbased implicit surface representation. IEEE Transactions on Visualization and Computer Graphics, pp. 1\u201312, 2022.",
        "ref_ids": [
          "27"
        ],
        "1": "Different from other methods, we use the plaster teeth models as our reconstruction target and learn a voxelsbased neural implicit representation [27] which is editable and flexible.",
        "2": "To reconstruct highquality results of the large-scale scene, V ox-Surf [27] adopts a sparse voxel structure to divide the spatial regions and store the geometry features in the nodes of the voxel.",
        "3": "To reconstruct the detail of the teeth model, we adopt the idea of voxel-based neural implicit representation proposed in Vox-Surf[27], which is flexible and lightweight compared to other methods [36, 58, 62, 63].",
        "4": "Although V ox-Surf [27] relieves the latter step, an accurate 6 DoF pose is still necessary.",
        "5": "From left to right, we show the RGB image of the plaster model, scanned surface (ground truth), and the surface reconstructed by the COLMAP [46,47], NeuS [58], Vox-Surf [27], our proposed ImTooth, respectively.",
        "6": "We adopt the surfaceaware voxel resampling strategy [27] after 30,000 iterations.",
        "7": "We compare our method with the traditional reconstruction method, COLMAP [46, 47], and the recent implicit surface reconstruction method, NeuS [58], V ox-Surf [27].",
        "8": "[27] H."
      },
      "ShapeMed-Knee: A Dataset and Neural Shape Model Benchmark for Modeling 3D Femurs": {
        "authors": [
          "AA Gatti",
          "L Blankemeier",
          "D Van Veen",
          "B Hargreaves"
        ],
        "url": "https://www.medrxiv.org/content/medrxiv/early/2024/05/07/2024.05.06.24306965.full.pdf",
        "ref_texts": "[23] H. Li, X. Yang, H. Zhai, Y . Liu, H. Bao and G. Zhang, \u201cV ox-Surf: V oxel-Based Implicit Surface Representation,\u201d IEEE Transactions on Visualization and Computer Graphics, pp. 1\u201312, 2022.",
        "ref_ids": [
          "23"
        ],
        "1": "To improve reconstruction of large scenes or fine details, instead of a single global z,a spatially localized zis input into the MLP [23], [24].",
        "2": "[23] H."
      },
      "DF-SLAM: Neural Feature Rendering Based on Dictionary Factors Representation for High-Fidelity Dense Visual SLAM System": {
        "authors": [
          "W Wei",
          "J Wang"
        ],
        "url": "https://arxiv.org/pdf/2404.17876",
        "ref_texts": "[24] H. Li, X. Yang, H. Zhai, Y. Liu, H. Bao, and G. Zhang, \u201cVox-surf: Voxelbased implicit surface representation,\u201d IEEE Transactions on Visualization and Computer Graphics , 2022.",
        "ref_ids": [
          "24"
        ],
        "1": "Subsequent works have adopted hybrid scene representations, encoding scene information into features and anchoring these features onto specific data structures such as octrees [21, 22], voxel grids [23, 24], tri-planes [25], and hash grids [26].",
        "2": "[24] H."
      },
      "TOWARDS VISION-GUIDED SKULL BASE SURGERY": {
        "authors": [
          "Z Li"
        ],
        "url": "https://jscholarship.library.jhu.edu/bitstreams/775f39cc-1280-445b-b371-26f380818de1/download",
        "ref_texts": "132.Li, H., Yang, X., Zhai, H., Liu, Y., Bao, H. & Zhang, G. Vox-Surf: Voxel-based implicit surface representation. IEEE Transactions on Visualization and Computer Graphics (2022).",
        "ref_ids": [
          "132"
        ],
        "1": ", within minutes) performancebutsacrificethesurfacequality[132, 133]."
      },
      "Universal structural map for indoor navigation in university campus": {
        "authors": [
          "P \u015awitalski",
          "A Salamo\u0144czyk"
        ],
        "url": "https://czasopisma.uws.edu.pl/studiainformatica/article/download/3619/3409",
        "ref_texts": "14. Li, H., Yang, X., Zhai, H., Liu, Y., Bao, H.,and Zhang G.: Vox-Surf: Voxel-Based Implicit Surface Representation. In IEEE Transactions on Visualization and Computer Graphics (2022), doi: 10.1109/TVCG.2022.3225844.",
        "ref_ids": [
          "14"
        ],
        "1": "[14] propose a Vox-Surf representation which divides the implicit surface into finite bounded voxels."
      }
    }
  },
  {
    "title": "vox-fusion: dense tracking and mapping with voxel-based neural implicit representation",
    "id": 4,
    "valid_pdf_number": "71/79",
    "matched_pdf_number": "64/71",
    "matched_rate": 0.9014084507042254,
    "citations": {
      "Sine: Semantic-driven image-based nerf editing with prior-guided editing field": {
        "authors": [
          "Chong Bao",
          "Yinda Zhang",
          "Bangbang Yang",
          "Tianxing Fan",
          "Zesong Yang",
          "Hujun Bao",
          "Guofeng Zhang",
          "Zhaopeng Cui"
        ],
        "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Bao_SINE_Semantic-Driven_Image-Based_NeRF_Editing_With_Prior-Guided_Editing_Field_CVPR_2023_paper.pdf",
        "ref_texts": "[72] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022. 2",
        "ref_ids": [
          "72"
        ],
        "1": "Recently, NeRF [42] achieves photo-realistic rendering with volume rendering and inspires many works, including surface reconstruction [31,65, 73], scene editing [4, 18, 67, 70, 71] and generation [22, 51], inverse rendering [5, 76], SLAM [72, 77], etc."
      },
      "Progressively optimized local radiance fields for robust view synthesis": {
        "authors": [
          "Andreas Meuleman",
          "Lun Liu",
          "Chen Gao",
          "Bin Huang",
          "Changil Kim",
          "Min H. Kim",
          "Johannes Kopf"
        ],
        "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Meuleman_Progressively_Optimized_Local_Radiance_Fields_for_Robust_View_Synthesis_CVPR_2023_paper.pdf",
        "ref_texts": "[45] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. arXiv:2210.15858, 2022. 3",
        "ref_ids": [
          "45"
        ],
        "1": "V ox-Fusion [45] and Nice-SLAM [53] achieve good pose estimation but are de-signed for RGB-D inputs and require accurate depth: V oxFusion to allocate a sparse voxel grid and Nice-SLAM to determine where to sample along the ray."
      },
      "vmap: Vectorised object mapping for neural field slam": {
        "authors": [
          "Xin Kong",
          "Shikun Liu",
          "Marwan Taher",
          "Andrew J. Davison"
        ],
        "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Kong_vMAP_Vectorised_Object_Mapping_for_Neural_Field_SLAM_CVPR_2023_paper.pdf",
        "ref_texts": "[40] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-Fusion: Dense tracking andmapping with voxel-based neural implicit representation. In Proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR) , 2022. 2",
        "ref_ids": [
          "40"
        ],
        "1": "To make implicit representation more scalable and efficient, a group of implicit SLAM systems [25, 35, 40, 45, 48] fused neural fields with conventional volumetric representations."
      },
      "Nicer-slam: Neural implicit scene encoding for rgb slam": {
        "authors": [
          "Z Zhu",
          "S Peng",
          "V Larsson",
          "Z Cui",
          "MR Oswald"
        ],
        "url": "https://arxiv.org/pdf/2302.03594",
        "ref_texts": "[67] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022.",
        "ref_ids": [
          "67"
        ],
        "1": "Although follow-up works [67, 30, 21, 18, 25, 39] try to improve upon NICE-SLAM and iMAP from different perspectives, all of these works still rely on the reliable depth input from RGB-D sensors.",
        "2": "Many follow-up works improve upon these two works from various perspectives, including efficient scene representation [18, 21], fast optimziation [67], add IMU measurements [25], or different shape representations [39, 30].",
        "3": "We compare to (a) SOTA neural implicitbased RGB-D SLAM system NICE-SLAM [76] and V oxFusion [67], (b) classic MVS method COLMAP [46], and (c) SOTA dense monocular SLAM system DROIDSLAM [57].",
        "4": "Unlike recent implicit-based dense SLAM systems [51, 76, 67] which use occupancy to implicitly represent scene geometry, we instead use SDFs."
      },
      "Point-slam: Dense neural point cloud-based slam": {
        "authors": [
          "Erik Sandstrom",
          "Yue Li",
          "Luc Van",
          "Martin R. Oswald"
        ],
        "url": "https://openaccess.thecvf.com/content/ICCV2023/papers/Sandstrom_Point-SLAM_Dense_Neural_Point_Cloud-based_SLAM_ICCV_2023_paper.pdf",
        "ref_texts": "[69] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022. 1, 2, 4, 5, 6, 7, 8",
        "ref_ids": [
          "69"
        ],
        "1": "To eliminate the potential domain gap between train and test time, recent SLAM methods rely on test time optimization via volume rendering [53, 69, 79].",
        "2": "18433\n tional approaches, neural scene representations have attractive properties for mapping like improved noise and outlier handling [64], better hole filling and inpainting capabilities for unobserved scene parts [69, 79], and data compression [42, 58].",
        "3": "Like DTAM [37] or BAD-SLAM [48] recent neural SLAM methods [79, 69, 53] only use a single scene representation for both tracking and mapping but they rely either on a regular grid structure [79, 69] or a single MLP [53].",
        "4": "These works have led to full dense SLAM pipelines [69, 79, 53, 28], which represent the current most promising trend towards accurate and robust visual SLAM.",
        "5": "The grid-based representation is perhaps the most explored one and can be further split into methods using dense grids [79, 36, 63, 64, 13, 54, 3, 24, 11, 77, 76, 66, 81], hierarchical octrees [69, 49, 29, 6, 26] and voxel hashing [38, 21, 15, 60, 33] to save memory.",
        "6": "This is in contrast to voxel-based frameworks [79, 69] which need to carve the empty space between the camera and the surface, thus requiring significantly more samples.",
        "7": "We primarily compare our method to existing state-of-the-art dense neural RGBD SLAM methods such as NICE-SLAM [79], V ox-Fusion [69] and ESLAM [28].",
        "8": "We reproduce the results from [69] using the open source code and report the results as V ox-Fusion\u2217.",
        "9": "86 V oxFusion\u2217[69]Depth L1 [cm] \u21931.",
        "10": "77\n(a) Office 0\n Office 3\n Room 0 NICE-SLAM [79] V ox-Fusion\u2217[69] Point-SLAM (ours) Ground Truth (b) Figure 3: Reconstruction Performance on Replica [51].",
        "11": "Office 0\n Room 1\n Room 2 NICE-SLAM [79] V ox-Fusion\u2217[69] Point-SLAM (ours) Ground Truth Figure 4: Rendering Performance on Replica [51] .",
        "12": "06 V ox-Fusion [69] 0.",
        "13": "54 V ox-Fusion\u2217[69] 1.",
        "14": "The grayed numbers of [69] are from the paper that come from a single run which we could not reproduce.",
        "15": "3a compares our method to NICE-SLAM [79], V oxFusion [69] and ESLAM [28] in terms of the geomet-ric reconstruction accuracy.",
        "16": "3b compares the mesh reconstructions of NICE-SLAM [79], V oxFusion [69] and our method to the ground truth mesh.",
        "17": "233 V ox-Fusion\u2217[69]PSNR [dB] \u2191 22.",
        "18": "For NICE-SLAM [79] and V ox-Fusion [69] we take the numbers from [78].",
        "19": "76) V ox-Fusion\u2217[69] 3.",
        "20": "70 V ox-Fusion [69] 8.",
        "21": "57 N/A V ox-Fusion\u2217[69] 68.",
        "22": "NICE-SLAM [79] and V ox-Fusion [69] which employ a large voxel size that leads to more averaging and a reduced sensitivity to specularities.",
        "23": "86 MB V ox-Fusion [69] 12 ms 55 ms 0."
      },
      "Nerf-loam: Neural implicit representation for large-scale incremental lidar odometry and mapping": {
        "authors": [
          "Junyuan Deng",
          "Qi Wu",
          "Xieyuanli Chen",
          "Songpengcheng Xia",
          "Zhen Sun",
          "Guoqing Liu",
          "Wenxian Yu",
          "Ling Pei"
        ],
        "url": "http://openaccess.thecvf.com/content/ICCV2023/papers/Deng_NeRF-LOAM_Neural_Implicit_Representation_for_Large-Scale_Incremental_LiDAR_Odometry_and_ICCV_2023_paper.pdf",
        "ref_texts": "[45] Xingrui Y ang, Hai Li, Hongjia Zhai, Y uhang Ming, Y uqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking andmapping with voxel-based neural implicit representation. In2022 IEEE International Symposium on Mixed and Aug-mented Reality (ISMAR) , pages 499\u2013507, 2022.",
        "ref_ids": [
          "45"
        ],
        "1": "Recently, neural radiance fields (NeRF) [32] has shown promising potentials in representing 3D scenes implicitlyusing a neural network and parallelly pose tracking meth-ods [33, 51, 45].",
        "2": "Compared to the existing 3D representations, the success of neural implicit representation [1, 18, 32, 40, 50] for novelview synthesis attach great attention, and many research in-vestigates the possibility to use this concept realizing simul-taneous localization and mapping (SLAM) [42, 46, 27, 33,51, 45].",
        "3": "Although [45, 50] adoptan octree-based sparse grid with voxel embeddings and canbe applied in larger areas, the pre-allocated embeddings ortime-consuming loop to search the voxels is not available inoutdoor for both odometry and mapping.",
        "4": "Different from existing methods [45, 50], we treat the environments dif-ferently when optimizing the SDF values, e.",
        "5": "Although utilizing the code, the pre-allocate em-beddings [34, 45] or time-consuming one by one search inhash table [50] is not suitable for our task, especially whenit needs to retrieve hundreds of thousands of embeddingsfrom a hash table containing millions of entries."
      },
      "Pats: Patch area transportation with subdivision for local feature matching": {
        "authors": [
          "Junjie Ni",
          "Yijin Li",
          "Zhaoyang Huang",
          "Hongsheng Li",
          "Hujun Bao",
          "Zhaopeng Cui",
          "Guofeng Zhang"
        ],
        "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Ni_PATS_Patch_Area_Transportation_With_Subdivision_for_Local_Feature_Matching_CVPR_2023_paper.pdf",
        "ref_texts": "[62] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In IEEE International Symposium on Mixed and Augmented Reality , pages 499\u2013507. IEEE, 2022.",
        "ref_ids": [
          "62"
        ],
        "1": "In the past decades, local feature matching [3, 40] has been widely used in a large number of applications such as structure from motion (SfM) [44, 64], simultaneous localization and mapping (SLAM) [30,36,62], visual localization [19,41], object pose estimation [22, 61], etc."
      },
      "Recent advances in 3d gaussian splatting": {
        "authors": [
          "T Wu",
          "YJ Yuan",
          "LX Zhang",
          "J Yang",
          "YP Cao"
        ],
        "url": "https://arxiv.org/pdf/2403.11134",
        "ref_texts": "[201] Yang X, Li H, Zhai H, Ming Y, Liu Y, Zhang G. Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit Representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , 2022, 499\u2013",
        "ref_ids": [
          "201"
        ],
        "1": "23 Vox-Fusion [201] 24.",
        "2": "[201] Yang X, Li H, Zhai H, Ming Y, Liu Y, Zhang G."
      },
      "Cp-slam: Collaborative neural point-based slam system": {
        "authors": [
          "J Hu",
          "M Mao",
          "H Bao",
          "G Zhang"
        ],
        "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/7c10e259c7e56fa218ee03d9ae7d728e-Paper-Conference.pdf",
        "ref_texts": "[44] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V oxfusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022.",
        "ref_ids": [
          "44"
        ],
        "1": "Very recently, some methods [36,50,44] exploit the Neural Radiance Fields (NeRF) for dense visual SLAM in a rendering-based optimization framework showing appealing rendering quality in novel view.",
        "2": "Inspired by different representations of neural field including voxel grid [21] and point cloud [43], NICE-SLAM [50] and V ox-Fusion [44] chose voxel grid to perform tracking and mapping instead of a single neural network which is limited by expression ability and forgetting problem.",
        "3": "In the single-agent experiment, because we use the rendered loop-closure data, we primarily choose the state-of-the-art neural SLAM systems such as NICE-SLAM [50], V ox-Fusion [44] and ORB-SLAM3 [4] for comparison on the loop-closure dataset.",
        "4": "71 V ox-Fusion [44]RMSE [cm]\u2193 0.",
        "5": "V ox-Fusion [44] has incorporated an important modification, implementing a sparse grid that is tailored to the specific scene instead of a dense grid.",
        "6": "4 present a quantitative analysis of the geometric reconstruction produced by our proposed system in comparison to NICE-SLAM [50] and V ox-Fusion [44].",
        "7": "33 V ox-Fusion [44]Depth L1 [cm]\u2193 0.",
        "8": "88MB V ox-Fusion [44] 0."
      },
      "Mips-fusion: Multi-implicit-submaps for scalable and robust online neural rgb-d reconstruction": {
        "authors": [
          "Y Tang",
          "J Zhang",
          "Z Yu",
          "H Wang",
          "K Xu"
        ],
        "url": "https://arxiv.org/pdf/2308.08741",
        "ref_texts": "2022. Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit Representation. arXiv preprint arXiv:2210.15858 (2022). Yijun Yuan and Andreas N\u00fcchter. 2022. An algorithm for the SE (3)-transformation on neural implicit maps for remapping functions. IEEE Robotics and Automation Letters 7, 3 (2022), 7763\u20137770. Jiazhao Zhang, Yijie Tang, He Wang, and Kai Xu. 2022. ASRO-DIO: Active Subspace Random Optimization Based Depth Inertial Odometry. IEEE Transactions on Robotics (2022). Jiazhao Zhang, Chenyang Zhu, Lintao Zheng, and Kai Xu. 2020. Fusion-aware point convolution for online semantic 3d scene segmentation. In Proc. CVPR . 4534\u20134543. Jiazhao Zhang, Chenyang Zhu, Lintao Zheng, and Kai Xu. 2021. ROSEFusion: random optimization for online dense reconstruction under fast camera motion. ACM Trans. on Graph. (SIGGRAPH) 40, 4 (2021), 1\u201317. Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Martin R Oswald, and Marc Pollefeys. 2022. Nice-slam: Neural implicit scalable encoding for slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 12786\u201312796. ACM Trans. Graph., Vol. 40, No. 4, Article 56. Publication date: August 2023.",
        "ref_ids": [
          "2022"
        ],
        "1": "[2022] propose a unique mapping scheme based on on-the-fly implicits of Hermite Radial Basis Functions (HRBFs) demonstrating good accuracy and robustness of RGB-D reconstruction.",
        "2": "[2022] propose to represent scene surface using an implicit TSDF and incorporate this representation in the NeRF framework for rendering-based learning.",
        "3": "[2022] propose a geometric and photometric 3D mapping pipeline from monocular images based on hierarchical volumetric neural radiance fields.",
        "4": "Yuan and N\u00fcchter [2022] propose an algorithm for the \ud835\udc46\ud835\udc38(3)-transformation of neural implicit maps for remapping in loop closure."
      },
      "Compact 3d gaussian splatting for dense visual slam": {
        "authors": [
          "T Deng",
          "Y Chen",
          "L Zhang",
          "J Yang",
          "S Yuan"
        ],
        "url": "https://arxiv.org/pdf/2403.11247",
        "ref_texts": "40. Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., Zhang, G.: Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507",
        "ref_ids": [
          "40"
        ],
        "1": "Vox-Fusion [40] employs octree architecture for dynamic map scalability.",
        "2": "R0 R1 R2 Of0 Of1 Of2 Of3 Of4 Vox-Fusion [40] 3.",
        "3": "0000 0059 0106 0169 0181 0207 Vox-Fusion [40] 26.",
        "4": "We also compared to other NeRF-based SLAM methods, such as NICE-SLAM [45], Co-SLAM [34], ESLAM [11], Vox-Fusion [40].",
        "5": "87 Vox-Fusion [40] 11.",
        "6": "as Vox-Fusion [40], NICE-SLAM [45], Co-SLAM [34], and ESLAM [11].",
        "7": "R0 R1 R2 Of0 Of1 Of2 Of3 Of4 Vox-Fusion [40]PSNR\u2191 SSIM\u2191 LPIPS \u219324."
      },
      "Learning neural implicit through volume rendering with attentive depth fusion priors": {
        "authors": [
          "P Hu",
          "Z Han"
        ],
        "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/68637ee6b30276f900bc67320466b69f-Paper-Conference.pdf",
        "ref_texts": "[83] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , Dec 2022.",
        "ref_ids": [
          "83"
        ]
      },
      "UncLe-SLAM: Uncertainty Learning for Dense Neural SLAM": {
        "authors": [
          "Erik Sandstrom",
          "Kevin Ta",
          "Luc Van",
          "Martin R. Oswald"
        ],
        "url": "https://openaccess.thecvf.com/content/ICCV2023W/UnCV/papers/Sandstrom_UncLe-SLAM_Uncertainty_Learning_for_Dense_Neural_SLAM_ICCVW_2023_paper.pdf",
        "ref_texts": "[78] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking andmapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Aug-mented Reality (ISMAR), pages 499\u2013507. IEEE, 2022. 1, 6,7,8",
        "ref_ids": [
          "78"
        ],
        "1": "Introduction Neural scene representations have taken over the 3D reconstruction field by storm [47, 41,12,42] and have recently also been built into SLAM systems [67,81,78] with excellent results for geometric reconstruction, hole filling,and novel view synthesis.",
        "2": "However, their camera track-ing performance is typically inferior to the one of tradi-tional sparse methods [9] that rely on feature point matching [81, 78].",
        "3": "Currently, the majority of dense neural SLAM approaches employ a uniformweighting for all pixels during mapping [81, 78,37,80] and tracking [81, 78,67,80].",
        "4": "In the multi-sensor setting, we also compare to V oxFusion [78] by weighting all depth readings equally.",
        "5": "We compare to V ox-Fusion [78], a dense neural SLAM system and SenFuNet [58], which is a mapping only frame-work.",
        "6": "In Table 5we show for SGM+PSMNet fusion that we are able to consistentlyimprove over the single-sensor reconstructions in isolationand over SenFuNet [58] and V oxFusion [78].",
        "7": "22 V ox-Fusion [78] 6."
      },
      "H-Mapping: Real-time Dense Mapping Using Hierarchical Hybrid Representation": {
        "authors": [
          "C Jiang",
          "H Zhang",
          "P Liu",
          "Z Yu",
          "H Cheng"
        ],
        "url": "https://arxiv.org/pdf/2306.03207",
        "ref_texts": "[15] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "15"
        ],
        "1": "Several works [13]\u2013[15] employ NeRF to overcome limitations associated with explicit representations and achieve better mapping results in various aspects.",
        "2": "Our method avoids redundant sample calculations across all keyframes [13] and ensures quality in marginal areas, without increasing the number of training samples [15].",
        "3": "Numerous studies [13]\u2013[15, 19] have been inspired by NeRF [12] and utilize implicit representation for incremental dense mapping.",
        "4": "V ox-Fusion [15], instead, only allocates voxels to the area containing the surface, forcing the network to learn more details in those regions.",
        "5": "V ox-Fusion [15] adds a new keyframe based on the ratio of newly allocated voxels to the currently observed voxels.",
        "6": "SDF-based Volume rendering Like V ox-Fusion [15], we only sample points along the ray that intersects with any voxel.",
        "7": "Optimization Process 1) Loss Function: We apply loss functions like V ox-Fusion [15]: RGB Loss (Lrgb), Depth Loss (Ld), Free Space Loss (Lfs) and SDF Loss (Lsd f) on a batch of rays R.",
        "8": "C OMPARED WITH NICE-SLAM [14] AND VOX-FUSION [15], OUR APPROACH YIELDS BETTER RESULTS IN ALL THE METRICS .",
        "9": "2) Baselines: We select two advanced NeRF-based dense RGB-D SLAM methods currently open-source, NICE-SLAM\n[14] and V ox-Fusion [15] for comparison.",
        "10": "[15] X."
      },
      "Neural implicit dense semantic slam": {
        "authors": [
          "Y Haghighi",
          "S Kumar",
          "JP Thiran",
          "L Van Gool"
        ],
        "url": "https://arxiv.org/pdf/2304.14560",
        "ref_texts": "[21] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.[22] J. L. Schonberger and J.-M. Frahm, \u201cStructure-from-motion revisited,\u201d inProceedings of the IEEE conference on computer vision and pattern recognition , 2016, pp. 4104\u20134113.",
        "ref_ids": [
          "21",
          "22"
        ],
        "1": "216V ox-Fusion [21]PSNR [dB]\u2191 23.",
        "2": "247COLMAP [22]PSNR [dB]\u2191 20.",
        "3": "[21] X.",
        "4": "[22] J."
      },
      "Rgb-d mapping and tracking in a plenoxel radiance field": {
        "authors": [
          "Andreas L. Teigen",
          "Yeonsoo Park",
          "Annette Stahl",
          "Rudolf Mester"
        ],
        "url": "https://openaccess.thecvf.com/content/WACV2024/papers/Teigen_RGB-D_Mapping_and_Tracking_in_a_Plenoxel_Radiance_Field_WACV_2024_paper.pdf",
        "ref_texts": "[35] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022. 3, 6, 7, 8",
        "ref_ids": [
          "35"
        ],
        "1": "Due to NeRF\u2019s simple formulation of dense mapping and its small storage size, several authors have attempted to use NeRF as a map representation in dense SLAM algorithms [31, 35, 40].",
        "2": "V ox-Fusion [35] Figure 2.",
        "3": "Both of these subsets have been the standard for comparison in previous works [31, 35, 40].",
        "4": "Therefore, to draw comparisons with existing approaches, we select NICE-SLAM [40], and V ox-Fusion [35] as competing methods due to their status as state-of-the-art algorithms for radiance field-based simultaneous localization and mapping.",
        "5": ")(ms) V ox-Fusion [35]ATE [m]\u2193 0.",
        "6": "(m/pixel) \u2193 V ox-Fusion [35] 19.",
        "7": ")(ms) V ox-Fusion [35]ATE [m]\u2193 0."
      },
      "Gaussian splatting slam": {
        "authors": [
          "H Matsuki",
          "R Murai",
          "PHJ Kelly",
          "AJ Davison"
        ],
        "url": "https://arxiv.org/pdf/2312.06741",
        "ref_texts": "[45] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In Proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR) , 2022.",
        "ref_ids": [
          "45"
        ],
        "1": "In the RGBD case, we compare against neural-implicit SLAM methods [8, 9, 29, 35, 41, 45, 48] which are also map-centric, rendering-based and do not perform loop closure.",
        "2": "07 V ox-Fusion [45] 3.",
        "3": "07 V ox-Fusion [45] 1.",
        "4": "54 V ox-Fusion[45] 24.",
        "5": "For the RGB-D case, numbers for NICE-SLAM [48], DI-Fusion [8], V ox-Fusion [45], PointSLAM [29] are taken from Point-SLAM [29], and numbers for iMAP [35], BAD-SLAM [31], Kintinous [42], ORBSLAM [21] are from iMAP [35], and ald all the other baselines: ESLAM [9], Co-SLAM [41] are from each individual papers.",
        "6": "233 V ox-Fusion [45]PSNR[dB] \u2191 22.",
        "7": "85 V ox-Fusion [45] 2."
      },
      "Ro-map: Real-time multi-object mapping with neural radiance fields": {
        "authors": [
          "X Han",
          "H Liu",
          "Y Ding",
          "L Yang"
        ],
        "url": "https://arxiv.org/pdf/2304.05735",
        "ref_texts": "[31] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "31"
        ],
        "1": "Subsequent works [29], [30] have made further improvements, including the integration with traditional voxel grids [31] and different shape representations [32].",
        "2": "[31] X."
      },
      "Loner: Lidar only neural representations for real-time slam": {
        "authors": [
          "S Isaacson",
          "PC Kung",
          "M Ramanagopal"
        ],
        "url": "https://arxiv.org/pdf/2309.04937",
        "ref_texts": "[22] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality , 2022, pp. 499\u2013507.",
        "ref_ids": [
          "22"
        ],
        "1": "Recently, several more papers have introduced architectures and encodings to improve neuralimplicit SLAM\u2019s memory efficiency, computation speed, and accuracy [19, 20, 21, 22].",
        "2": "[22] X."
      },
      "PIN-SLAM: LiDAR SLAM Using a Point-Based Implicit Neural Representation for Achieving Global Map Consistency": {
        "authors": [
          "Y Pan",
          "X Zhong",
          "L Wiesmann",
          "T Posewsky"
        ],
        "url": "https://arxiv.org/pdf/2401.09101",
        "ref_texts": "[101] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In Proc. of the Intl. Symposium on Mixed and Augmented Reality (ISMAR) , 2022.",
        "ref_ids": [
          "101"
        ],
        "1": "Consequently, several mapping and SLAM systems based on neural implicit representation have been proposed, mainly for RGB-D cameras operating indoor [29], [71], [79], [91], [101], [112] but also for LiDAR sensors operating outdoor [12], [111].",
        "2": "In the realm of mapping and SLAM from a stream of RGB-D data, several works propose to use a single MLP [2], [55], [79] and a scalable hybrid representations combining dense or sparse local latent features and a shallow MLP [26], [29], [71], [91], [92], [101], [112] to model the geometry or radiance field of the scene and optionally track the camera within the scene.",
        "3": "They use an octree-based feature grid [101], [111].",
        "4": "The de facto optimization target in previous RGB-D based neural implicit SLAM approaches [71], [79], [101], [112] is the depth rendering loss, which can be seen as related to the point-to-point metric with projection-based data association.",
        "5": "05 V ox-Fusion [101] 1.",
        "6": "[101] X."
      },
      "Gs-slam: Dense visual slam with 3d gaussian splatting": {
        "authors": [
          "C Yan",
          "D Qu",
          "D Wang",
          "D Xu",
          "Z Wang",
          "B Zhao"
        ],
        "url": "https://arxiv.org/pdf/2311.11700",
        "ref_texts": "[48] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. ISMAR , pages 499\u2013507, 2022. 2, 5, 6, 7, 8",
        "ref_ids": [
          "48"
        ],
        "1": "For example, NICE-SLAM [55] integrates MLPs with multiresolution voxel grids, enabling large scene reconstruction, and V ox-Fusion [48] employs octree expansion for dynamic map scalability, while ESLAM [11] and Point-SLAM [27] utilize tri-planes and neural point clouds respectively to improve the mapping capability.",
        "2": "Following [11, 27, 41, 48, 55], we use 8 scenes from the Replica dataset for localization, mesh reconstruction, and rendering quality comparison.",
        "3": "We compare our method with existing SOTA NeRF-based dense visual SLAM: NICE-SLAM [55], V oxFusion [48], CoSLAM [41], ESLAM [11] and PointSLAM [27].",
        "4": "Our method surpasses iMAP [35], NICE-SLAM [55] and V oxfusion [48], and achieves a comparable performance, average 3.",
        "5": "06 V ox-Fusion\u2217[48] 1.",
        "6": "3 V ox-Fusion\u2217[48] 3.",
        "7": "86 V oxFus ion [48]Depth L1 \u21931.",
        "8": "It is noticeable that GS-SLAM achieves 386 FPS rendering speed on average, which is 100\u00d7faster than the second-best method V ox-Fusion [48].",
        "9": "Room 0 Room 1 Room 2 Office 3NICE-SLAM [55]\n V ox-Fusion [48]\n CoSLAM [41]\n ESLAM [11]\n Ours Ground Truth Figure 4.",
        "10": "48 MB V ox-Fusion [48] 0.",
        "11": "233 V ox-Fusion\u2217[48]PSNR [dB] \u219122.",
        "12": "front-to-back order rather than the volume rendering technique used by current NeRF-Based SLAM [11, 27, 35, 41, 48, 53, 55]."
      },
      "NeRF-VO: Real-Time Sparse Visual Odometry with Neural Radiance Fields": {
        "authors": [
          "J Naumann",
          "B Xu",
          "S Leutenegger",
          "X Zuo"
        ],
        "url": "https://arxiv.org/pdf/2312.13471",
        "ref_texts": "[51] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and 10 mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507, 2022. 2, 3",
        "ref_ids": [
          "51"
        ],
        "1": "Lately, numerous works have aimed at integrating SLAM with neural implicit mapping [24, 34, 38, 45, 51, 57].",
        "2": "Subsequent works aimed at enhancing the scene representation [34, 45, 51], introducing implicit semantic encoding [24], and integrating inertial measurements [22]."
      },
      "Sni-slam: Semantic neural implicit slam": {
        "authors": [
          "S Zhu",
          "G Wang",
          "H Blum",
          "J Liu",
          "L Song"
        ],
        "url": "https://arxiv.org/pdf/2311.11016",
        "ref_texts": "[50] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022. 2, 3, 4, 6, 7",
        "ref_ids": [
          "50"
        ],
        "1": "Following the advantages of implicit representation, NeRF-based SLAM [16, 40, 46, 50, 54] methods have been developed.",
        "2": "V ox-Fusion [50] is based on octree management for incremental mapping.",
        "3": "We utilize feature planes [16] to store features, which saves storage space compared with voxel grid [50, 54].",
        "4": "Another approach [50] utilizes the decoder network to obtain geometric and color information from a single feature.",
        "5": "503 V ox-Fusion [50] 2.",
        "6": "For SLAM accuracy, we compare our method with state-of-the-art NeRF-based dense visual SLAM methods [16, 37, 40, 46, 50, 54].",
        "7": "V ox-Fusion [50] achieves the highest Accuracy (cm) because it only reconstructs observed areas and ignores errors in predicted unseen regions, but this strategy results in nearly worst Completion (cm) andCompletion ratio (%) metrics compared with other NeRF-SLAM methods.",
        "8": "69 V ox-Fusion [50] 8.",
        "9": "87 V ox-Fusion [50] 3.",
        "10": "2M V ox-Fusion [50] 2.",
        "11": "Following previous methods [16, 46, 50, 54], we evaluate tracking accuracy on the ScanNet dataset [5]."
      },
      "A review of visual SLAM for robotics: evolution, properties, and future applications": {
        "authors": [
          "B Al-Tawil",
          "T Hempel",
          "A Abdelrahman"
        ],
        "url": "https://www.frontiersin.org/articles/10.3389/frobt.2024.1347985/pdf",
        "ref_texts": "24, 7048\u20137060. doi: 10.1109/tits.2023.3258526 Wu, W., Guo, L., Gao, H., You, Z., Liu, Y., and Chen, Z. (2022). Yolo-slam: a semantic slam system towards dynamic environment with geometric constraint. NeuralComput. Appl.34, 6011\u20136026. doi: 10.1007/s00521-021-06764-3 Xiao, L., Wang, J., Qiu, X., Rong, Z., and Zou, X. (2019). Dynamic-slam: semantic monocular visual localization and mapping based on deep learning in dynamic environment. RoboticsAut.Syst. 117, 1\u201316. doi: 10.1016/j.robot.2019.03.012 Xu, C., Liu, Z., and Li, Z. (2021). Robust visual-inertial navigation system for low precision sensors under indoor and outdoor environments. Remote Sens. 13, 772. doi:10.3390/rs13040772 Yan, L., Hu, X., Zhao, L., Chen, Y., Wei, P., and Xie, H. (2022). Dgs-slam: a fast and robust rgbd slam in dynamic environments combined by geometric and semantic information. RemoteSens. 14, 795. doi: 10.3390/rs14030795 Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., and Zhang, G. (2022). \u201cVox-fusion: dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) (IEEE), 499\u2013507. Yousif, K., Bab-Hadiashar, A., and Hoseinnezhad, R. (2015). An overview to visual odometry and visual slam: applications to mobile robotics. Intell.Ind.Syst. 1, 289\u2013311. doi:10.1007/s40903-015-0032-7 Zang, Q., Zhang, K., Wang, L., and Wu, L. (2023). An adaptive orb-slam3 system for outdoor dynamic environments. Sensors 23, 1359. doi: 10.3390/s23031359 Zhang, J., Zhu, C., Zheng, L., and Xu, K. (2021a). Rosefusion: random optimization for online dense reconstruction under fast camera motion. ACMTrans.Graph.(TOG)"
      },
      "Semgauss-slam: Dense semantic gaussian splatting slam": {
        "authors": [
          "S Zhu",
          "R Qin",
          "G Wang",
          "J Liu",
          "H Wang"
        ],
        "url": "https://arxiv.org/pdf/2403.07494",
        "ref_texts": "[31] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V oxfusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022.",
        "ref_ids": [
          "31"
        ],
        "1": "Following this, several works [31,32,33,34,35,36,37] introduce more efficient scene representation, such as hash-based feature grid and feature plane, to achieve more accurate SLAM performance.",
        "2": "We compare our method with the existing state of-the-art dense visual SLAM, including NeRF-based SLAM [30,33,32,34,31] and 3DGS-based SLAM [17].",
        "3": "233 V ox-Fusion [31] 2.",
        "4": "30 V ox-Fusion [31] 68."
      },
      "Gaussian-slam: Photo-realistic dense slam with gaussian splatting": {
        "authors": [
          "V Yugay",
          "Y Li",
          "T Gevers",
          "MR Oswald"
        ],
        "url": "https://arxiv.org/pdf/2312.10070",
        "ref_texts": "78. Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., Zhang, G.: Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE",
        "ref_ids": [
          "78"
        ],
        "1": "These efforts have led to the development of comprehensive dense SLAM systems [34,54,63,78,85,88,89], showing a trend in the pursuit of precise and reliable visual SLAM.",
        "2": "They further divide into methods using dense grids [3,9,11,28,44,64,70\u201373,86,87,89], hierarchical octrees [6,30,31,36,57,78] and voxel hashing [13,20,41,46,67] for efficient memory management.",
        "3": "We primarily compare our method to existing state-ofthe-art dense neural RGBD SLAM methods such as NICE-SLAM [89], VoxFusion [78], ESLAM [34], and Point-SLAM [53].",
        "4": "233 Vox-Fusion [78]PSNR\u219122.",
        "5": "441 Vox-Fusion [78]PSNR\u2191 15.",
        "6": "548 Vox-Fusion [78]PSNR\u219119.",
        "7": "7 we compare our method to NICESLAM [89], Vox-Fusion [78], ESLAM [34], Point-SLAM [53], and concurrent SplaTAM [23] in terms of the geometric reconstruction accuracy on the Replica dataset [59].",
        "8": "95 Vox-Fusion [78] 0.",
        "9": "3 Vox-Fusion [78] 3.",
        "10": "70 Vox-Fusion [78] 68.",
        "11": "9 Vox-Fusion [78]Depth L1 [cm] \u21931.",
        "12": "64 Vox-Fusion [78] 98 1."
      },
      "A survey on 3d gaussian splatting": {
        "authors": [
          "G Chen",
          "W Wang"
        ],
        "url": "https://arxiv.org/pdf/2401.03890",
        "ref_texts": "[196] X. Yang, H. Li, H. Zhai, Y. Ming, Y. Liu, and G. Zhang, \u201cVoxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , 2022, pp. 499\u2013507.",
        "ref_ids": [
          "196"
        ],
        "1": "58 Vox-Fusion [196] [ISMAR22] 1.",
        "2": "23 Vox-Fusion [196] [ISMAR22]PSNR \u2191 22.",
        "3": "\u2022Benchmarking Algorithms: For performance comparison, we involve four recent papers which introduce 3D Gaussians into their systems [116]\u2013[119], as well as three dense SLAM methods [196], [197], [199].",
        "4": "[196] X."
      },
      "Swift-Mapping: Online Neural Implicit Dense Mapping in Urban Scenes": {
        "authors": [
          "Ke Wu",
          "Kaizhao Zhang",
          "Mingzhe Gao",
          "Jieru Zhao",
          "Zhongxue Gan",
          "Wenchao Ding"
        ],
        "url": "https://ojs.aaai.org/index.php/AAAI/article/download/28420/28820",
        "ref_texts": "6055 Roessle, B.; Barron, J. T.; Mildenhall, B.; Srinivasan, P. P.; and Nie\u00dfner, M. 2022. Dense depth priors for neural radiance fields from sparse input views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12892\u201312901. Rudnev, V.; Elgharib, M.; Smith, W.; Liu, L.; Golyanik, V.; and Theobalt, C. 2022. NeRF for Outdoor Scene Relighting. InEuropean Conference on Computer Vision (ECCV). Straub, J.; Whelan, T.; Ma, L.; Chen, Y.; Wijmans, E.; Green, S.; Engel, J. J.; Mur-Artal, R.; Ren, C.; Verma, S.; Clarkson, A.; Yan, M.; Budge, B.; Yan, Y.; Pan, X.; Yon, J.; Zou, Y.; Leon, K.; Carter, N.; Briales, J.; Gillingham, T.; Mueggler, E.; Pesqueira, L.; Savva, M.; Batra, D.; Strasdat, H. M.; Nardi, R. D.; Goesele, M.; Lovegrove, S.; and Newcombe, R. 2019. The Replica Dataset: A Digital Replica of Indoor Spaces. arXiv preprint arXiv:1906.05797. Sucar, E.; Liu, S.; Ortiz, J.; and Davison, A. J. 2021. iMAP: Implicit mapping and positioning in real-time. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 6229\u20136238. Sun, C.; Sun, M.; and Chen, H.-T. 2022. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5459\u20135469. Takikawa, T.; Litalien, J.; Yin, K.; Kreis, K.; Loop, C.; Nowrouzezahrai, D.; Jacobson, A.; McGuire, M.; and Fidler, S. 2021. Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes. Tancik, M.; Casser, V.; Yan, X.; Pradhan, S.; Mildenhall, B.; Srinivasan, P. P.; Barron, J. T.; and Kretzschmar, H. 2022. Block-nerf: Scalable large scene neural view synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 8248\u20138258. Xie, Z.; Zhang, J.; Li, W.; Zhang, F.; and Zhang, L. 2023. Snerf: Neural radiance fields for street views. arXiv preprint arXiv:2303.00749. Yang, X.; Li, H.; Zhai, H.; Ming, Y.; Liu, Y.; and Zhang, G. 2022. Vox-Fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), 499\u2013507. IEEE. Yu, A.; Li, R.; Tancik, M.; Li, H.; Ng, R.; and Kanazawa, A. 2021. Plenoctrees for real-time rendering of neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 5752\u20135761. Zeng, C.; Chen, G.; Dong, Y.; Peers, P.; Wu, H.; and Tong, X. 2023. Relighting Neural Radiance Fields with Shadow and Highlight Hints. In ACM SIGGRAPH 2023 Conference Proceedings, 1\u201311. Zhang, X.; Bi, S.; Sunkavalli, K.; Su, H.; and Xu, Z. 2022. Nerfusion: Fusing radiance fields for large-scale scene reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5449\u20135458. Zhang, X.; Srinivasan, P. P.; Deng, B.; Debevec, P.; Freeman, W. T.; and Barron, J. T. 2021. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. ACM Transactions on Graphics (ToG), 40(6): 1\u201318.Zhang, Y.; Guo, X.; Poggi, M.; Zhu, Z.; Huang, G.; and Mattoccia, S. 2023. Completionformer: Depth completion with convolutions and vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 18527\u201318536. Zhu, Z.; Peng, S.; Larsson, V.; Xu, W.; Bao, H.; Cui, Z.; Oswald, M. R.; and Pollefeys, M. 2022. Nice-slam: Neural implicit scalable encoding for slam. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12786\u201312796. The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)"
      },
      "Ngel-slam: Neural implicit representation-based global consistent low-latency slam system": {
        "authors": [
          "Y Mao",
          "X Yu",
          "K Wang",
          "Y Wang",
          "R Xiong"
        ],
        "url": "https://arxiv.org/pdf/2311.09525",
        "ref_texts": "[29] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "29"
        ],
        "1": "In robotics,neural implicit representations have been used for object tracking and SLAM, enabling the construction of environment maps and estimation of robot or camera positions [5], [4], [27], [28], [29], [30], [31], which are closely related to our work.",
        "2": "[29] X."
      },
      "NeRF in Robotics: A Survey": {
        "authors": [
          "G Wang",
          "L Pan",
          "S Peng",
          "S Liu",
          "C Xu",
          "Y Miao"
        ],
        "url": "https://arxiv.org/pdf/2405.01333",
        "ref_texts": "[22] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in ISMAR , 2022, pp. 499\u2013507.",
        "ref_ids": [
          "22"
        ],
        "1": "V ox-Fusion [22] uses a treelike structure to store grid embeddings, allowing the dynamic allocation of new spatial voxels as the scene expands.",
        "2": "V oxFusion [22] employs voxel feature embedding as input, generating RGB and SDF values as output.",
        "3": "[22] X."
      },
      "Towards open world nerf-based slam": {
        "authors": [
          "D Lisus",
          "C Holmes",
          "S Waslander"
        ],
        "url": "https://arxiv.org/pdf/2301.03102",
        "ref_texts": "[18] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV ox-Fusion: Dense Tracking and Mapping with V oxel-based Neural Implicit Representation,\u201d in IEEE Int. Symp. Mixed Augmented Reality (ISMAR) , 2022, pp. 499\u2013507.",
        "ref_ids": [
          "18"
        ],
        "1": "One future avenue of interest is to leverage ideas in [18] to avoid using a predefined grid.",
        "2": "[18] X."
      },
      "GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image": {
        "authors": [
          "C Bao",
          "Y Zhang",
          "Y Li",
          "X Zhang",
          "B Yang",
          "H Bao"
        ],
        "url": "https://arxiv.org/pdf/2404.02152",
        "ref_texts": "[62] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022. 2",
        "ref_ids": [
          "62"
        ],
        "1": "Neural Radiance Field [33] has exhibited great reconstruction and rendering qualities in SLAM [62, 73], scene editing [5, 58\u201360, 64] and relighting [63, 66, 67], especially promoting the emergence of many 3D avatar reconstruction [4, 16, 53, 68, 69, 76] and generation [50, 52, 54]."
      },
      "Neural Radiance Field in Autonomous Driving: A Survey": {
        "authors": [
          "L He",
          "L Li",
          "W Sun",
          "Z Han",
          "Y Liu",
          "S Zheng"
        ],
        "url": "https://arxiv.org/pdf/2404.13816",
        "ref_texts": "[60] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV ox-fusion: Dense tracking and mapping with voxelbased neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "60"
        ],
        "1": "Furthermore, V ox-Fusion[60] incrementally allocates voxels by an octree-based structure without a pre-trained geometry decoder and proposes a keyframe selection strategy suitable for sparse voxels, resulting in better performance than NICE-SLAM on the Replica dataset in both tracking and mapping.",
        "2": "[60] X."
      },
      "Towards real-time scalable dense mapping using robot-centric implicit representation": {
        "authors": [
          "J Liu",
          "H Chen"
        ],
        "url": "https://arxiv.org/pdf/2306.10472",
        "ref_texts": ""
      },
      "Q-slam: Quadric representations for monocular slam": {
        "authors": [
          "C Peng",
          "C Xu",
          "Y Wang",
          "M Ding",
          "H Yang"
        ],
        "url": "https://arxiv.org/pdf/2403.08125",
        "ref_texts": "44. Yang, X., Li, H., Zhai, H., Ming, Y ., Liu, Y ., Zhang, G.: V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022)",
        "ref_ids": [
          "44"
        ],
        "1": "87 Vox-Fusion [44]\n(GT depth)PSNR \u2191 22.",
        "2": "92s V ox-Fusion [44] 12.",
        "3": "It can be observed that our method can achieve comparable speed with V ox-Fusion [44], but providing much higher rendering quality as shown in Tab 1."
      },
      "NeuV-SLAM: Fast Neural Multiresolution Voxel Optimization for RGBD Dense SLAM": {
        "authors": [
          "W Guo",
          "B Wang",
          "L Chen"
        ],
        "url": "https://arxiv.org/pdf/2402.02020",
        "ref_texts": "[17] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "17"
        ],
        "1": "Recent research, as indicated in references [17], [18], has experimented with the use of voxel representations for scene depiction and dynamic management through octree structures to achieve real-time expansion and reconstruction of unknownarXiv:2402.",
        "2": "For instance, the V ox-Fusion [17] method attempts to store neural features directly in voxel vertices to accelerate the optimization process of the scene, but this approach still relies on larger-scale neural networks for accurate scene representation.",
        "3": "V ox-Fusion [17] integrates neural implicit representations with traditional volume fusion methods, utilizing voxel-based neural implicit surface representations to encode and optimize scenes within each voxel.",
        "4": "Multiresolution Voxel Generation and Management 1) Generation: Contrary to the [14], [17] approach, which employs a single-resolution voxel, we utilize multiresolution voxels based on scene details to represent the scene.",
        "5": "2) Neural Multiresolution Voxel Representation: Diverging from V ox-Fusion [17] and DVGO [74] methodologies, our voxel grid representation employs trilinear interpolation for simultaneous modeling of SDF and color features within voxel cells, which enhances precision in querying any space position, significantly boosting scene convergence efficiency: interp (x,V(D),V(S)) :x\u2208R3,V(D),V(S)\u2208RA\u00d7N.",
        "6": "(5) In contrast to existing methods like Point-SLAM [19] and V ox-Fusion [17], which rely on neural networks to concurrently regress both identity and RGB values during their processing phases, this study introduces a more rapid and streamlined implicit representation approach, named VDF .",
        "7": "(8) Then, we adopt a volumetric rendering technique similar to the V ox-Fusion [17] to compute and render depth Dand color JOURNAL OF L ATEX CLASS FILES, VOL.",
        "8": "2) Baseline: For our comparative analysis, we employed iMAP [13], V ox-Fusion [17], NICE-SLAM [14], and DIFusion [76] as our baseline methods.",
        "9": "In addition, The codes for tracking and mapping evaluation are both from V ox-Fusion [17].",
        "10": "THE DATA OF I MAP, V OX-FUSION ARE FROM [17], V OX-FUSION *AND NICE-SLAM ARE IMPLEMENTED BY THE OPEN -SOURCE CODE .",
        "11": "We conducted a quantitative comparison of our system with NICE-SLAM [14] and V ox-Fusion [17].",
        "12": "THE DATA OF I MAP, NICE-SLAM, AND VOX-FUSION ARE FROM [17].",
        "13": "Mapping We performed a qualitative comparison of our system with NICE-SLAM [14] and V ox-Fusion [17].",
        "14": "THE DATA OF VOX-FUSION ARE FROM [17].",
        "15": "THE DATA OF NICE-SLAM AND VOX-FUSION ARE FROM [17].",
        "16": "[17] X."
      },
      "Hi-Map: Hierarchical Factorized Radiance Field for High-Fidelity Monocular Dense Mapping": {
        "authors": [
          "T Hua",
          "H Bai",
          "Z Cao",
          "M Liu",
          "D Tao",
          "L Wang"
        ],
        "url": "https://arxiv.org/pdf/2401.03203",
        "ref_texts": "[14] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "14"
        ],
        "1": "With the advent of Neural Radiance Fields (NeRF) [11], several research attempts [12], [13], [14], [15], [16] leverage neural field to better represent the scene by encoding the appearance and geometry in a compact and learnable way, benefiting both memory consumption and mapping quality.",
        "2": "The Neural Radiance Field [11], a novel approach rooted in Implicit Neural Representation (INR) combined with volume rendering techniques, has inspired substantial implicit dense mapping [12], [13], [14], [15], [16], [22], [23], [24], [25], [26], resulting in higher reconstruction quality with more compact representation.",
        "3": ", scalability and computational efficiency [13], [14], [15], [16].",
        "4": "01245\n[14] X."
      },
      "SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM": {
        "authors": [
          "M Li",
          "S Liu",
          "H Zhou"
        ],
        "url": "https://arxiv.org/pdf/2402.03246",
        "ref_texts": "41. Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., Zhang, G.: Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022)",
        "ref_ids": [
          "41"
        ],
        "1": "Theseexperimentscompareour method against both implicit NeRF-based approaches [15,37,41,47], and novel 3D-Gaussian-based methods [16], evaluating performance in mapping, tracking, and semantic segmentation.",
        "2": "We compared our method with Vox-Fusion [41], NICE-SLAM [47], Co-SLAM [37], ESLAM [15], and Point-SLAM [33] for ATE RMSE evaluation."
      },
      "Implicit Event-RGBD Neural SLAM": {
        "authors": [
          "D Qu",
          "C Yan",
          "D Wang",
          "J Yin",
          "D Xu",
          "B Zhao"
        ],
        "url": "https://arxiv.org/pdf/2311.11013",
        "ref_texts": "[72] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In ISMAR , pages 499\u2013507. IEEE, 2022. 2",
        "ref_ids": [
          "72"
        ],
        "1": "V ox-Fusion [72] utilizes an octreebased structure to expand the scene dynamically."
      },
      "NGM-SLAM: Gaussian Splatting SLAM with Radiance Field Submap": {
        "authors": [
          "M Li",
          "J Huang",
          "L Sun",
          "AX Tian",
          "T Deng"
        ],
        "url": "https://arxiv.org/pdf/2405.05702",
        "ref_texts": ""
      },
      "DDN-SLAM: Real-time Dense Dynamic Neural Implicit SLAM with Joint Semantic Encoding": {
        "authors": [
          "M Li",
          "J He",
          "G Jiang",
          "H Wang"
        ],
        "url": "https://arxiv.org/pdf/2401.01545",
        "ref_texts": ""
      },
      "Nid-slam: Neural implicit representation-based rgb-d slam in dynamic environments": {
        "authors": [
          "Z Xu",
          "J Niu",
          "Q Li",
          "T Ren",
          "C Chen"
        ],
        "url": "https://arxiv.org/pdf/2401.01189",
        "ref_texts": "[13] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , 2022.",
        "ref_ids": [
          "13"
        ],
        "1": "Previous methods [2], [5], [13]\u2013\n[16] have demonstrated the feasibility of using neural networks to model the color and geometric information of static scenes; however, they have not fully exploited the potential of neural implicit representations in dynamic environments.",
        "2": "[13] X."
      },
      "Just flip: Flipped observation generation and optimization for neural radiance fields to cover unobserved view": {
        "authors": [
          "M Lee",
          "K Kang",
          "H Yu"
        ],
        "url": "https://arxiv.org/pdf/2303.06335",
        "ref_texts": "[9] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "9"
        ],
        "1": "Due to these features, many recent works [6], [7], [8], [9], [10] have applied NeRF to SLAM and 3D mapping.",
        "2": "To address these limitations, recent research [6], [7], [8], [9], [10] has applied NeRF to SLAM for 3D mapping.",
        "3": "[9] X."
      },
      "SimpleMapping: Real-time visual-inertial dense mapping with deep multi-view stereo": {
        "authors": [
          "Y Xin",
          "X Zuo",
          "D Lu"
        ],
        "url": "https://arxiv.org/pdf/2306.08648",
        "ref_texts": "[60] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang. V ox-Fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pp. 499\u2013507, 2022.",
        "ref_ids": [
          "60"
        ],
        "1": "Furthermore, we showcase the comparable reconstruction performance of SimpleMapping utilizing only a monocular camera setup without IMU, against a state-of-theart RGB-D dense SLAM method with neural implicit representation, V ox-Fusion [60].",
        "2": "Furthermore, we evaluate our approach against V ox-Fusion [60], a RGB-D based dense tracking and mapping system using a voxel based neural implicit representation, on ScanNet [6] test set.",
        "3": "V oxFusion [60] optimizes feature embeddings in voxels and the weights of a MLP decoder on-the-fly with intensive computation, while ours relying on offline training of the MVS network is much more efficient at inference stage.",
        "4": "As shown in Table 6 and Figure 6, our approach consistently outperforms TANDEM [20] and exhibits competitive performance compared to the RGB-D method, V ox-Fusion [60].",
        "5": "Note that Vox-Fusion [60] takes RGB-D inputs.",
        "6": "10 V ox-Fusion [60] 2.",
        "7": "84 V ox-Fusion [60] 18.",
        "8": "60 V ox-Fusion [60] 47.",
        "9": "30 V ox-Fusion [60] 60.",
        "10": "Vox-Fusion [60] tends to produce over-smoothed geometries and experience drift during long-time tracking, resulting in inconsistent reconstruction, as observed in Scene0787.",
        "11": "For example, when applying V ox-Fusion [60] to the ScanNet dataset [6], each frame\u2019s average processing time for tracking amounts to 2.",
        "12": "[60] X."
      },
      "SLAM Meets NeRF: A Survey of Implicit SLAM Methods": {
        "authors": [
          "Kaiyun Yang",
          "Yunqi Cheng",
          "Zonghai Chen",
          "Jikai Wang"
        ],
        "url": "https://www.mdpi.com/2032-6653/15/3/85/pdf",
        "ref_texts": "35. Yang, X.; Li, H.; Zhai, H.; Ming, Y.; Liu, Y.; Zhang, G. Vox-Fusion: Dense tracking and mapping with voxel-based neural implicit representation. In Proceedings of the 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), Singapore, 17\u201321 October 2022; pp. 499\u2013507.",
        "ref_ids": [
          "35"
        ],
        "1": "To address this problem, Vox-Fusion [35] dynamically allocates new voxels by using an explicit octree structure and encodes the voxel coordinates by Morton coding to improve the voxel retrieval speed.",
        "2": "Method Name YearUtilized Sensors Decoded Parameters RGB-D RGB LiDAR SDF Density Color NICE-SLAM [11] 2022 \u2713 \u2713 \u2713 Vox-Fusion [35] 2022 \u2713 \u2713 \u2713 NICER-SLAM [34] 2023 \u2713 \u2713 \u2713 Co-SLAM [12] 2023 \u2713 \u2713 \u2713 LONER [38] 2023 \u2713 \u2713 \u2713 Shine-mapping [39] 2023 \u2713 \u2713 \u2713 NF-Atlas [41] 2023 \u2713 \u2713 \u2713 LODE [42] 2023 \u2713 \u2713 \u2713 NeRF-LOAM [45] 2023 \u2713 \u2713 \u2713 LocNDF [11] 2023 \u2713 \u2713 \u2713 C."
      },
      "Continuous Pose for Monocular Cameras in Neural Implicit Representation": {
        "authors": [
          "Q Ma",
          "DP Paudel",
          "A Chhatkuli",
          "L Van Gool"
        ],
        "url": "https://arxiv.org/pdf/2311.17119",
        "ref_texts": "[61] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022. 8",
        "ref_ids": [
          "61"
        ],
        "1": "7 Method Rm 0 Rm 1 Rm 2 Off 0 Off 1 Off 2 Off 3 Off 4 Avg V ox-Fusion* [61] 1.",
        "2": "89 V ox-Fusion* [61] 68.",
        "3": "1 V ox-Fusion* [61] 3."
      },
      "Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians": {
        "authors": [
          "E Sandstr\u00f6m",
          "K Tateno",
          "M Oechsle",
          "M Niemeyer"
        ],
        "url": "https://arxiv.org/pdf/2405.16544",
        "ref_texts": "[72] Yang, X., Li, H., Zhai, H., Ming, Y ., Liu, Y ., Zhang, G.: V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022)",
        "ref_ids": [
          "72"
        ],
        "1": "1 Introduction A common factor within the recent trend of dense SLAM is that the majority of works reconstruct a dense map by optimizing a neural implicit encoding of the scene, either as weights of an MLP [1, 57,39,45], as features anchored in dense grids [82,42,66,67,58,3,29,83,51], using hierarchical octrees [72], via voxel hashing [79,78,8,49,40], point clouds [18,50,30,75] or axis-aligned feature planes [33,47].",
        "2": "Enhancements like voxel hashing [43,23,44,11,40] and octrees [53,72,37,5,31] improved scalability, while point-based SLAM [68,52,4,23,25,6,76,50,30,75] has also been effective.",
        "3": "10891 (2024)\n[72] Yang, X."
      },
      "NeRF-Guided Unsupervised Learning of RGB-D Registration": {
        "authors": [
          "Z Yu",
          "Z Qin",
          "Y Tang",
          "Y Wang",
          "R Yi",
          "C Zhu"
        ],
        "url": "https://arxiv.org/pdf/2405.00507",
        "ref_texts": "33. Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., Zhang, G.: Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022) 5",
        "ref_ids": [
          "33"
        ],
        "1": "3 Pose Optimization in Neural SLAM Existing Neural SLAM methods [19,26,28,30,33,40,41] incorporate neural implicit representations into RGB-D SLAM systems, allowing tracking and mapping from scratch.",
        "2": "In the subsequent works, NICESLAM [41] and Vox-Fusion [33] introduce a hybrid representation that combines learnable grid-based features with a neural decoder, enabling the utilization of local scene color and geometry to guide pose optimization."
      },
      "Benchmarking Neural Radiance Fields for Autonomous Robots: An Overview": {
        "authors": [
          "Y Ming",
          "X Yang",
          "W Wang",
          "Z Chen",
          "J Feng"
        ],
        "url": "https://arxiv.org/pdf/2405.05526",
        "ref_texts": "[196] X. Yang, H. Li, H. Zhai, Y. Ming, Y. Liu, G. Zhang, Voxfusion:Densetrackingandmappingwithvoxel-basedneuralimplicit Ming et al.: Preprint submitted to Elsevier Page 29 of 32 Benchmarking NeRF for Autonomous Robots representation, in: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "196"
        ],
        "1": "Vox-Fusion [196] proposes a hybrid SLAM system that blends voxel-based mapping with neural implicit networks for efficient, detailed environment reconstruction using SDF representation.",
        "2": "66 Vox-Fusion [196] 2.",
        "3": "52 Vox-Fusion [196] 0.",
        "4": "97 Vox-Fusion [196] 8.",
        "5": "[196] X."
      },
      "CG-SLAM: Efficient Dense RGB-D SLAM in a Consistent Uncertainty-aware 3D Gaussian Field": {
        "authors": [
          "J Hu",
          "X Chen",
          "B Feng",
          "G Li",
          "L Yang",
          "H Bao"
        ],
        "url": "https://arxiv.org/pdf/2403.16095",
        "ref_texts": "53. Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., Zhang, G.: Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022) 4, 10, 11",
        "ref_ids": [
          "53"
        ],
        "1": "NICE-SLAM [58] chose a fully covered voxel grid to store neural features, while Vox-Fusion [53] further improved this grid to an adaptive size.",
        "2": "We primarily consider state-of-the-art NeRF-SLAM works, includingNICE-SLAM[58],Co-SLAM[47],Point-SLAM[34],andVox-Fusion[53], as baselines.",
        "3": "\"-\" indicates failure results in Vox-Fusion [53].",
        "4": "4, we quantitatively measure the mapping performance of our proposed system, in comparison to NICE-SLAM [58], Co-SLAM [47], Point-SLAM [34], and Vox-Fusion [53]."
      },
      "Enhancing Vehicle Aerodynamics with Deep Reinforcement Learning in Voxelised Models": {
        "authors": [
          "J Patel",
          "Y Spyridis",
          "V Argyriou"
        ],
        "url": "https://arxiv.org/pdf/2405.11492",
        "ref_texts": "[12] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "12"
        ],
        "1": "Beyond CFD, voxelisation has found applications in 3D city modeling [11], virtual/augmented reality [12], and 3D printing [13].",
        "2": "[12] X."
      },
      "TAMBRIDGE: Bridging Frame-Centered Tracking and 3D Gaussian Splatting for Enhanced SLAM": {
        "authors": [
          "P Jiang",
          "H Liu",
          "X Li",
          "T Wang",
          "F Zhang"
        ],
        "url": "https://arxiv.org/pdf/2405.19614",
        "ref_texts": "[37] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V oxfusion: Dense tracking and mapping with voxel-based neural implicit representation. In ISMAR , pages 499\u2013507, 2022.",
        "ref_ids": [
          "37"
        ],
        "1": "39 V ox-Fusion [37] 3."
      },
      "Bayesian NeRF: Quantifying Uncertainty with Volume Density in Neural Radiance Fields": {
        "authors": [
          "S Lee",
          "K Kang",
          "H Yu"
        ],
        "url": "https://arxiv.org/pdf/2404.06727",
        "ref_texts": "42. Yang, X., Li, H., Zhai, H., Ming, Y ., Liu, Y ., Zhang, G.: V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022) 3",
        "ref_ids": [
          "42"
        ],
        "1": "NeRF\u2019s application scope has also expanded, encompassing areas such as scene editing [35,44], converting text to 3D models [15,23], and enhancing visual scene-based SLAM technologies [12,24,42,48], demonstrating its versatility and potential in various domains."
      },
      "Gaussian-LIC: Photo-realistic LiDAR-Inertial-Camera SLAM with 3D Gaussian Splatting": {
        "authors": [
          "X Lang",
          "L Li",
          "H Zhang",
          "F Xiong",
          "M Xu",
          "Y Liu"
        ],
        "url": "https://arxiv.org/pdf/2404.06926",
        "ref_texts": "[8] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang. \u201cV ox-Fusion: Dense tracking and mapping with voxel-based neural implicit representation\u201d. In:2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE. 2022, pp. 499\u2013507.",
        "ref_ids": [
          "8"
        ],
        "1": "Further, V ox-Fusion [8] utilizes octree to dynamically expand the volumetric neural implicit map, eliminating the need for pre-allocated grids.",
        "2": "[8] X."
      },
      "Rgbd gs-icp slam": {
        "authors": [
          "S Ha",
          "J Yeon",
          "H Yu"
        ],
        "url": "https://arxiv.org/pdf/2403.12550",
        "ref_texts": "45. Yang, X., Li, H., Zhai, H., Ming, Y ., Liu, Y ., Zhang, G.: V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022) 1",
        "ref_ids": [
          "45"
        ],
        "1": "Various approaches [8, 18, 32, 39, 45, 47] have been attempted to utilize INR for the real-time SLAM mapping process."
      },
      "3QFP: Efficient neural implicit surface reconstruction using Tri-Quadtrees and Fourier feature Positional encoding": {
        "authors": [
          "S Sun",
          "M Mielle",
          "AJ Lilienthal",
          "M Magnusson"
        ],
        "url": "https://arxiv.org/pdf/2401.07164",
        "ref_texts": "[23] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. \u201cV ox-Fusion: Dense Tracking and Mapping with V oxel-based Neural Implicit Representation\u201d. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . 2022, pp. 499\u2013507. DOI:10 . 1109 / ISMAR55827.2022.00066 .",
        "ref_ids": [
          "23"
        ],
        "1": "Instead of storing features in 3D voxel grids [20, 23, 24] or dense feature planes [21], we use three planar quadtrees to represent surfaces.",
        "2": "Accounting for the large memory footprint when applying dense feature voxel grids, several techniques have been proposed to reduce memory usage, such as hash-tables [35], octree-trees [16]; these compact data structures have been leveraged in recent robotic applications [20, 18, 19, 26, 24, 23, 36].",
        "3": "To avoid storing unnecessary features in free space, prior work [20, 23, 16, 24] employs octree to store features only within voxel grids where surface points are located."
      },
      "H3-Mapping: Quasi-Heterogeneous Feature Grids for Real-time Dense Mapping Using Hierarchical Hybrid Representation": {
        "authors": [
          "C Jiang",
          "Y Luo",
          "B Zhou",
          "S Shen"
        ],
        "url": "https://arxiv.org/pdf/2403.10821",
        "ref_texts": "[4] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE Int. Symp. Mixed Augmented Reality . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "4"
        ],
        "1": "To enhance mapping speed and expand representation capacity, various grid representations have been introduced, including dense 3D grids [3], sparse octree grids [4], multiresolution hash grids [6], and factored grids [5].",
        "2": "SDF-based Volume rendering Like V ox-Fusion [4], we only sample points along the ray that intersects with any leaf node voxel of octree.",
        "3": "[4] X."
      },
      "A*\u2013Ant Colony Optimization Algorithm for Multi-Branch Wire Harness Layout Planning": {
        "authors": [
          "Feng Yang",
          "Renjie Zhang",
          "Ping Wang",
          "Shuyu Xing",
          "Zhenlin Wang",
          "Ming Li",
          "Qiang Fang"
        ],
        "url": "https://www.mdpi.com/2079-9292/13/3/529/pdf",
        "ref_texts": "24. Yang, X.; Li, H.; Zhai, H.; Ming, Y.; Liu, Y.; Zhang, G. Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit Representation. In Proceedings of the 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), Singapore, 17\u201321 October 2022; pp. 499\u2013507.",
        "ref_ids": [
          "24"
        ],
        "1": "Here, considering that path planning does not have a strong requirement for strict distance, in order to meet the requirements of subsequent interference detection and other aspects, we consider adopting a method based on implicit representation [24]."
      },
      "DVN-SLAM: Dynamic Visual Neural SLAM Based on Local-Global Encoding": {
        "authors": [
          "W Wu",
          "G Wang",
          "T Deng",
          "S Aegidius",
          "S Shanks"
        ],
        "url": "https://arxiv.org/pdf/2403.11776",
        "ref_texts": "[3]Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022.",
        "ref_ids": [
          "3"
        ],
        "1": "Compared to current NeRF-based SLAM methods, such as iMAP [1], NICE-SLAM\n[2], V ox-Fusion [3], ESLAM[4] and Co-SLAM [5], DVN-SLAM not only achieves competitive performance in static scenes, but also remains effective in high-dynamic scenes.",
        "2": "05 V ox-Fusion [3] 2.",
        "3": "Due to the adoption of an Octree for scene representation in V ox-Fusion [3], the reconstructed results tend to have more holes, resulting in a high accuracy (Acc) but a low completeness ratio.",
        "4": "V ox-Fusion [3] utilizes an octree structure for scene representation, resulting in significant holes in the reconstruction.",
        "5": "4M V ox-Fusion [3] 2.",
        "6": "69 V ox-Fusion [3] 1.",
        "7": "13 V ox-Fusion [3] 2.",
        "8": "87 V ox-Fusion [3] 3.",
        "9": "26 V ox-Fusion [3] 3.",
        "10": "84 V ox-Fusion [3] 1.",
        "11": "71 V ox-Fusion [3] 3.",
        "12": "98 V ox-Fusion [3] 1.",
        "13": "82 V ox-Fusion [3] 4."
      },
      "Monocular Gaussian SLAM with Language Extended Loop Closure": {
        "authors": [
          "T Lan",
          "Q Lin",
          "H Wang"
        ],
        "url": "https://arxiv.org/pdf/2405.13748",
        "ref_texts": "41. Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., Zhang, G.: Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: IEEE Int. Symp. Mix. Augment. Real. pp. 499\u2013507. IEEE (2022) 3, 9, 10, 11, 12, 13 Monocular Gaussian SLAM with Language Extended Loop Closure 17",
        "ref_ids": [
          "41"
        ],
        "1": "Differentiable Rendering SLAM As the emergence of Neural Radiance Field [23] (NeRF), methods such as [5,14,20,29,30,34,39,41,43,45] have achieved excellent improvement in high-fidelity reconstruction with NeRF-based representation.",
        "2": "In addition, some previous NeRF-based methods [30,41,45] are also involved in the comparison.",
        "3": "Furthermore, though based on monocular input, our method shows competitive performance with RGBD-based Gaussian SLAM [17] and outperforms previous NeRF-based methods [40,41,45].",
        "4": "The results show that our RGB-based method outperforms early RGB-D based NeRF SLAM methods [41,45] in almost all the metrics, and only slightly lower in PSNR than SOTA RGB-D based methods [17,30].",
        "5": "Results of [17,40,41,45] are taken from [17] and results of [36] and [43] are obtained by running their codes.",
        "6": "Method Office0 Office01 Office02 Office03 Office04 Room0 Room1 Room2 AvgRGB-DVox-Fusion [41] 0.",
        "7": "Results of [17,30,41,45] are taken from [17].",
        "8": "Method Metric Office0 Office01 Office02 Office03 Office04 Room0 Room1 Room2 Avg Vox-Fusion [41]PSNR \u219127.",
        "9": "Results of [17,30,41,45] are taken from [17] and results of [36,43] are taken from [43].",
        "10": "Method 0000 0059 0106 0169 0181 AvgRGB-DVox-Fusion [41] 0.",
        "11": "Results of [17,25,30, 38,41,45] are taken from [17].",
        "12": "0198 Vox-Fusion [41] 0."
      },
      "N-Mapping: Normal Guided Neural Non-Projective Signed Distance Fields for Large-scale 3D Mapping": {
        "authors": [
          "S Song",
          "J Zhao",
          "K Huang",
          "J Lin",
          "C Ye"
        ],
        "url": "https://arxiv.org/pdf/2401.03412",
        "ref_texts": "[10] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxFusion: Dense Tracking and Mapping with V oxel-based Neural Implicit Representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , 2022, pp. 499\u2013507.",
        "ref_ids": [
          "10"
        ],
        "1": "Subsequent works tackle this issue by employing various data structures such as sparse octree [10], [11], hash encoding [26], and neural points [27].",
        "2": "Most of these approaches [8], [10], [23]\u2013[25] mitigate this issue by replaying historical keyframes.",
        "3": "Voxel-oriented Training 1) Voxel-oriented Sliding Window: Current feature gridbased methods [9], [10], [25] often select recently observed keyframes from the global set for efficient local optimization, similar to the sliding window method employed in traditional SLAM systems.",
        "4": "[10] X."
      },
      "Blending Distributed NeRFs with Tri-stage Robust Pose Optimization": {
        "authors": [
          "B Ye",
          "C Liu",
          "X Ye",
          "Y Chen",
          "Y Wang",
          "Z Yan"
        ],
        "url": "https://arxiv.org/pdf/2405.02880",
        "ref_texts": "[25] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V ox-fusion: Dense tracking and mapping with voxelbased neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013",
        "ref_ids": [
          "25"
        ],
        "1": "Besides, current approaches using explicit encoding methods like grid [28] [17] and octree[25] for realtime performance, which face the challenge of exponentially expanding encoding components as the scene scale increases, leading to substantially increased storage requirements."
      },
      "S3-SLAM: Sparse Tri-plane Encoding for Neural Implicit SLAM": {
        "authors": [
          "Z Zhang",
          "Y Zhang",
          "Y Wu",
          "B Zhao",
          "X Wang"
        ],
        "url": "https://arxiv.org/pdf/2404.18284",
        "ref_texts": "32. Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., Zhang, G.: Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022) 1, 2",
        "ref_ids": [
          "32"
        ],
        "1": "Existing neural implicit SLAM [10,21,25,29,32,35] excels in reconstructing high-quality scenes and accurately predicting camera poses.",
        "2": "For instance, methods like Vox-Fusion [32] and Co-SLAM [29] have fewer parameters but lose some detailed appearance information.",
        "3": "Notable efforts to enhance the efficiency of neural implicit representations include techniques such as sparse voxel octrees [32], tri-planes [8], dense grids [26,35], hash grids [14], and tensor decomposition [3,9]."
      },
      "MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth Smooth Regularization": {
        "authors": [
          "P Zhu",
          "Y Zhuang",
          "B Chen",
          "L Li",
          "C Wu",
          "Z Liu"
        ],
        "url": "https://arxiv.org/pdf/2405.06241",
        "ref_texts": "[30] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "30"
        ],
        "1": "The comparative work is very comprehensive including traditional direct visual odometry [9], learningbased SLAM[10], neural implicit slam [26], [30], and more recently Gaussian Splatting-based SLAM [22], [23].",
        "2": "[30] X."
      },
      "ImTooth: Neural Implicit Tooth for Dental Augmented Reality": {
        "authors": [
          "Hai Li",
          "Hongjia Zhai",
          "Xingrui Yang",
          "Zhirong Wu",
          "Yihao Zheng",
          "Haofan Wang",
          "Jianchao Wu",
          "Hujun Bao",
          "Guofeng Zhang"
        ],
        "url": "http://www.cad.zju.edu.cn/home/gfzhang/papers/VR-TVCG-2023-ImTooth/ImTooth.pdf",
        "ref_texts": "[61] X. Yang, H. Li, H. Zhai, Y. Ming, Y. Liu, and G. Zhang. Vox-Fusion: Dense tracking and mapping with voxel-based neural implicit representation. In IEEE International Symposium on Mixed and Augmented Reality, pp. 80\u201389, 2021.",
        "ref_ids": [
          "61"
        ],
        "1": "To establish enough co-visibility with a small number of images, we borrow the concept of key-frames from SLAM methods [61], and select the key-frames based on the mutual visibility of voxels.",
        "2": "[61] X."
      },
      "MUTE-SLAM: Real-Time Neural SLAM with Multiple Tri-Plane Hash Representations": {
        "authors": [
          "Y Yan",
          "R He",
          "Z Liu"
        ],
        "url": "https://arxiv.org/pdf/2403.17765",
        "ref_texts": "33. Yang, X., Li, H., Zhai, H., Ming, Y., Liu, Y., Zhang, G.: Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In: 2022 IEEE InMUTE-SLAM 17 ternational Symposium on Mixed and Augmented Reality (ISMAR). pp. 499\u2013507. IEEE (2022) 2, 3, 4, 8, 9, 11, 12",
        "ref_ids": [
          "33"
        ],
        "1": "Some [11,33] address this with octree-based voxel grids, but they still necessitate an initially defined loose boundary and struggle to reconstruct beyond these limits.",
        "2": "Vox-Fusion [33] attempts to address this by introducing octree-based voxel grids as the map representation but is still limited to the initially defined spatial scope.",
        "3": "1 Multi-map Scene Representation As previous neural implicit SLAM methods [12,25,28,33,35] are restricted to functioning within pre-defined scene boundaries, they are unsuitable for navigating and mapping large, unknown indoor environments.",
        "4": "To better evaluate our proposed MUTE-SLAM on pose estimation, we also compare with previous methods NICE-SLAM [35] and Vox-Fusion [33].",
        "5": "We compare the reconstruction performance on Replica [23] only with Co-SLAM [28] and ESLAM [12] as they significantly outperform previous methods [25,33,35].",
        "6": "62 Vox-Fusion [33] 8.",
        "7": "[33] ESLAM [12] Co."
      },
      "DF-SLAM: Neural Feature Rendering Based on Dictionary Factors Representation for High-Fidelity Dense Visual SLAM System": {
        "authors": [
          "W Wei",
          "J Wang"
        ],
        "url": "https://arxiv.org/pdf/2404.17876",
        "ref_texts": "[7] X. Yang, H. Li, H. Zhai, Y. Ming, Y. Liu, and G. Zhang, \u201cVox-fusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "7"
        ],
        "1": "Vox-Fusion [7] employs a sparse octree for scene representation and does not require a predefined scene bounding box.",
        "2": "2 Baseline We compare our method to existing state-of-the-art neural implicit SLAM methods: iMAP [5], NICE-SLAM[6], Vox-Fusion [7], ESLAM [8], Co-SLAM [9], Point-SLAM\n[27] and GS-SLAM [28].",
        "3": "32Vox-Fusion* [7] Depth L1 \u2193 1.",
        "4": "73 VoxFusion* [7] 0.",
        "5": "2 VoxFusion* [7] 11.",
        "6": "31 VoxFusion* [7] 3.",
        "7": "56 Vox-Fusion* [7] 11.",
        "8": "[7] X."
      },
      "HVOFusion: Incremental Mesh Reconstruction Using Hybrid Voxel Octree": {
        "authors": [
          "S Liu",
          "J Chen",
          "J Zhu"
        ],
        "url": "https://arxiv.org/pdf/2404.17974",
        "ref_texts": "[Yang et al. , 2022 ]Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. V oxfusion: Dense tracking and mapping with voxel-based neural implicit representation. In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , pages 499\u2013507. IEEE, 2022.",
        "ref_ids": [
          "Yang et al\\. , 2022 "
        ]
      },
      "TiV-NeRF: Tracking and Mapping via Time-Varying Representation with Dynamic Neural Radiance Fields": {
        "authors": [
          "C Duan",
          "Z Yang"
        ],
        "url": "https://arxiv.org/pdf/2310.18917",
        "ref_texts": "[6] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "6"
        ],
        "1": "V ox-Fusion [6] exploits octree-based representation and achieves scalable implicit scene reconstruction.",
        "2": "V oxFusion [6] proposes an accurate and effective SLAM system based on sparse voxel octree.",
        "3": "[36] and V oxFusion [6].",
        "4": "V ox-Fusion [6] randomly selects keyframes from keyframe database to optimize the global map, which leads to incomplete reconstruction.",
        "5": "After self-supervised training of each frame, we obtain an octree-based map up to current frame and newly NICE-SLAM [5] V ox-Fusion [6] Ours (Random) Ours (Overlap)Room4-1\n Room4-2\n ToyCar3\n Teddy Fig.",
        "6": "However, NICE-SLAM [5] and V ox-Fusion [6] are unable to capture it.",
        "7": "0486 V ox-Fusion [6]RMSE [m](\u2193) 0.",
        "8": "5420 V ox-Fusion [6]MSE(\u2193) 0.",
        "9": "Object Completion The keyframe selection strategy used in V ox-Fusion [6] is unstable, its experimental results are different across multiple executions of experiments.",
        "10": "Random Keyframe selection strategy used in V ox-Fusion [6] is unable to reconstruct the 3D mesh that do not appear in the current view (Left column), ours based on overlap, however, fully reconstructs the blue car (Right column), even for some parts of the car those are not observed in current camera view.",
        "11": "[6] X."
      },
      "NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using 3D Gaussian Splatting": {
        "authors": [
          "Y Ji",
          "Y Liu",
          "G Xie",
          "B Ma",
          "Z Xie"
        ],
        "url": "https://arxiv.org/pdf/2403.11679",
        "ref_texts": "[23] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV ox fusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) . IEEE, 2022, pp. 499\u2013507.",
        "ref_ids": [
          "23"
        ],
        "1": "503 V ox-Fusion [23] 2.",
        "2": "[23] X."
      },
      "MotionGS: Compact Gaussian Splatting SLAM by Motion Filter": {
        "authors": [
          "X Guo",
          "P Han",
          "W Zhang",
          "H Chen"
        ],
        "url": "https://arxiv.org/pdf/2405.11129",
        "ref_texts": "[18] X. Yang, H. Li, H. Zhai, Y . Ming, Y . Liu, and G. Zhang, \u201cV oxfusion: Dense tracking and mapping with voxel-based neural implicit representation,\u201d in 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) , 2022, pp. 499\u2013507. 1, 5",
        "ref_ids": [
          "18"
        ],
        "1": "2) Baseline Methods: We compare and analyze MotionGS against classic traditional SLAM approach (ORB-SLAM2 [6]), deep learning based method (DROID-SLAM [31]), NeRF-based SLAM methods (iMAP [17], NICE-SLAM [19], V ox-Fusion [18], ESLAM [20], Co-SLAM [22], Point-SLAM [23]), and 3DGS-based SLAM methods (MonoGS [27], SplaTAM [28], GS-SLAM [25]).",
        "2": "1, 2, 5\n[18] X."
      }
    }
  },
  {
    "title": "dp-mvs: detail preserving multi-view surface reconstruction of large-scale scenes",
    "id": 0,
    "valid_pdf_number": "9/12",
    "matched_pdf_number": "7/9",
    "matched_rate": 0.7777777777777778,
    "citations": {
      "Emo-mvs: Error-aware multi-scale iterative variable optimizer for efficient multi-view stereo": {
        "authors": [
          "Huizhou Zhou",
          "Haoliang Zhao",
          "Qi Wang",
          "Liang Lei",
          "Gefei Hao",
          "Yusheng Xu",
          "Zhen Ye"
        ],
        "url": "https://www.mdpi.com/2072-4292/14/23/6085/pdf",
        "ref_texts": "37. Zhou, L.; Zhang, Z.; Jiang, H.; Sun, H.; Bao, H.; Zhang, G. DP-MVS: Detail Preserving Multi-View Surface Reconstruction of Large-Scale Scenes. Remote Sens. 2021 ,13, 4569. [CrossRef]",
        "ref_ids": [
          "37"
        ]
      },
      "Hash Encoding and Brightness Correction in 3D Industrial and Environmental Reconstruction of Tidal Flat Neural Radiation": {
        "authors": [
          "Huilin Ge",
          "Biao Wang",
          "Zhiyu Zhu",
          "Jin Zhu",
          "Nan Zhou"
        ],
        "url": "https://www.mdpi.com/1424-8220/24/5/1451/pdf",
        "ref_texts": "20. Zhou, L.; Zhang, Z.; Jiang, H.; Sun, H.; Bao, H.; Zhang, G. DP-MVS: Detail Preserving Multi-View Surface Reconstruction of Large-Scale Scenes. Remote Sens. 2021 ,13, 4569. [CrossRef]",
        "ref_ids": [
          "20"
        ],
        "1": "With advancements in photogrammetry, methods for generating dense point clouds and constructing 3D triangular grid models from 2D images have evolved, incorporating sparse reconstruction (structure from motion, SFM) [18,19] and dense reconstruction (multiple-view stereo, MVS) [20,21]."
      },
      "Geometric Prior-Guided Self-Supervised Learning for Multi-View Stereo": {
        "authors": [
          "Fenghao Zhang",
          "Wanjuan Su",
          "Yuhang Qi"
        ],
        "url": "https://www.mdpi.com/2072-4292/15/8/2109/pdf",
        "ref_texts": "24. Zhou, L.; Zhang, Z.; Jiang, H.; Sun, H.; Bao, H.; Zhang, G. DP-MVS: Detail Preserving Multi-View Surface Reconstruction of Large-Scale Scenes. Remote Sens. 2021 ,13, 4569. [CrossRef]",
        "ref_ids": [
          "24"
        ],
        "1": "Gipuma [21], COLMAP [6], ACMM [23], DP-MVS [24], and PatchMatch MVS [25] are PatchMatch-based [26] MVS methods."
      },
      "Confidence-Guided Planar-Recovering Multiview Stereo for Weakly Textured Plane of High-Resolution Image Scenes": {
        "authors": [
          "Chuanyu Fu",
          "Nan Huang",
          "Zijie Huang",
          "Yongjian Liao",
          "Xiaoming Xiong",
          "Xuexi Zhang",
          "Shuting Cai"
        ],
        "url": "https://www.mdpi.com/2072-4292/15/9/2474/pdf",
        "ref_texts": "2. Zhou, L.; Zhang, Z.; Jiang, H.; Sun, H.; Bao, H.; Zhang, G. DP-MVS: Detail Preserving Multi-View Surface Reconstruction of Large-Scale Scenes. Remote Sens. 2021 ,13, 4569. [CrossRef]",
        "ref_ids": [
          "2"
        ],
        "1": "[2] focuses on the geometric details of reconstruction, especially the preservation of geometric details of thin structures."
      },
      "Large-Scale Mussel Farm Reconstruction with GPS Auxiliary": {
        "authors": [
          "J Zhao",
          "B Xue",
          "R Vennell"
        ],
        "url": "https://openaccess.wgtn.ac.nz/articles/conference_contribution/Large-Scale_Mussel_Farm_Reconstruction_with_GPS_Auxiliary/24633795/1/files/43285662.pdf",
        "ref_texts": "[24] Zhang, Xiaoshuai, et al. \u201cNerfusion: Fusing radiance fields for largescale scene reconstruction.\u201d Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.",
        "ref_ids": [
          "24"
        ],
        "1": "Some recent approaches leverage deep learning [24], [30], like Neural Radiance Field (NeRF), for large-scale 3D reconstruction.",
        "2": "To achieve this, we employed the widely utilized COLMAP method [32], [33], a well-established SfM technique widely applied in various studies [24], [30].",
        "3": "[24] Zhang, Xiaoshuai, et al."
      },
      "Gaussian Control with Hierarchical Semantic Graphs in 3D Human Recovery": {
        "authors": [
          "H Wang",
          "W Zhang",
          "S Liu",
          "X Zhou",
          "S Zhang"
        ],
        "url": "https://arxiv.org/pdf/2405.12477",
        "ref_texts": "[45] Liyang Zhou, Zhuang Zhang, Hanqing Jiang, Han Sun, Hujun Bao, and Guofeng Zhang. Dp-mvs: Detail preserving multi-view surface reconstruction of large-scale scenes. Remote Sensing , 13(22):4569, 2021. 1",
        "ref_ids": [
          "45"
        ],
        "1": "Traditional 3D representation methods [3, 6, 10, 14, 22, 37, 45]\n\u2020Corresponding Author."
      },
      "Editorial on Special Issue \u201cTechniques and Applications of UAV-Based Photogrammetric 3D Mapping\u201d": {
        "authors": [
          "Wanshou Jiang",
          "San Jiang",
          "Xiongwu Xiao"
        ],
        "url": "https://www.mdpi.com/2072-4292/14/15/3804/pdf",
        "ref_texts": "5. Zhou, L.; Zhang, Z.; Jiang, H.; Sun, H.; Bao, H.; Zhang, G. DP-MVS: Detail Preserving Multi-View Surface Reconstruction of Large-Scale Scenes. Remote Sens. 2021 ,13, 4569. [CrossRef] Remote Sens. 2022 ,14, 3804 4 of 4",
        "ref_ids": [
          "5"
        ],
        "1": "[5] proposed a dense matching algorithm, termed DP-MVS, for detail-preserving 3D reconstruction."
      },
      "Rekonstruksi Model 3D dari Set Citra Menggunakan Metode SFM-MVS dan Algoritma Poisson": {
        "authors": [
          "G Hanbudi",
          "E Fauzi"
        ],
        "url": "http://www.stmik-budidarma.ac.id/ejurnal/index.php/mib/article/viewFile/4126/2804",
        "ref_texts": "[18] L. Zhou, Z. Zhang, H. Jiang, H. Sun, H. Bao, and G. Zhang, \u201cDP -MVS: Detail Preserving Multi -View Surface Reconstruction of Large -Scale Scenes,\u201d Remote Sensing , vol. 13, no. 22, p. 4569, Nov. 2021, doi: 10.3390/rs13224569. ",
        "ref_ids": [
          "18"
        ],
        "1": "Furukawa mempresentasikan metode SOTA MVS (State of The Art Multi -View Stereo) yang disebut Patch -based MVS (PMVS) dimana metode ini dimulai de ngan menentukan seed patch (patch awalan) dengan merekonstruksi sekumpulan matched key point yang cocok dan kemudian secara iteratif memperluas patch tersebut [18].",
        "2": "[18] L."
      },
      "\u0395\u03bd\u03c3\u03c9\u03bc\u03ac\u03c4\u03c9\u03c3\u03b7 \u03b4\u03b5\u03c3\u03bc\u03b5\u03cd\u03c3\u03b5\u03c9\u03bd \u03c3\u03c4\u03b7\u03bd \u03c0\u03bf\u03bb\u03c5\u03b5\u03b9\u03ba\u03bf\u03bd\u03b9\u03ba\u03ae \u03b1\u03bd\u03b1\u03ba\u03b1\u03c4\u03b1\u03c3\u03ba\u03b5\u03c5\u03ae": {
        "authors": [
          "\u0395\u039a \u03a3\u03c4\u03b1\u03b8\u03bf\u03c0\u03bf\u03cd\u03bb\u03bf\u03c5"
        ],
        "url": "https://dspace.lib.ntua.gr/xmlui/bitstream/handle/123456789/56640/PhD_Stathopoulou_Integrating_scene_priors_in_MVS.pdf",
        "ref_texts": "206 BIBLIOGRAPHY. Zheng, E., Dunn, E., Jojic, V., and Frahm, J.-M. Patchmatch based joint view selection and depthmap estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 1510\u20131517, 2014. Zheng, S., Jayasumana, S., Romera-Paredes, B., Vineet, V., Su, Z., Du, D., Huang, C., and Torr, P. H. Conditional random fields as recurrent neural networks. In Proceedings of the IEEE international conference on computer vision , pages 1529\u20131537, 2015. Zhong, Y., Dai, Y., and Li, H. Self-supervised learning for stereo matching with selfimproving ability. arXiv preprint arXiv:1709.00930 , 2017. Zhou, C., Zhang, H., Shen, X., and Jia, J. Unsupervised learning of stereo matching. InProceedings of the IEEE International Conference on Computer Vision , pages 1567\u20131575, 2017. Zhou, K., Meng, X., and Cheng, B. Review of stereo matching algorithms based on deep learning. Computational intelligence and neuroscience , 2020, 2020. Zhou, L., Zhang, Z., Jiang, H., Sun, H., Bao, H., and Zhang, G. Dp-mvs: Detail preserving multi-view surface reconstruction of large-scale scenes. Remote Sensing , 13"
      }
    }
  },
  {
    "title": "intrinsicnerf: learning intrinsic neural radiance fields for editable novel view synthesis",
    "id": 5,
    "valid_pdf_number": "21/24",
    "matched_pdf_number": "12/21",
    "matched_rate": 0.5714285714285714,
    "citations": {
      "Palettenerf: Palette-based appearance editing of neural radiance fields": {
        "authors": [
          "Zhengfei Kuang",
          "Fujun Luan",
          "Sai Bi",
          "Zhixin Shu",
          "Gordon Wetzstein",
          "Kalyan Sunkavalli"
        ],
        "url": "https://openaccess.thecvf.com/content/CVPR2023/papers/Kuang_PaletteNeRF_Palette-Based_Appearance_Editing_of_Neural_Radiance_Fields_CVPR_2023_paper.pdf",
        "ref_texts": "[38] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. arXiv preprint arXiv:2210.00647, 2022. 2[39] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. CoRR, abs/2112.05131, 2021. 1,2",
        "ref_ids": [
          "38",
          "39"
        ],
        "1": "Introduction Neural Radiance Fields (NeRF) [23] and its variants [8, 25, 27, 39] have received increasing attention in recent years for their ability to robustly reconstruct real-world 3D scenes from 2D images and enable high-quality, photorealistic novel view synthesis.",
        "2": "Many recent works [8, 25,36,39,44] propose to speed up the training and improve the performance of the models by applying a combination of light-weight MLPs and neural feature maps or volumes.",
        "3": "[38] introduces a NeRF-based intrinsic decomposition model which enables 3D intuitive recoloring, but it does not support palette-based editing.",
        "4": "2[39] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa."
      },
      "PVO: Panoptic visual odometry": {
        "authors": [
          "Weicai Ye",
          "Xinyue Lan",
          "Shuo Chen",
          "Yuhang Ming",
          "Xingyuan Yu",
          "Hujun Bao",
          "Zhaopeng Cui",
          "Guofeng Zhang"
        ],
        "url": "http://openaccess.thecvf.com/content/CVPR2023/papers/Ye_PVO_Panoptic_Visual_Odometry_CVPR_2023_paper.pdf",
        "ref_texts": ""
      },
      "Nero: Neural geometry and brdf reconstruction of reflective objects from multiview images": {
        "authors": [
          "Y Liu",
          "P Wang",
          "C Lin",
          "X Long",
          "J Wang",
          "L Liu"
        ],
        "url": "https://arxiv.org/pdf/2305.17398",
        "ref_texts": "2022b. S3-NeRF: Neural Reflectance Field from Shading and Shadow under a Single Viewpoint. In NeurIPS . Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. 2018. MVSNet: Depth inference for unstructured multi-view stereo. In ECCV . Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. 2022. NeILF: Neural incident light field for physically-based material estimation. In ECCV . Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. 2021. Volume rendering of neural implicit surfaces. In NeurIPS . Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. 2020. Multiview neural surface reconstruction by disentangling geometry and appearance. In NeurIPS . Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. 2022. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. arXiv preprint arXiv:2210.00647 (2022). Ye Yu, Abhimitra Meka, Mohamed Elgharib, Hans-Peter Seidel, Christian Theobalt, and William AP Smith. 2020. Self-supervised outdoor scene relighting. In ECCV . Ye Yu and William AP Smith. 2019. Inverserendernet: Learning single image inverse rendering. In CVPR . Jason Zhang, Gengshan Yang, Shubham Tulsiani, and Deva Ramanan. 2021c. NeRS: Neural reflectance surfaces for sparse-view 3d reconstruction in the wild. In NeurIPS . Kai Zhang, Fujun Luan, Zhengqi Li, and Noah Snavely. 2022a. IRON: Inverse Rendering by Optimizing Neural SDFs and Materials from Photometric Images. In CVPR . Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. 2021a. PhySG: Inverse rendering with spherical gaussians for physics-based material editing and relighting. In CVPR . Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. 2020. Nerf++: Analyzing and improving neural radiance fields. arXiv preprint arXiv:2010.07492 (2020). Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR . Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul Debevec, William T Freeman, and Jonathan T Barron. 2021b. NeRFactor: Neural factorization of shape and reflectance under an unknown illumination. In SIGGRAPH . Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei Jia, and Xiaowei Zhou."
      },
      "Clean-NeRF: Reformulating NeRF to account for View-Dependent Observations": {
        "authors": [
          "X Liu",
          "YW Tai",
          "CK Tang"
        ],
        "url": "https://arxiv.org/pdf/2303.14707",
        "ref_texts": "[66] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. arXiv preprint arXiv:2210.00647 , 2022. 3",
        "ref_ids": [
          "66"
        ],
        "1": "IntrinsicNeRF [66] introduces intrinsic decomposition to the NeRF-based neural rendering method, which allows for editable novel view synthesis in room-scale scenes."
      },
      "PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF": {
        "authors": [
          "Y Feng",
          "Y Shang",
          "X Li",
          "T Shao",
          "C Jiang"
        ],
        "url": "https://arxiv.org/pdf/2311.13099",
        "ref_texts": "[84] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 339\u2013351, 2023. 2",
        "ref_ids": [
          "84"
        ],
        "1": "These include semantic-driven editing [3, 13, 24, 45, 66, 73], shading-driven adjustments (like relighting and texturing) [21, 43, 64, 68, 78, 84], scene modifications (such as object addition or removal) [35, 36, 76, 83, 90], face editing [27, 31, 70, 89], physics based editing from video[25, 62], and multi-purpose editing [30, 75, 82]."
      },
      "GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image": {
        "authors": [
          "C Bao",
          "Y Zhang",
          "Y Li",
          "X Zhang",
          "B Yang",
          "H Bao"
        ],
        "url": "https://arxiv.org/pdf/2404.02152",
        "ref_texts": "[63] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 339\u2013351, 2023. 2",
        "ref_ids": [
          "63"
        ],
        "1": "Neural Radiance Field [33] has exhibited great reconstruction and rendering qualities in SLAM [62, 73], scene editing [5, 58\u201360, 64] and relighting [63, 66, 67], especially promoting the emergence of many 3D avatar reconstruction [4, 16, 53, 68, 69, 76] and generation [50, 52, 54]."
      },
      "Semantically-aware Neural Radiance Fields for Visual Scene Understanding: A Comprehensive Review": {
        "authors": [
          "TAQ Nguyen",
          "A Bourki",
          "M Macudzinski"
        ],
        "url": "https://arxiv.org/pdf/2402.11141",
        "ref_texts": "[277] Jingbo Zhang et al. \u201cFdnerf: Few-shot dynamic neural radiance fields for face reconstruction and expression editing\u201d. In: SIGGRAPH Asia 2022 Conference Papers . 2022, pp. 1\u20139.",
        "ref_ids": [
          "277"
        ],
        "1": "Users can manipulate facial attributes effectively by providing simple expression codes [47, 277], mask 16\n(a)\n (b) Fig.",
        "2": "[277] Jingbo Zhang et al."
      },
      "Few-Shot Neural Radiance Fields under Unconstrained Illumination": {
        "authors": [
          "Yeong Lee",
          "Yong Choi",
          "Seungryong Kim",
          "Jae Kim",
          "Junghyun Cho"
        ],
        "url": "https://ojs.aaai.org/index.php/AAAI/article/view/28075/28156",
        "ref_texts": "12901. Rudnev, V.; Elgharib, M.; Smith, W.; Liu, L.; Golyanik, V.; and Theobalt, C. 2022. Nerf for outdoor scene relighting. In ECCV, 615\u2013631. Springer. Sch\u00a8onberger, J. L.; Zheng, E.; Pollefeys, M.; and Frahm, J.M. 2016. Pixelwise View Selection for Unstructured MultiView Stereo. In ECCV. Sitzmann, V.; Zollh \u00a8ofer, M.; and Wetzstein, G. 2019. Scene representation networks: Continuous 3d-structureaware neural scene representations. NeurIPS, 32. Snavely, N.; Seitz, S. M.; and Szeliski, R. 2006. Photo tourism: exploring photo collections in 3D. In ACM SIGGRAPH, 835\u2013846. Toschi, M.; De Matteo, R.; Spezialetti, R.; De Gregorio, D.; Di Stefano, L.; and Salti, S. 2023. ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects. In CVPR, 20762\u201320772. Wang, C.; Chai, M.; He, M.; Chen, D.; and Liao, J. 2022. CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields. In CVPR, 3835\u20133844. Wang, Q.; Wang, Z.; Genova, K.; Srinivasan, P. P.; Zhou, H.; Barron, J. T.; Martin-Brualla, R.; Snavely, N.; and Funkhouser, T. 2021. Ibrnet: Learning multi-view imagebased rendering. In CVPR, 4690\u20134699. Watson, D.; Chan, W.; Martin-Brualla, R.; Ho, J.; Tagliasacchi, A.; and Norouzi, M. 2022. Novel view synthesis with diffusion models. arXiv preprint arXiv:2210.04628. Wynn, J.; and Turmukhambetov, D. 2023. Diffusionerf: Regularizing neural radiance fields with denoising diffusion models. In CVPR, 4180\u20134189. Xu, D.; Jiang, Y.; Wang, P.; Fan, Z.; Shi, H.; and Wang, Z. 2022. SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single Image. arXiv preprint arXiv:2204.00928. Yang, J.; Pavone, M.; and Wang, Y. 2023. FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization. In CVPR, 8254\u20138263. Yang, S.; Cui, X.; Zhu, Y.; Tang, J.; Li, S.; Yu, Z.; and Shi, B. 2023. Complementary Intrinsics From Neural Radiance Fields and CNNs for Outdoor Scene Relighting. In CVPR, 16600\u201316609. Ye, W.; Chen, S.; Bao, C.; Bao, H.; Pollefeys, M.; Cui, Z.; and Zhang, G. 2022. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. arXiv preprint arXiv:2210.00647.Yu, A.; Ye, V.; Tancik, M.; and Kanazawa, A. 2021. pixelnerf: Neural radiance fields from one or few images. In CVPR, 4578\u20134587. Yuan, Y.-J.; Lai, Y.-K.; Huang, Y.-H.; Kobbelt, L.; and Gao, L. 2022a. Neural Radiance Fields from Sparse RGB-D Images for High-Quality View Synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 1\u201316. Yuan, Y.-J.; Sun, Y.-T.; Lai, Y.-K.; Ma, Y.; Jia, R.; and Gao, L. 2022b. Nerf-editing: geometry editing of neural radiance fields. In CVPR, 18353\u201318364. The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)",
        "ref_ids": [
          "12901"
        ]
      },
      "SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild": {
        "authors": [
          "A Engelhardt",
          "A Raj",
          "M Boss",
          "Y Zhang",
          "A Kar"
        ],
        "url": "https://arxiv.org/pdf/2401.10171",
        "ref_texts": "[15] Yue Chen, Xingyu Chen, Xuan Wang, Qi Zhang, Yu Guo, Ying Shan, and Fei Wang. Local-to-global registration forbundle-adjusting neural radiance fields. CVPR , pages 8264\u2013",
        "ref_ids": [
          "15"
        ],
        "1": "Other recent methods rely on rough initialization of the camera, global alignment, or a template shape for joint optimization [15,44,77,84].",
        "2": "2, 3, 4, 5, 6, 7, 12, 13, 14, 15, 16\n[15] Yue Chen, Xingyu Chen, Xuan Wang, Qi Zhang, Yu Guo, Ying Shan, and Fei Wang."
      },
      "NeRF: Multi-Modal Decomposition NeRF with 3D Feature Fields": {
        "authors": [
          "N Wang",
          "L Zhang",
          "AX Chang"
        ],
        "url": "https://arxiv.org/pdf/2405.05010",
        "ref_texts": "59. Ye, W., Chen, S., Bao, C., Bao, H., Pollefeys, M., Cui, Z., Zhang, G.: Intrinsicnerf: Learningintrinsicneuralradiancefieldsforeditablenovelviewsynthesis.In:ICCV. pp. 339\u2013351 (2023) 4",
        "ref_ids": [
          "59"
        ],
        "1": "Neural rendering has spurred an exploration into implicit and hybrid representations, offering various approaches for 3D editing, such as changing global appearance [7,29], intrinsic decomposition [59,64], per-object decomposition [54, 56], geometry and texture editing [1,55,61], 3D inpainting [36,52], and others [20,39,50]."
      },
      "NeRF-Det++: Incorporating Semantic Cues and Perspective-aware Depth Supervision for Indoor Multi-View 3D Detection": {
        "authors": [
          "C Huang",
          "Y Hou",
          "W Ye",
          "D Huang",
          "X Huang"
        ],
        "url": "https://arxiv.org/pdf/2402.14464",
        "ref_texts": "[Yeet al. , 2023 ]Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 2023.",
        "ref_ids": [
          "Yeet al\\. , 2023 "
        ]
      },
      "Stylizing Sparse-View 3D Scenes with Hierarchical Neural Representation": {
        "authors": [
          "Y Wang",
          "A Gao",
          "Y Gong",
          "Y Zeng"
        ],
        "url": "https://arxiv.org/pdf/2404.05236"
      },
      "ExtremeNeRF: Few-shot Neural Radiance Fields Under Unconstrained Illumination": {
        "authors": [
          "SY Lee",
          "JY Choi",
          "S Kim",
          "IJ Kim",
          "J Cho"
        ],
        "url": "https://arxiv.org/pdf/2303.11728",
        "ref_texts": "12901. Rudnev, V .; Elgharib, M.; Smith, W.; Liu, L.; Golyanik, V .; and Theobalt, C. 2022. Nerf for outdoor scene relighting. In ECCV , 615\u2013631. Springer. Sch\u00a8onberger, J. L.; Zheng, E.; Pollefeys, M.; and Frahm, J.M. 2016. Pixelwise View Selection for Unstructured MultiView Stereo. In ECCV . Sitzmann, V .; Zollh \u00a8ofer, M.; and Wetzstein, G. 2019. Scene representation networks: Continuous 3d-structureaware neural scene representations. NeurIPS , 32. Snavely, N.; Seitz, S. M.; and Szeliski, R. 2006. Photo tourism: exploring photo collections in 3D. In ACM SIGGRAPH , 835\u2013846. Toschi, M.; De Matteo, R.; Spezialetti, R.; De Gregorio, D.; Di Stefano, L.; and Salti, S. 2023. ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects. In CVPR , 20762\u201320772. Wang, C.; Chai, M.; He, M.; Chen, D.; and Liao, J. 2022. CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields. In CVPR , 3835\u20133844. Wang, Q.; Wang, Z.; Genova, K.; Srinivasan, P. P.; Zhou, H.; Barron, J. T.; Martin-Brualla, R.; Snavely, N.; and Funkhouser, T. 2021. Ibrnet: Learning multi-view imagebased rendering. In CVPR , 4690\u20134699. Watson, D.; Chan, W.; Martin-Brualla, R.; Ho, J.; Tagliasacchi, A.; and Norouzi, M. 2022. Novel view synthesis with diffusion models. arXiv preprint arXiv:2210.04628 . Wynn, J.; and Turmukhambetov, D. 2023. Diffusionerf: Regularizing neural radiance fields with denoising diffusion models. In CVPR , 4180\u20134189. Xu, D.; Jiang, Y .; Wang, P.; Fan, Z.; Shi, H.; and Wang, Z. 2022. SinNeRF: Training Neural Radiance Fields on Complex Scenes from a Single Image. arXiv preprint arXiv:2204.00928 .Yang, J.; Pavone, M.; and Wang, Y . 2023. FreeNeRF: Improving Few-shot Neural Rendering with Free Frequency Regularization. In CVPR , 8254\u20138263. Yang, S.; Cui, X.; Zhu, Y .; Tang, J.; Li, S.; Yu, Z.; and Shi, B. 2023. Complementary Intrinsics From Neural Radiance Fields and CNNs for Outdoor Scene Relighting. In CVPR , 16600\u201316609. Ye, W.; Chen, S.; Bao, C.; Bao, H.; Pollefeys, M.; Cui, Z.; and Zhang, G. 2022. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. arXiv preprint arXiv:2210.00647 . Yu, A.; Ye, V .; Tancik, M.; and Kanazawa, A. 2021. pixelnerf: Neural radiance fields from one or few images. In CVPR , 4578\u20134587. Yuan, Y .-J.; Lai, Y .-K.; Huang, Y .-H.; Kobbelt, L.; and Gao, L. 2022a. Neural Radiance Fields from Sparse RGB-D Images for High-Quality View Synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence , 1\u201316. Yuan, Y .-J.; Sun, Y .-T.; Lai, Y .-K.; Ma, Y .; Jia, R.; and Gao, L. 2022b. Nerf-editing: geometry editing of neural radiance fields. In CVPR , 18353\u201318364. Supplementary Material In this section, we provide further details on the experiments and datasets, followed by additional ablation studies and experimental results. More Details on Experiments Experimental Details Comparisons on Phototourism F3.A subset for fewshot view synthesis was created by selecting 3 input views with similar depth bounds and frontal-facing poses. The image IDs are (185, 45, 1066), (964, 34, 478), and (82, 312, 803) for \u2018Brandenburg Gate\u2019, \u2018Sacre Coeur\u2019, and \u2018Trevi Fountain\u2019, respectively. Fig. 7 shows image samples of the dataset. Each scene has about eight test images to evaluate the performance. For the depth map comparison using Abs Rel, the original ground truth depth maps provided by Phototourism (Snavely, Seitz, and Szeliski 2006) are very noisy. Following the publicly available instructions of the dataset, we used the clean versions of the depth maps with the background masks. As a result, reported Abs Rel excludes background regions for the evaluation. More detailed information about the proposed datasets can be found in the next section. Comparisons on NeRF Extreme. For evaluating the fewshot view synthesis performance on NeRF Extreme, image IDs (0, 14, 29) were used as inputs for each scene. Given that our proposed multi-view consistency takes into account complex scene geometry, including occlusions, the optimal model and parameters might vary based on the specific scene characteristics. However, experimental results in Tab. 4of the paper were based on our final model. Ablation studies. In this paragraph, we provide detailed information about the ablation studies, especially for ablation 1-3, with albedo MLP. Ablations other than 1-3, were conducted with straightforward implementation with and without proposed consistency regularization. In the case of 1-3, we implemented the multi-view albedo consistency by directly synthesizing the albedo map using the Multi-Layered Perceptron (MLP) of NeRF. Given the viewindependent nature of albedo, we configured the MLP to output albedo as density. By doing so, the proposed framework may be free from the computational costs that come from continuous patch-wise sampling. However, as shown in Tab. 5 in the paper, the model with albedo MLP shows sub-optimal results, which has almost the same results as the model without albedo consistency (1-1). Implementation Details Neural radiance fields. Our framework is based on JAX (Bradbury et al. 2018) implementation of RegNeRF (Niemeyer et al. 2022), while partially adopting a frequency regularization mask of FreeNeRF (Yang, Pavone, and Wang 2023) when constructing MLP. Detailed algorithms of the proposed loss functions are provided in Alg. 1 and 2. Figure 7: Examples of inputs sampled from Phototourism F3.Sampled frontal-facing scenes with varying illumination from the \u2018Brandenburg Gate\u2019, \u2018Sacre Coeur\u2019, and \u2018Trevi Fountain\u2019, respectively. Intrinsic decomposition network. Building upon the concept of integrating an offline intrinsic decomposition network, our PIDNet adopts the architecture of the chosen FIDNet. Given that our ultimate model employs IIDWW (Li and Snavely 2018) as the FIDNet to provide pseudo-albedo ground truth, our PIDNet shares a similar architecture with IIDWW, albeit in a shallower configuration. However, it\u2019s worth highlighting that any intrinsic decomposition network demonstrating superior performance can substitute IIDWW as the FIDNet with a paired PIDNet that has a similar, and shallower architecture. Additional Losses In addition to the losses suggested in the main paper, we incorporates several losses to better optimize the PIDNet as described below. For Lcolor andLds, they were part of the baseline and showed a performance decrease upon removal. Edge-preserving loss. Motivated by (Godard, Mac Aodha, and Brostow 2017), we used the gradient-based edge-preserving loss, to enforce the input and the novel view patches to preserve geometric properties. Using a weight term, \u03c9(x), which already has been discussed in the paper, our edge-preserving loss on the predicted albedo can be formulated as: Ledge=X x\u2032\u2208P\u2032\u03c9(x)\u2225\u2202(\u02c6a(x)\u2212\u02c6a(x\u2032))\u22252,(12) where \u2202denotes the partial derivatives of the vertical and the horizontal directions, and P\u2032denotes all the pixels in the target image. Chromaticity consistency loss. Similar to (Ye et al.",
        "ref_ids": [
          "12901"
        ]
      },
      "DreamMat: High-quality PBR Material Generation with Geometry-and Light-aware Diffusion Models": {
        "authors": [
          "Y Zhang",
          "Y Liu",
          "Z Xie",
          "L Yang",
          "Z Liu",
          "M Yang"
        ],
        "url": "https://arxiv.org/pdf/2405.17176",
        "ref_texts": "(2023). Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian Fang, David McKinnon, Yanghai Tsin, and Long Quan. 2022. Neilf: Neural incident light field for physically-based material estimation. In ECCV . Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. 2020. Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance. In NeurIPS . Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. 2023. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. In ICCV . Jounathan Young. 2021. xatlas. https://github.com/jpcy/xatlas.git Kim Youwang, Tae-Hyun Oh, and Gerard Pons-Moll. 2023. Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering. arXiv preprint arXiv:2312.11360 (2023). Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, and Xiaojuan Qi. 2023a. Texture Generation on 3D Meshes with Point-UV Diffusion. In ICCV . Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-Hai Zhang, and Xiaojuan Qi."
      },
      "Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo": {
        "authors": [
          "Z Li",
          "Z Lu",
          "H Yan",
          "B Shi",
          "G Pan",
          "Q Zheng"
        ],
        "url": "https://arxiv.org/pdf/2404.01612",
        "ref_texts": "[30] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. IntrinsicNeRF: Learning intrinsic neural radiance fields for editable novel view synthesis. Proc. International Conference on Computer Vision (ICCV) , 2023. 5",
        "ref_ids": [
          "30"
        ],
        "1": "Similar to [30], the normalized color loss calculated as \u2225Nor(A)\u2212Nor(I)\u2225is implemented to help Spin-UP learn a better albedo representation, where Nor(."
      },
      "Neural Implicit Field Editing Considering Object-environment Interaction": {
        "authors": [
          "Z Zeng",
          "Z Wang",
          "Y Zhang",
          "W Cai",
          "Z Cao"
        ],
        "url": "https://arxiv.org/pdf/2311.00425",
        "ref_texts": "[36] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. 2023. Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 339\u2013351.",
        "ref_ids": [
          "36"
        ],
        "1": "Intrinsic NeRF [36] performs unsupervised clustering of image color labels in training process to reconstruct the albedo and shading scene colors.",
        "2": "[36].",
        "3": "To verify the effectiveness of the intrinsic decomposition method we used, we compared the albedo images obtained by Intrinsic NeRF [36] with our own results, which showed our separation in shadow areas were superior to existing methods.",
        "4": "The main method Intrinsic NeRF [36], which did not release code and datasets, mentioned the reference of preprocessed Replica indoor scene dataset provided by Semantic NeRF [40].",
        "5": "Intrinsic NeRF [36] is evaluated their albedo images according to the similarity of ground truth provided by earlier methods such as PhySG [37].",
        "6": "Replica-room_0 Config PSNR \u2193SSIM\u2191LPIPS\u2193 Intrinsic NeRF [36] 30."
      },
      "Neural Radiance Field-based Visual Rendering: A Comprehensive Review": {
        "authors": [
          "M Yao",
          "Y Huo",
          "Y Ran",
          "Q Tian",
          "R Wang"
        ],
        "url": "https://arxiv.org/pdf/2404.00714",
        "ref_texts": "[111] W. Ye, S. Chen, C. Bao, H. Bao, M. Pollefeys, Z. Cui, and G. Zhang, \u201cIntrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision , 2023, pp. 339\u2013351.",
        "ref_ids": [
          "111"
        ],
        "1": "Others SparseNeRF [110](depth prior), NeRF-Det [111](end-to-end detection), IntrinsicNeRF [112](Unsupervised), etc.",
        "2": "20 IntrinsicNeRF [111] (2023) possesses the capability to break down a static scene\u2019s multi-view image into consistent elements like reflectance, shading, and residual layers, achieved through intrinsic decomposition in a NeRF-based neural rendering technique.",
        "3": "[111] W."
      },
      "3D Scene Creation and Rendering via Rough Meshes: A Lighting Transfer Avenue": {
        "authors": [
          "Y Li",
          "B Cai",
          "Y Liang",
          "R Jia",
          "B Zhao",
          "M Gong"
        ],
        "url": "https://arxiv.org/pdf/2211.14823",
        "ref_texts": "[54] W. Ye, S. Chen, C. Bao, H. Bao, M. Pollefeys, Z. Cui, and G. Zhang, \u201cIntrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis,\u201d arXiv preprint arXiv:2210.00647 , 2022.",
        "ref_ids": [
          "54"
        ],
        "1": "There are several works [53], [54], [55] that have also exploited free scene lighting editing.",
        "2": "[54] W."
      },
      "Light Source Estimation via Intrinsic Decomposition for Novel View Synthesis": {
        "authors": [
          "DS Tetruashvili"
        ],
        "url": "https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/645817/Tetruashvili_David.pdf?sequence=1",
        "ref_texts": ""
      },
      "Clean-NeRF: Defogging using Ray Statistics Prior in Natural NeRFs": {
        "authors": [
          "X Liu",
          "YW Tai",
          "CK Tang"
        ],
        "url": "https://openreview.net/pdf?id=YHqEWF5gt8",
        "ref_texts": "612, 2004. Frederik Warburg, Ethan Weber, Matthew Tancik, Aleksander Ho\u0142y \u00b4nski, and Angjoo Kanazawa. Nerfbusters: Removing ghostly artifacts from casually captured nerfs. 2023. Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, and Cengiz Oztireli. D\u02c6 2nerf: Self-supervised decoupling of dynamic and static objects from a monocular video. Advances in Neural Information Processing Systems (NeurIPS) , 35:32653\u201332666, 2022. Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. In IEEE/CVF International Conference on Computer Vision (ICCV) , 2023. Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for realtime rendering of neural radiance fields. In IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 5752\u20135761, 2021. Ye Yu and William AP Smith. Inverserendernet: Learning single image inverse rendering. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 3155\u20133164, 2019. Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu, Yingliang Zhang, Lan Xu, and Jingyi Yu. Editable free-viewpoint video using a layered neural representation. ACM Transactions on Graphics (TOG) , 40(4):1\u201318, 2021a. Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. Physg: Inverse rendering with spherical gaussians for physics-based material editing and relighting. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 5453\u20135462, 2021b. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 586\u2013595, 2018. Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang Zhang, Jingyi Yu, and Lan Xu. Humannerf: Efficiently generated human radiance field from sparse inputs. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 7743\u20137753, 2022. Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and Andrew J Davison. In-place scene labelling and understanding with implicit scene representation. In IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 15838\u201315847, 2021. Rui Zhu, Zhengqin Li, Janarbek Matai, Fatih Porikli, and Manmohan Chandraker. Irisformer: Dense vision transformers for single-image inverse rendering in indoor scenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 2822\u20132831, 2022."
      },
      "Learning Relighting and Intrinsic Decomposition in Neural Radiance Fields": {
        "authors": [
          "Y Yang",
          "S Hu",
          "H Wu",
          "R Baldrich",
          "D Samaras",
          "M Vanrell"
        ],
        "url": "https://neural-rendering.com/papers/22.pdf",
        "ref_texts": "[37] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Pollefeys, Zhaopeng Cui, and Guofeng Zhang. IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis. In Proceedings oftheIEEE/CVF International Conference onComputer Vision, 2023. 1, 2, 3, 4",
        "ref_ids": [
          "37"
        ],
        "1": "Concurrently, there has been an exploration towards scene editing [35], such as recoloring [37] and relighting [21, 38].",
        "2": "The second approach [37], based on intrinsic decomposition[2], aims to provide an interpretable representation of a scene (in terms of reflectance and shading) suitable for image editing.",
        "3": "While IntrinsicNeRF [37] has pioneered the integration of intrinsic decomposition within NeRF, they have not utilized relighting or fully leveraged the 3D information available through neural rendering.",
        "4": "IntrinsicNeRF [37] has been a pioneer in applying intrinsic decomposition to neural rendering.",
        "5": "However, real-world scenes often require a residual term to account for discrepancies [11, 37].",
        "6": "As demonstrated in [37], the diffuse components dominate the scene, so it is crucial to prevent the training from converging to undesirable local minima (R= 0, S=\n0, Re=I).",
        "7": "1194 IntrinsicNeRF*[37] 25.",
        "8": "[4]) and the state-of-the-art neural rendering approach (IntrinsicNeRF [37]).",
        "9": "The other neural rendering method, IntrinsicNeRF [37], also fails to achieve correct decomposition, primarily attributed to the failure in distinguishing intrinsic components and also the difficulty in scene reconstruction."
      }
    }
  },
  {
    "title": "sine: semantic-driven image-based nerf editing with prior-guided editing field",
    "id": 6,
    "valid_pdf_number": "56/60",
    "matched_pdf_number": "38/56",
    "matched_rate": 0.6785714285714286,
    "citations": {
      "Shap-e: Generating conditional 3d implicit functions": {
        "authors": [
          "H Jun",
          "A Nichol"
        ],
        "url": "https://arxiv.org/pdf/2305.02463",
        "ref_texts": "[3]Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. arXiv:2303.13277 , 2023.",
        "ref_ids": [
          "3"
        ],
        "1": "Since they are end-to-end differentiable, INRs also enable various downstream applications such as style transfer [72] and differentiable shape editing [3]."
      },
      "Dreameditor: Text-driven 3d scene editing with neural fields": {
        "authors": [
          "J Zhuang",
          "C Wang",
          "L Lin",
          "L Liu",
          "G Li"
        ],
        "url": "https://arxiv.org/pdf/2306.13455",
        "ref_texts": "Omri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for textdriven editing of natural images. In CVPR 2022 . 18208\u201318218. Chong Bao, Yinda Zhang, and Bangbang et al. Yang. 2023. Sine: Semantic-driven imagebased nerf editing with prior-guided editing field. In CVPR 2023 . 20919\u201320929. Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2022. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800 (2022). Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Xu Jia, and Huchuan Lu. 2021. Animatable neural radiance fields from monocular rgb videos. arXiv preprint arXiv:2106.13629 (2021). Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023. Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. arXiv preprint arXiv:2303.13873 (2023). Yongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, and Kui Jia. 2022. Tango: Textdriven photorealistic and robust 3d stylization via lighting decomposition. arXiv preprint arXiv:2210.11277 (2022). Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. 2022. Diffedit: Diffusion-based semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427 (2022). Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2022a. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022). Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2022b. StyleGAN-NADA: CLIP-guided domain adaptation of image generators. ACM Transactions on Graphics (TOG) 41, 4 (2022), 1\u201313. William Gao, Noam Aigerman, Thibault Groueix, Vladimir G Kim, and Rana Hanocka."
      },
      "Multimodal image synthesis and editing: A survey": {
        "authors": [
          "F Zhan",
          "Y Yu",
          "R Wu",
          "J Zhang",
          "S Lu",
          "L Liu"
        ],
        "url": "https://pure.mpg.de/rest/items/item_3487306/component/file_3487307/content",
        "ref_texts": ""
      },
      "Blended-nerf: Zero-shot object generation and blending in existing neural radiance fields": {
        "authors": [
          "Ori Gordon",
          "Omri Avrahami",
          "Dani Lischinski"
        ],
        "url": "https://openaccess.thecvf.com/content/ICCV2023W/AI3DCC/papers/Gordon_Blended-NeRF_Zero-Shot_Object_Generation_and_Blending_in_Existing_Neural_Radiance_ICCVW_2023_paper.pdf",
        "ref_texts": ""
      },
      "Intrinsicnerf: Learning intrinsic neural radiance fields for editable novel view synthesis": {
        "authors": [
          "Weicai Ye",
          "Shuo Chen",
          "Chong Bao",
          "Hujun Bao",
          "Marc Pollefeys",
          "Zhaopeng Cui",
          "Guofeng Zhang"
        ],
        "url": "https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_IntrinsicNeRF_Learning_Intrinsic_Neural_Radiance_Fields_for_Editable_Novel_View_ICCV_2023_paper.pdf",
        "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field. In Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference , 2023.",
        "ref_ids": [
          "1"
        ],
        "1": "Given the high degree of integration of our approach with NeRF, NeRF extensions can be seamlessly incorporated into our IntrinsicNeRF, such as NeRF in the wild [12, 46, 59], NeRF in dynamic environments [33, 51, 52, 69], fast NeRF [48, 18, 10, 71], NeRF with generalization [11, 64, 72, 27], generative NeRF [55, 62], NeRF with panoptic segmentation [26, 68], NeRFbased SLAM [47, 58, 82], Geometry and Texture Editing with NeRF [1, 14] etc, which will be helpful to the comOriginal Original Recoloring RecoloringFigure 11: Recoloring on Synthetic/Real-World Data."
      },
      "Gaussianeditor: Editing 3d gaussians delicately with text instructions": {
        "authors": [
          "J Fang",
          "J Wang",
          "X Zhang",
          "L Xie",
          "Q Tian"
        ],
        "url": "https://arxiv.org/pdf/2311.16037",
        "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023. 3",
        "ref_ids": [
          "1"
        ],
        "1": "3D Scene Editing of Radiance Fields 3D Scene Editing of Radiance Fields has gained significant popularity as a recent research direction [1, 10, 15, 20, 22\u201325, 28, 34, 47, 48, 52\u201354]."
      },
      "Or-nerf: Object removing from 3d scenes guided by multiview segmentation with neural radiance fields": {
        "authors": [
          "Y Yin",
          "Z Fu",
          "F Yang",
          "G Lin"
        ],
        "url": "https://arxiv.org/pdf/2305.10503",
        "ref_texts": "[30] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field,\u201d 2023. [Online]. Available: http: //arxiv.org/abs/2303.13277",
        "ref_ids": [
          "30",
          "Online"
        ],
        "1": "Scene Object Removal NeRF has greatly facilitated the area of 3D scene editing and research [30], [31], [32], [33] focuses on various editing typesemerging in large numbers.",
        "2": "[Online].",
        "3": "[Online].",
        "4": "[Online].",
        "5": "[Online].",
        "6": "[Online].",
        "7": "[Online].",
        "8": "[Online].",
        "9": "[Online].",
        "10": "[Online].",
        "11": "[Online].",
        "12": "[Online].",
        "13": "[Online].",
        "14": "[Online].",
        "15": "[Online].",
        "16": "[Online].",
        "17": "[Online].",
        "18": "15224\n[30] C.",
        "19": "[Online].",
        "20": "[Online].",
        "21": "[Online].",
        "22": "[Online].",
        "23": "[Online].",
        "24": "[Online].",
        "25": "[Online].",
        "26": "[Online].",
        "27": "[Online].",
        "28": "[Online].",
        "29": "[Online].",
        "30": "[Online].",
        "31": "[Online].",
        "32": "[Online].",
        "33": "[Online].",
        "34": "[Online].",
        "35": "[Online].",
        "36": "[Online].",
        "37": "[Online].",
        "38": "[Online]."
      },
      "Gaussianeditor: Swift and controllable 3d editing with gaussian splatting": {
        "authors": [
          "Y Chen",
          "Z Chen",
          "C Zhang",
          "F Wang",
          "X Yang"
        ],
        "url": "https://arxiv.org/pdf/2311.14521",
        "ref_texts": "[1]Chong Bao, Yinda Zhang, and Bangbang et al. Yang. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In CVPR 2023 , pages 20919\u201320929, 2023. 3",
        "ref_ids": [
          "1"
        ],
        "1": "Additionally, some works [1,10,45,46] leverage CLIP models to facilitate editing through the use of text prompts or reference images.",
        "2": "[1]Chong Bao, Yinda Zhang, and Bangbang et al.",
        "3": "Specifically, when the user clicks a point on the screen, we back-project this point into a spatial point based on the intrinsic and extrinsic parameters of the current viewpoint camera: [x, y, z ]T= [R|t]z(p)K\u22121[px, py,1]T, (9) where [R|t]andKdenote the extrinsic and intrinsic of the current camera, p,z(p)and[x, y, z ]Trefer to the userclicked pixel, its corresponding depth, and the spatial point, respectively."
      },
      "Dreamspace: Dreaming your room space with text-driven panoramic texture propagation": {
        "authors": [
          "B Yang",
          "W Dong",
          "L Ma",
          "W Hu",
          "X Liu"
        ],
        "url": "https://arxiv.org/pdf/2310.13119.pdf?!%5B%E5%9B%BE%E7%89%87%5D(https://mmbiz.qpic.cn/sz_mmbiz_png/tGynVEPiakb9lruS9sv1HdDZ7vhDqdSHTglAfA3BTYFnjkbjPq1ScXWEdvTr7zziboby5kzsWghbScUOPKSziag0g/640?wx_fmt=png)",
        "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 2, 3",
        "ref_ids": [
          "2"
        ],
        "1": ", by giving text prompts, and automatically transferring textures of our living room with enchanting and meaningful details? Over the past few years, enormous efforts have been paid in the field of scene stylization (or texture synthesis) [2, 5, 16, 18, 22, 40, 56].",
        "2": ", imitating Van Gogh\u2019s paintings instead of generating recognizable visual elements [22,56]), or focus on texture editing [2,18] on 3D objects with NeRF representation [31] but struggle to generate high-fidelity textures for the whole space and achieve real-time rendering on HMD devices.",
        "3": ", CLIP model [39]) for style transfer (or editing) [2,18], which achieves stylized results that also follow human language prompts, but these works mainly cannot be scaled to large indoor scenes that allow immersive room touring.",
        "4": "Therefore, existing works for scene-level stylization either are not applicable for immersive indoor scenescale scenarios with affordable computation on HMD devices [2,18], cannot support semantic meaningful style generation [6, 7, 11, 22, 23, 56], or require well-structured CAD model instead of real-world reconstruction [49]."
      },
      "ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields": {
        "authors": [
          "J Dong",
          "YX Wang"
        ],
        "url": "https://proceedings.neurips.cc/paper_files/paper/2023/file/c1e2faff6f588870935f114ebe04a3e5-Paper-Conference.pdf",
        "ref_texts": "[29] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. SINE: Semantic-driven image-based NeRF editing with prior-guided editing field. In CVPR , 2023.",
        "ref_ids": [
          "29"
        ]
      },
      "Advances in 3D Generation: A Survey": {
        "authors": [
          "X Li",
          "Q Zhang",
          "D Kang",
          "W Cheng",
          "Y Gao"
        ],
        "url": "https://arxiv.org/pdf/2401.17807",
        "ref_texts": ""
      },
      "Dyn-e: Local appearance editing of dynamic neural radiance fields": {
        "authors": [
          "S Zhang",
          "S Peng",
          "Y ShenTu",
          "Q Shuai",
          "T Chen"
        ],
        "url": "https://arxiv.org/pdf/2307.12909",
        "ref_texts": "Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. 2023. SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field. In CVPR . Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. 2022. Text2live: Text-driven layered image and video editing. In ECCV . Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo MartinBrualla, and Pratul P Srinivasan. 2021. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In ICCV\u2018\u2019 . Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman."
      },
      "Dynvideo-e: Harnessing dynamic nerf for large-scale motion-and view-change human-centric video editing": {
        "authors": [
          "JW Liu",
          "YP Cao",
          "JZ Wu",
          "W Mao",
          "Y Gu",
          "R Zhao"
        ],
        "url": "https://arxiv.org/pdf/2310.10624",
        "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 3",
        "ref_ids": [
          "1"
        ],
        "1": "SINE [1] supports editing a local region of static NeRF from a single view by delivering edited contents to multi-views through pretrained NeRF priors."
      },
      "ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF": {
        "authors": [
          "J Park",
          "G Kwon",
          "JC Ye"
        ],
        "url": "https://arxiv.org/pdf/2310.02712",
        "ref_texts": "Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 20919\u201320929, 2023. Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. InProceedings of the IEEE/CVF International Conference on Computer Vision , pp. 5855\u20135864, 2021. Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision , pp. 333\u2013350. Springer, 2022. Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 5501\u20135510, 2022. Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clipguided domain adaptation of image generators. arXiv preprint arXiv:2108.00946 , 2021. Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. arXiv preprint arXiv:2303.12789 , 2023. Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. arXiv preprint arXiv:2304.07090 , 2023. Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 867\u2013876, 2022. Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy Mitra. Relu fields: The little nonlinearity that could. In ACM SIGGRAPH 2022 Conference Proceedings , pp. 1\u20139, 2022. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643 , 2023."
      },
      "Mirror-NeRF: Learning Neural Radiance Fields for Mirrors with Whitted-Style Ray Tracing": {
        "authors": [
          "J Zeng",
          "C Bao",
          "R Chen",
          "Z Dong",
          "G Zhang"
        ],
        "url": "https://arxiv.org/pdf/2308.03280",
        "ref_texts": "[2]Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. 2023. SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field. arXiv preprint arXiv:2303.13277",
        "ref_ids": [
          "2"
        ],
        "1": "Several extensions and improvements have been proposed to apply NeRF to more challenging problems, such as scene reconstruction [1,8,13,29,30,32,36,38,39,44,48], generalization [24,33], novel view extrapolation [35,45], scene manipulation [2,28,40\u201342], SLAM [23,54], segmentation [20,53], human body [18,31] and so on."
      },
      "NeRF in Robotics: A Survey": {
        "authors": [
          "G Wang",
          "L Pan",
          "S Peng",
          "S Liu",
          "C Xu",
          "Y Miao"
        ],
        "url": "https://arxiv.org/pdf/2405.01333",
        "ref_texts": "[90] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with priorguided editing field,\u201d in CVPR , 2023, pp. 20 919\u201320 929.",
        "ref_ids": [
          "90"
        ],
        "1": "SINE\n[90] employs a prior-guided editing field to adjust spatial point coordinates and colours for semantic-driven editing.",
        "2": "[90] C."
      },
      "PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF": {
        "authors": [
          "Y Feng",
          "Y Shang",
          "X Li",
          "T Shao",
          "C Jiang"
        ],
        "url": "https://arxiv.org/pdf/2311.13099",
        "ref_texts": "[3] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 2",
        "ref_ids": [
          "3"
        ],
        "1": "These include semantic-driven editing [3, 13, 24, 45, 66, 73], shading-driven adjustments (like relighting and texturing) [21, 43, 64, 68, 78, 84], scene modifications (such as object addition or removal) [35, 36, 76, 83, 90], face editing [27, 31, 70, 89], physics based editing from video[25, 62], and multi-purpose editing [30, 75, 82]."
      },
      "GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image": {
        "authors": [
          "C Bao",
          "Y Zhang",
          "Y Li",
          "X Zhang",
          "B Yang",
          "H Bao"
        ],
        "url": "https://arxiv.org/pdf/2404.02152",
        "ref_texts": "[5] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 2",
        "ref_ids": [
          "5"
        ],
        "1": "Neural Radiance Field [33] has exhibited great reconstruction and rendering qualities in SLAM [62, 73], scene editing [5, 58\u201360, 64] and relighting [63, 66, 67], especially promoting the emergence of many 3D avatar reconstruction [4, 16, 53, 68, 69, 76] and generation [50, 52, 54]."
      },
      "Coarf: Controllable 3d artistic style transfer for radiance fields": {
        "authors": [
          "D Zhang",
          "C Fernandez-Labrador"
        ],
        "url": "https://arxiv.org/pdf/2404.14967",
        "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023. 3",
        "ref_ids": [
          "1"
        ],
        "1": "CLIPNeRF and SINE [1, 51] enable text-driven editing, whereas [21] distills the 2D semantic feature from LSeg [26] to train 3D semantic feature using volumetric rendering, enabling editing including colorization, translation, deletion, and text-driven editing.",
        "2": "Our Semantic Aware Nearest Neighbor Feature Matching (SANNFM) function performs nearest neighbor matching between content and style features in both VGG (FV GG r,FV GG s ) and LSeg (FLSeg r,FLSeg s ) spaces for each specific pixel x, y with label m: SANNFM (x, y, m ) =argminx\u2032,y\u2032\u2208SDsannfm, (9) where S={x\u2032, y\u2032|Ms(x\u2032, y\u2032) =m}, and the distance functionDsannfm is defined as a weighted average of the VGG cosine distance and LSeg cosine distance: Dsannfm =\u03b1\u00b7D(FV GG r(x, y),FV GG s(x\u2032, y\u2032))\n+ (1\u2212\u03b1)\u00b7D(FLSeg r(x, y),FLSeg s(x\u2032, y\u2032)),(10) where \u03b1\u2208[0,1]is a hyperparameter to control the weight of VGG and LSeg features and D(."
      },
      "Aprf: Anti-aliasing projection representation field for inverse problem in imaging": {
        "authors": [
          "Z Chen",
          "L Yang",
          "J Lai",
          "X Xie"
        ],
        "url": "https://arxiv.org/pdf/2307.05270",
        "ref_texts": "[27] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with priorguided editing field,\u201d in Proceedings of the IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023.",
        "ref_ids": [
          "27"
        ],
        "1": "INR techniques have yielded impressive advances in image reconstruction for numerous tasks: single image super-resolution [19], video super-resolution [22], novel view synthesis [18], generative modeling [23]\u2013[25], and editing [26], [27].",
        "2": "[27] C."
      },
      "Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training": {
        "authors": [
          "R He",
          "S Huang",
          "X Nie",
          "T Hui",
          "L Liu",
          "J Dai",
          "J Han"
        ],
        "url": "https://arxiv.org/pdf/2312.01663",
        "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In CVPR , 2023. 3, 6",
        "ref_ids": [
          "2"
        ],
        "1": "CLIP-NeRF [42] and SINE [2] leverage prior models to optimize the geometry and texture of NeRF based on text descriptions or exemplar images.",
        "2": "For image-driven editing, due to the absence of existing methods for this setting, we modified several existing works to serve as baselines for comparison with our method, including: (1) Ours+Splice Loss: Splice loss [41] is proposed to disentangle structure and appearance information from an image for image editing, which is further demonstrated effective to transfer the texture from an exemplar image for NeRF editing in SINE [2]."
      },
      ": A 3D Neural Additive Model for Interpretable Shape Representation": {
        "authors": [
          "Y Jiao",
          "CJ ZDANSKI",
          "JS Kimbell",
          "A Prince"
        ],
        "url": "https://openreview.net/pdf?id=wg8NPfeMF9",
        "ref_texts": "40\u201349. PMLR, 2018. Rishabh Agarwal, Nicholas Frosst, Xuezhou Zhang, Rich Caruana, and Geoffrey E Hinton. Neural additive models: Interpretable machine learning with neural nets. arXiv preprint arXiv:2004.13912 , 2020. Sercan \u00d6 Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 35, pp. 6679\u20136687, 2021. K Somani Arun, Thomas S Huang, and Steven D Blostein. Least-squares fitting of two 3-d point sets. IEEE Transactions on pattern analysis and machine intelligence , (5):698\u2013700, 1987. Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp."
      },
      "S2RF: Semantically Stylized Radiance Fields": {
        "authors": [
          "D Lahiri",
          "N Panse",
          "M Kumar"
        ],
        "url": "https://arxiv.org/pdf/2309.01252",
        "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023.",
        "ref_ids": [
          "2"
        ],
        "1": "Similar to previous methods [25, 12, 2, 26, 8, 6], addressing style transfer in 3D, we adopted an optimization-based approach.",
        "2": "An interesting work Sine [2], requires one image from a scene edited by the user and can generate a 3D view of the scene with the edited objects."
      },
      "Mirror-3DGS: Incorporating Mirror Reflections into 3D Gaussian Splatting": {
        "authors": [
          "J Meng",
          "H Li",
          "Y Wu",
          "Q Gao",
          "S Yang",
          "J Zhang"
        ],
        "url": "https://arxiv.org/pdf/2404.01168",
        "ref_texts": "1. Bao, C., Zhang, Y., Yang, B., Fan, T., Yang, Z., Bao, H., Zhang, G., Cui, Z.: Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition (CVPR) (2023)",
        "ref_ids": [
          "1"
        ],
        "1": "These endeavors have been geared towards refining reconstruction quality [2\u20134,15,38,43], enhancing computational efficiency [13,28,30,33,37], enabling advanced editing functionalities [1,23,40,44,45], and progressing dynamic scene representation [8,10,12,22,29].",
        "2": "We add a learnable mirror attribute m\u2208[0,1]for each Gaussian in the original 3DGS, representing the probability of it being a mirror.",
        "3": "(28) We apply SINE and Sigmoid activation function to normalize the opacity and mirror properties respectively, both of which are bounded within the range of [0, 1], while keeping all other experimental variables constant."
      },
      "ED-NeRF: Efficient Text-Guided Editing of 3D Scene With Latent Space NeRF": {
        "authors": [
          "JH Park",
          "G Kwon",
          "JC Ye"
        ],
        "url": "https://openreview.net/pdf?id=9DvDRTTdlu",
        "ref_texts": "Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 20919\u201320929, 2023. Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. InProceedings of the IEEE/CVF International Conference on Computer Vision , pp. 5855\u20135864, 2021. Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision , pp. 333\u2013350. Springer, 2022. Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 5501\u20135510, 2022. Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clipguided domain adaptation of image generators. arXiv preprint arXiv:2108.00946 , 2021. Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. arXiv preprint arXiv:2303.12789 , 2023. Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. arXiv preprint arXiv:2304.07090 , 2023. Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 867\u2013876, 2022. Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy Mitra. Relu fields: The little nonlinearity that could. In ACM SIGGRAPH 2022 Conference Proceedings , pp. 1\u20139, 2022. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643 , 2023."
      },
      "SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds": {
        "authors": [
          "M Chen",
          "J Xie",
          "I Laina",
          "A Vedaldi"
        ],
        "url": "https://arxiv.org/pdf/2312.09246",
        "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In CVPR , 2023. 2",
        "ref_ids": [
          "1"
        ],
        "1": "Approaches that followed include 3D editing from just a single edited view [1], or via 2D sketches [44], keypoints [87], attributes [25], meshes [22, 52, 76, 78, 80] or point clouds [5]."
      },
      "CaesarNeRF: Calibrated Semantic Representation for Few-shot Generalizable Neural Rendering": {
        "authors": [
          "H Zhu",
          "T Ding",
          "T Chen",
          "I Zharkov",
          "R Nevatia"
        ],
        "url": "https://arxiv.org/pdf/2311.15510",
        "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In CVPR , pages 20919\u201320929, 2023. 2",
        "ref_ids": [
          "1"
        ],
        "1": "In recent years, NeRF has witnessed improvements in a wide range of applications, such as photo-realistic novel view synthesis for large-scale scenes [34, 60, 71], dynamic scene decomposition and deformation [21, 27, 31, 40\u2013\n42, 44, 75, 76], occupancy or depth estimation [58, 62, 74], scene generation and editing [1, 20, 28, 30, 35, 43, 63, 64, 70], and so on."
      },
      "Neural Rendering and Its Hardware Acceleration: A Review": {
        "authors": [
          "X Yan",
          "J Xu",
          "Y Huo",
          "H Bao"
        ],
        "url": "https://arxiv.org/pdf/2402.00028",
        "ref_texts": "[8] C. Bao, Y. Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023.",
        "ref_ids": [
          "8"
        ],
        "1": "NeRF [62] and related works [60]\u2013 [8] have demonstrated good performance in learning scene representation from multi-view input data using volume rendering, which can be utilized in neural rendering-based inverse rendering frameworks.",
        "2": "[8] C."
      },
      "LLMs Meet Multimodal Generation and Editing: A Survey": {
        "authors": [
          "Y He",
          "Z Liu",
          "J Chen",
          "Z Tian",
          "H Liu",
          "X Chi",
          "R Liu"
        ],
        "url": "https://arxiv.org/pdf/2405.19334",
        "ref_texts": "[358] C. Bao, Y. Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with prior-guided editing field,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023, pp.",
        "ref_ids": [
          "358"
        ],
        "1": "Sine [358] presented a prior-guided editing field that encodes finegrained geometric and texture modifications.",
        "2": "5 CLIP for 3D editing CLIP-NeRF [138] CVPR 2022 CLIP Loss NeRF CLIP Blended-NeRF [356] ICCVW 2023 CLIP Loss NeRF CLIP SKED [359] ICCV 2023 Score Distillation NeRF SD DreamEditor [360] SIGGRAPH Asia 2023 Score Distillation NeRF SD Instruct-NeRF2NeRF [361] SIGGRAPH Asia 2023 Score Distillation NeRF SD TextDeformer [357] TVCG 2022 Score Distillation Mesh SD SINE [358] CVPR 2023 Score Distillation NeRF SD Blending-NeRF [378] ICCV2023 CLIP Loss NeRF CLIP CustomNeRF [379] CVPR 2024 Score Distillation NeRF SD Paint3D [380] arXiv 2023 Mesh SD\n3D Paintbrush [362] arXiv 2023 Score Distillation NeRF SD\n20 TABLE 8: Audio datasets that can be adopted for language-based audio research.",
        "3": "[358] C."
      },
      "TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts": {
        "authors": [
          "J Zhuang",
          "D Kang",
          "YP Cao",
          "G Li",
          "L Lin"
        ],
        "url": "https://arxiv.org/pdf/2401.14828",
        "ref_texts": "Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. 2023. Break-a-scene: Extracting multiple concepts from a single image. In SIGGRAPH Asia 2023 Conference Papers . 1\u201312. Omri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for text-driven editing of natural images. In CVPR 2022 . 18208\u201318218. Chong Bao, Yinda Zhang, and Bangbang et al. Yang. 2023. SINE: Semantic-driven imagebased nerf editing with prior-guided editing field. In CVPR 2023 . 20919\u201320929. Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2022. InstructPix2Pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800 (2022). de Charette Raoul Cao, Anh-Quan. 2023. SceneRF: Self-supervised monocular 3D scene reconstruction with radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision . 9387\u20139398. Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei Shen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. 2023. Segment Anything in 3D with NeRFs. In NeurIPS . Jun-Kun Chen, Jipeng Lyu, and Yu-Xiong Wang. 2023c. Neuraleditor: Editing neural radiance fields via manipulating point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 12439\u201312448. Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023b. Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. arXiv preprint arXiv:2303.13873 (2023). Yiwen Chen, Zilong Chen, and Chi .eta Zhang. 2023a. GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting. arXiv preprint arXiv:2311.14521"
      },
      "Consolidating Attention Features for Multi-view Image Editing": {
        "authors": [
          "O Patashnik",
          "R Gal",
          "D Cohen-Or",
          "JY Zhu"
        ],
        "url": "https://arxiv.org/pdf/2402.14792",
        "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023. 3",
        "ref_ids": [
          "2"
        ],
        "1": "Recent works employ advances in text-based image editing to edit an implicit 3D representation with text [2, 20, 47, 54, 55]."
      },
      "Coin3D: Controllable and Interactive 3D Assets Generation with Proxy-Guided Conditioning": {
        "authors": [
          "W Dong",
          "B Yang",
          "L Ma",
          "X Liu",
          "L Cui",
          "H Bao"
        ],
        "url": "https://arxiv.org/pdf/2405.08054",
        "ref_texts": "Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. 2018. Learning representations and generative models for 3d point clouds. In International conference on machine learning . PMLR, 40\u201349. Chong Bao, Yinda Zhang, Yuan Li, Xiyu Zhang, Bangbang Yang, Hujun Bao, Marc Pollefeys, Guofeng Zhang, and Zhaopeng Cui. 2024. GeneAvatar: Generic ExpressionAware Volumetric Head Avatar Editing from a Single Image. arXiv preprint arXiv:2404.02152 (2024). Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. 2023. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 20919\u201320929. Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. 2023. Multidiffusion: Fusing diffusion paths for controlled image generation. (2023). Shariq Farooq Bhat, Niloy J Mitra, and Peter Wonka. 2023. LooseControl: Lifting ControlNet for Generalized Depth Conditioning. arXiv preprint arXiv:2312.03079"
      },
      "RHINO: Regularizing the Hash-based Implicit Neural Representation": {
        "authors": [
          "H Zhu",
          "F Liu",
          "Q Zhang",
          "X Cao",
          "Z Ma"
        ],
        "url": "https://arxiv.org/pdf/2309.12642",
        "ref_texts": "(2023) Sine: Semantic-driven image-based nerf editing with priorguided editing field. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp 20919\u201320929 Barron JT, Mildenhall B, Tancik M, Hedman P, Martin-Brualla R, Srinivasan PP (2021) Mip-nerf: A multiscale representation for antialiasing neural radiance fields. In: Proceedings of the IEEE/CVF International Conference on Computer Vision , pp 5855\u20135864 Cao A, Johnson J (2023) Hexplane: A fast representation for dynamic scenes. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp 130\u2013141 Chabra R, Lenssen JE, Ilg E, Schmidt T, Straub J, Lovegrove S, Newcombe R (2020) Deep local shapes: Learning local sdf priors for detailed 3d reconstruction. In: European Conference on Computer Vision , Springer, pp 608\u2013625 Chan ER, Lin CZ, Chan MA, Nagano K, Pan B, De Mello S, Gallo O, Guibas LJ, Tremblay J, Khamis S, et al. (2022) Efficient geometryaware 3d generative adversarial networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 16123\u201316133 Chen A, Xu Z, Zhao F, Zhang X, Xiang F, Yu J, Su H (2021) MVSNeRF: Fast generalizable radiance field reconstruction from multiview stereo. In: Proceedings of the IEEE/CVF International Conference on Computer Vision , pp 14124\u201314133 Chen A, Xu Z, Geiger A, Yu J, Su H (2022) Tensorf: Tensorial radiance fields. In: European Conference on Computer Vision , Springer, pp 333\u2013350 Chen Y, Lu L, Karniadakis GE, Dal Negro L (2020) Physics-informed neural networks for inverse problems in nano-optics and metamaterials. Optics express 28(8):11618\u201311633 Dupont E, Goli \u00b4nski A, Alizadeh M, Teh YW, Doucet A (2021) Coin: Compression with implicit neural representations. arXiv preprint arXiv:210303123 Fang J, Yi T, Wang X, Xie L, Zhang X, Liu W, Nie\u00dfner M, Tian Q"
      },
      "Advances in 3D Neural Stylization: A Survey": {
        "authors": [
          "Y Chen",
          "G Shao",
          "KC Shum",
          "BS Hua"
        ],
        "url": "https://arxiv.org/pdf/2311.18328"
      },
      "StyleGaussian: Instant 3D Style Transfer with Gaussian Splatting": {
        "authors": [
          "K Liu",
          "F Zhan",
          "M Xu",
          "C Theobalt",
          "L Shao"
        ],
        "url": "https://arxiv.org/pdf/2403.07807",
        "ref_texts": "1. Bao, C., Zhang, Y., Yang, B., Fan, T., Yang, Z., Bao, H., Zhang, G., Cui, Z.: Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition. pp. 20919\u201320929 (2023) 4",
        "ref_ids": [
          "1"
        ],
        "1": "Consequently, previous studies have resorted to learning-based methods for editing radiance fields [1,6,7,14,23\u201325, 34,42,45,46,50,52,57], guided by images [1,7,24,53], text [6,14,45,46,57], or other forms of user input [6,25,50], encompassing modifications such as deformation [34,50,52], appearance changes [6,14,24,45,46,57], removal [6], relighting [42], and inpainting [23,28]."
      },
      "DATENeRF: Depth-Aware Text-based Editing of NeRFs": {
        "authors": [
          "S Rojas",
          "J Philip",
          "K Zhang",
          "S Bi",
          "F Luan"
        ],
        "url": "https://arxiv.org/pdf/2404.04526",
        "ref_texts": "2. Bao, C., Zhang, Y., Yang, B., Fan, T., Yang, Z., Bao, H., Zhang, G., Cui, Z.: Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition. pp. 20919\u201320929 (2023) 3",
        "ref_ids": [
          "2"
        ],
        "1": "SINE [2] transfers edits from a single edited image across the entire scene using a ViT model [6] as a semantic texture prior."
      },
      "NeRF: Multi-Modal Decomposition NeRF with 3D Feature Fields": {
        "authors": [
          "N Wang",
          "L Zhang",
          "AX Chang"
        ],
        "url": "https://arxiv.org/pdf/2405.05010",
        "ref_texts": "1. Bao, C., Zhang, Y., Yang, B., Fan, T., Yang, Z., Bao, H., Zhang, G., Cui, Z.: Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In: CVPR. pp. 20919\u201320929 (2023) 4",
        "ref_ids": [
          "1"
        ],
        "1": "Neural rendering has spurred an exploration into implicit and hybrid representations, offering various approaches for 3D editing, such as changing global appearance [7,29], intrinsic decomposition [59,64], per-object decomposition [54, 56], geometry and texture editing [1,55,61], 3D inpainting [36,52], and others [20,39,50]."
      },
      "Semantically-aware Neural Radiance Fields for Visual Scene Understanding: A Comprehensive Review": {
        "authors": [
          "TAQ Nguyen",
          "A Bourki",
          "M Macudzinski"
        ],
        "url": "https://arxiv.org/pdf/2402.11141",
        "ref_texts": ""
      },
      "SIGNeRF: Scene Integrated Generation for Neural Radiance Fields": {
        "authors": [
          "JN Dihlmann",
          "A Engelhardt",
          "H Lensch"
        ],
        "url": "https://arxiv.org/pdf/2401.01647",
        "ref_texts": "[2]Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. pages 20919\u201320929, 2023. 3",
        "ref_ids": [
          "2"
        ],
        "1": "On the other hand, SINE [2] allows direct NeRF editing by changing a reference image in 2D space."
      },
      "Noise-NeRF: Hide Information in Neural Radiance Fields using Trainable Noise": {
        "authors": [
          "Q Huang",
          "Y Liao",
          "Y Hao",
          "P Zhou"
        ],
        "url": "https://arxiv.org/pdf/2401.01216",
        "ref_texts": "[11] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui, \u201cSine: Semanticdriven image-based nerf editing with prior-guided editing field,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023, pp. 20919\u201320929.",
        "ref_ids": [
          "11"
        ],
        "1": "There are currently many improvements and application research on NeRF, including accelerated training of NeRF [8]\u2013[10], editing research on NeRF [11]\u2013\n[13], generalization research on NeRF [14]\u2013[17], and largescale scenes [18], [19], etc."
      },
      "4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance Fields via 4D Semantic Segmentation": {
        "authors": [
          "D Jiang",
          "Z Ke",
          "X Zhou",
          "X Shi"
        ],
        "url": "https://arxiv.org/pdf/2310.16858",
        "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023. 2",
        "ref_ids": [
          "2"
        ],
        "1": "Additionally, researchers have proposed blending editing methods [2, 14], which combine an auxiliary editing field with original NeRF to support creative editing."
      },
      "Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh": {
        "authors": [
          "X Gao",
          "X Li",
          "Y Zhuang",
          "Q Zhang",
          "W Hu"
        ],
        "url": "https://arxiv.org/pdf/2405.17811",
        "ref_texts": "1. Bao, C., Zhang, Y., Yang, B., Fan, T., Yang, Z., Bao, H., Zhang, G., Cui, Z.: Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition. pp. 20919\u201320929 (2023) 4",
        "ref_ids": [
          "1"
        ],
        "1": "Some other work [1,32,34,44] edit the NeRF in texture level which is not the focus of this paper."
      },
      "NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs": {
        "authors": [
          "M Fischer",
          "Z Li",
          "T Nguyen-Phuoc",
          "A Bozic"
        ],
        "url": "https://arxiv.org/pdf/2402.08622",
        "ref_texts": "[4] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023. 2",
        "ref_ids": [
          "4"
        ],
        "1": "Most of the aforementioned methods, however, ignore semantic similarity while performing stylization or appearance editing, with the exception of [4, 30, 50], who perform region-based stylization or appearance-editing of NeRFs, but do not change geometry.",
        "2": ", excluding topological changes [4, 30, 68])."
      },
      "SERF: Fine-Grained Interactive 3D Segmentation and Editing with Radiance Fields": {
        "authors": [
          "K Zhou",
          "L Hong",
          "E Xie",
          "Y Yang",
          "Z Li"
        ],
        "url": "https://arxiv.org/pdf/2312.15856",
        "ref_texts": "[3] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 1, 9",
        "ref_ids": [
          "3"
        ],
        "1": "In contrast, approaches like Get3D [17] and SINE [3] offer direct editing in the feature space, deviating from the high-resolution 2D feature maps common in 2D editing.",
        "2": "Subsequently, we leverage Du and{eto estimate the scene flow \u222beby applying the scene flow MLP proposed in DynPoint [75] and SINE [3]."
      },
      "Survey on controlable image synthesis with deep learning": {
        "authors": [
          "S Zhang",
          "J Li",
          "L Yang"
        ],
        "url": "https://arxiv.org/pdf/2307.10275",
        "ref_texts": "[171] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with priorguided editing field,\u201d in The IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) , 2023.",
        "ref_ids": [
          "171"
        ],
        "1": "[171] proposed SINE, a novel approach for editing a neural radiance field (NeRF) with a single image or text prompts.",
        "2": "[171] C."
      },
      "Three-Dimensional-Consistent Scene Inpainting via Uncertainty-Aware Neural Radiance Field": {
        "authors": [
          "Meng Wang",
          "Qinkang Yu",
          "Haipeng Liu"
        ],
        "url": "https://www.mdpi.com/2079-9292/13/2/448/pdf",
        "ref_texts": "9. Bao, C.; Zhang, Y.; Yang, B.; Fan, T.; Yang, Z.; Bao, H.; Zhang, G.; Cui, Z. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Vancouver, BC, Canada, 17\u201324 June 2023; pp. 20919\u201320929.",
        "ref_ids": [
          "9"
        ],
        "1": "Numerous endeavors have aimed at augmenting their performance and expanding their applicability via, for instance, improving the training speed [3\u20135], reducing view input requirements [6,7], facilitating scene editing [8,9], and extending their functionality to dynamic scenes [10,11].",
        "2": "Driven by the needs of practical applications, NeRf editing methods [9,12\u201319] have emerged as a focal point of current research.",
        "3": "Furthermore, research related to interactive NeRF editing has focused more on interactive target selection [16] or semantic editing [9]."
      },
      "SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field": {
        "authors": [
          "Z Song",
          "W Duan",
          "Y Zhang",
          "S Wang",
          "S Ma"
        ],
        "url": "https://arxiv.org/pdf/2402.16366",
        "ref_texts": "[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 2",
        "ref_ids": [
          "2"
        ],
        "1": "Neural Radiance Field Neural radiance field [28] has shown great ability in 3D reconstruction, and motivated massive follow-up works, such as editing [2, 43] and speeding-up [7, 29]."
      },
      "3D StreetUnveiler with Semantic-Aware 2DGS": {
        "authors": [
          "J Xu",
          "Y Wang",
          "Y Zhao",
          "Y Fu",
          "S Gao"
        ],
        "url": "https://arxiv.org/pdf/2405.18416",
        "ref_texts": "[2]Chong Bao, Yinda Zhang, and Bangbang et al. Yang. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In CVPR , pages 20919\u201320929, 2023.",
        "ref_ids": [
          "2"
        ],
        "1": "With the rapid development of Neural Scene Representation, editing a 3D scene has been explored by lots of works [10,88,75,81,2,23,19,40].",
        "2": "Subsequent works [2,23,19,40] utilized CLIP models to provide editing guidance from text prompts or reference images.",
        "3": "[2]Chong Bao, Yinda Zhang, and Bangbang et al."
      },
      "Semantic-Human: Neural Rendering of Humans from Monocular Video with Human Parsing": {
        "authors": [
          "J Zhang",
          "P Shi",
          "Z Gu",
          "Y Zhou",
          "Z Wang"
        ],
        "url": "https://arxiv.org/pdf/2308.09894",
        "ref_texts": "Bao, C.; Zhang, Y .; Yang, B.; Fan, T.; Yang, Z.; Bao, H.; Zhang, G.; and Cui, Z. 2023. Sine: Semantic-driven imagebased nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 20919\u201320929. Barron, J. T.; Mildenhall, B.; Tancik, M.; Hedman, P.; Martin-Brualla, R.; and Srinivasan, P. P. 2021. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision , 5855\u20135864. Cao, A.; and Johnson, J. 2023. Hexplane: A fast representation for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 130\u2013141. Chen, Y .; Wang, X.; Chen, X.; Zhang, Q.; Li, X.; Guo, Y .; Wang, J.; and Wang, F. 2023. UV V olumes for real-time rendering of editable free-view human performance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 16621\u201316631. Cheng, W.; Xu, S.; Piao, J.; Qian, C.; Wu, W.; Lin, K.-Y .; and Li, H. 2022. Generalizable neural performer: Learning robust radiance fields for human novel view synthesis. arXiv preprint arXiv:2204.11798 . Deng, C. L.; and Tartaglione, E. 2023. Compressing explicit voxel grid representations: fast nerfs become also small. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , 1236\u20131245. Fridovich-Keil, S.; Meanti, G.; Warburg, F. R.; Recht, B.; and Kanazawa, A. 2023. K-planes: Explicit radiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 12479\u201312488. Gong, K.; Liang, X.; Li, Y .; Chen, Y .; Yang, M.; and Lin, L."
      },
      "VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System in Virtual Reality": {
        "authors": [
          "Y Jiang",
          "C Yu",
          "T Xie",
          "X Li",
          "Y Feng",
          "H Wang",
          "M Li"
        ],
        "url": "https://arxiv.org/pdf/2401.16663",
        "ref_texts": "(2009), 114\u2013123. Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. 2023. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 20919\u201320929. Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Xu Jia, and Huchuan Lu. 2021. Animatable neural radiance fields from monocular rgb videos. arXiv preprint arXiv:2106.13629 (2021). Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. 2023. Gaussianeditor: Swift and controllable 3d editing with gaussian splatting. arXiv preprint arXiv:2311.14521"
      },
      "DragGaussian: Enabling Drag-style Manipulation on 3D Gaussian Representation": {
        "authors": [
          "S Shen",
          "J Xu",
          "Y Yuan",
          "X Yang",
          "Q Shen"
        ],
        "url": "https://arxiv.org/pdf/2405.05800"
      },
      "LIVE: LaTex Interactive Visual Editing": {
        "authors": [
          "J Lin"
        ],
        "url": "https://arxiv.org/pdf/2405.06762",
        "ref_texts": "[33] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with priorguided editing field,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023, pp. 20 919\u201320 929.",
        "ref_ids": [
          "33"
        ],
        "1": "2021\n|{z}2022\n|{z}2023\n|{z}\n\u22c6\u2192 \u22c6\u22c6\u2192 \u22c6 \u22c6 \u22c6\u2192\n\u2191 \u2191 \u2191\n[13]NeRF\n[28]pixelnerf [29]NeRF -[15]Mip -nerf [14]D-nerf[30]Headnerf [21]Block -nerf [31]Mip -nerf360[32]Nope -nerf [33]Sine The main function of Gitem FlowGraph is automatically generating a time sequence or other developing sequence flow interactive graph to describe the development of one issue.",
        "2": "[33] C."
      },
      "Plasticine3D: Non-rigid 3D editting with text guidance": {
        "authors": [
          "Y Chen",
          "A Chen",
          "S Chen",
          "R Yi"
        ],
        "url": "https://arxiv.org/pdf/2312.10111",
        "ref_texts": "[3] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 20919\u201320929, 2023. 1, 3",
        "ref_ids": [
          "3"
        ],
        "1": "(4) Directly perform a controllable non-rigid transformation with a modified 3D NeRF representation[3], however they often require an additional guidance (e.",
        "2": "SINE[3] on the other hand, realizes controllable non-rigid editing by a modified 3D NeRF representation."
      },
      "ViFu: Multiple 360 Objects Reconstruction with Clean Background via Visible Part Fusion": {
        "authors": [
          "T Xu",
          "T Ikeda",
          "K Nishiwaki"
        ],
        "url": "https://arxiv.org/pdf/2404.09426",
        "ref_texts": "[27] C. Bao, Y . Zhang, B. Yang, T. Fan, Z. Yang, H. Bao, G. Zhang, and Z. Cui, \u201cSine: Semantic-driven image-based nerf editing with priorguided editing field,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2023.",
        "ref_ids": [
          "27"
        ],
        "1": "Another direction explores object-level manipulations on scene content, enabling editing to object appearance [26], [27] or geometry [4], [5].",
        "2": "[27] C."
      },
      "ViFu: Visible Part Fusion for Multiple Scene Radiance Fields": {
        "authors": [
          "T Xu",
          "T Ikeda",
          "K Nishiwaki"
        ],
        "url": "https://openreview.net/pdf?id=C3msSjudA7",
        "ref_texts": "Kaxlamangla S. Arun, T. S. Huang, and Steven D. Blostein. Least-squares fitting of two 3-d point sets. IEEE Transactions on Pattern Analysis and Machine Intelligence , PAMI-9:698\u2013700, 1987. Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 20919\u201320929, 2023. Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M Dollar. The ycb object and model set: Towards common benchmarks for manipulation research. In 2015 international conference on advanced robotics (ICAR) , pp. 510\u2013517. IEEE, 2015. Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In European Conference on Computer Vision , 2022. Blender Online Community. Blender a 3D modelling and rendering package . Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. URL http://www.blender.org . Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Michael Hickman, Krista Reymann, Thomas Barlow McHugh, and Vincent Vanhoucke. Google scanned objects: A highquality dataset of 3d scanned household items. 2022 International Conference on Robotics and Automation (ICRA) , pp. 2553\u20132560, 2022. Martin Ester, Hans-Peter Kriegel, J \u00a8org Sander, and Xiaowei Xu. A density-based algorithm for discovering clusters in large spatial databases with noise. In Knowledge Discovery and Data Mining , 1996. Jiading Fang, Shengjie Lin, Igor Vasiljevic, Vitor Guizilini, Rares Ambrus, Adrien Gaidon, Gregory Shakhnarovich, and Matthew R Walter. Nerfuser: Large-scale scene representation by nerf fusion. arXiv preprint arXiv:2305.13307 , 2023. Martin A. Fischler and Robert C. Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Commun. ACM , 24:381\u2013395, 1981. Michelle Guo, Alireza Fathi, Jiajun Wu, and Thomas Funkhouser. Object-centric neural scene rendering. arXiv preprint arXiv:2012.08503 , 2020. Won Jun Jang and Lourdes de Agapito. Codenerf: Disentangled neural radiance fields for object categories. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 12929\u2013"
      },
      "Blended-NeRF: Zero-Shot Object Generation and Blending In Existing Neural Radiance Fields (Supplementary Material)": {
        "authors": [
          "O Gordon",
          "O Avrahami",
          "D Lischinski"
        ],
        "url": "https://openaccess.thecvf.com/content/ICCV2023W/AI3DCC/supplemental/Gordon_Blended-NeRF_Zero-Shot_Object_ICCVW_2023_supplemental.pdf",
        "ref_texts": "[1] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan, Zesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Sine: Semantic-driven image-based nerf editing with prior-guided editing field. arXiv preprint arXiv:2303.13277 , 2023. 5",
        "ref_ids": [
          "1"
        ],
        "1": "After sampling a camera pose, we recenter its rays around the ROI by moving its center location according to the center of mass inside the ROI (tracked by exponential moving average during training), but allow with a probability p\u2208[0,1](hyperparameter, set to 0.",
        "2": "In SINE [1], they suggest a method for editing NeRF scene by only editing a single view, and than apply the edit to the entire scene."
      }
    }
  },
  {
    "title": "mobile3dscanner: an online 3d scanner for high-quality object reconstruction with a mobile device",
    "id": 2,
    "valid_pdf_number": "2/4",
    "matched_pdf_number": "1/2",
    "matched_rate": 0.5,
    "citations": {
      "A portable V-SLAM based solution for advanced visual 3D mobile mapping": {
        "authors": [
          "A Torresani"
        ],
        "url": "https://iris.unitn.it/bitstream/11572/362031/1/PhD_thesis_final.pdf",
        "ref_texts": "[39] X. Xiang, H. Jiang, G. Zhang, Y. Yu, C. Li, X. Yang, D. Chen, and H.Bao, \u201cMobile3dscanner: Anonline3dscannerforhigh-qualityobject 93 reconstruction with a mobile device,\u201d IEEE Transactions on Visualization and Computer Graphics , vol. 27, no. 11, pp. 4245\u20134255, 2021.",
        "ref_ids": [
          "39"
        ],
        "1": "Since then, different real-time 3D reconstruction applications have been developed, both in the research [39] and commercial domains [40, 41, 42].",
        "2": "[39] X."
      },
      "\u30e2\u30eb\u30d5\u30a9\u30ed\u30b8\u30fc\u6f14\u7b97\u3092\u7528\u3044\u305f\u70b9\u7fa4\u30c7\u30fc\u30bf\u306e\u6b20\u640d\u691c\u51fa\u6cd5": {
        "authors": [
          "\u624b\u5cf6\u88d5\u8a5e\uff0c \u4e2d\u5c3e\u4eae\uff0c \u677e\u7530\u671d\u967d\uff0c \u5d8b\u7530\u82f1\u6a39\uff0c \u517c\u7530\u4e00\u5e78"
        ],
        "url": "https://www.jstage.jst.go.jp/article/jjsde/59/2/59_2023.2993/_pdf",
        "ref_texts": "8) Xiang, X., Jiang, H., Zhang, G., Yu, Y., Li, C., Yang , X., Chen, D. and Bao, H.: Mobile3DScanner: An "
      }
    }
  },
  {
    "title": "mobile3drecon: real-time monocular 3d reconstruction on a mobile phone",
    "id": 3,
    "valid_pdf_number": "32/44",
    "matched_pdf_number": "23/32",
    "matched_rate": 0.71875,
    "citations": {
      "Neuralrecon: Real-time coherent 3d reconstruction from monocular video": {
        "authors": [
          "Jiaming Sun",
          "Yiming Xie",
          "Linghao Chen",
          "Xiaowei Zhou",
          "Hujun Bao"
        ],
        "url": "http://openaccess.thecvf.com/content/CVPR2021/papers/Sun_NeuralRecon_Real-Time_Coherent_3D_Reconstruction_From_Monocular_Video_CVPR_2021_paper.pdf",
        "ref_texts": "[51] Xingbin Yang, L. Zhou, Hanqing Jiang, Z. Tang, Yuanbo Wang, H. Bao, and Guofeng Zhang. Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone. IEEE TVCG , 2020. 2",
        "ref_ids": [
          "51"
        ],
        "1": "[46,51] optimize this line of research towards low power consumption on mobile platforms.",
        "2": "2\n[51] Xingbin Yang, L."
      },
      "Depthformer: Exploiting long-range correlation and local information for accurate monocular depth estimation": {
        "authors": [
          "Z Li",
          "Z Chen",
          "X Liu",
          "J Jiang"
        ],
        "url": "https://link.springer.com/content/pdf/10.1007/s11633-023-1458-0.pdf",
        "ref_texts": "\u00a0X.\u00a0B.\u00a0Yang,\u00a0 L.\u00a0Y.\u00a0Zhou,\u00a0 H.\u00a0Q.\u00a0Jiang,\u00a0 Z.\u00a0L.\u00a0Tang,\u00a0 Y.\u00a0B. Wang,\u00a0 H.\u00a0J.\u00a0Bao,\u00a0 G.\u00a0F.\u00a0Zhang.\u00a0 Mobile3DRecon:\u00a0 Real-time monocular\u00a0 3D\u00a0reconstruction\u00a0 on\u00a0a\u00a0mobile\u00a0 phone.\u00a0 IEEE Transactions on Visualization and Computer Graphics , vol.\u00a026,\u00a0no.\u00a012,\u00a0pp.\u00a03446\u20133456,\u00a0 2020.\u00a0 DOI:\u00a0 10.1109/TVCG."
      },
      "Simplerecon: 3d reconstruction without 3d convolutions": {
        "authors": [
          "M Sayed",
          "J Gibson",
          "J Watson",
          "V Prisacariu"
        ],
        "url": "https://arxiv.org/pdf/2208.14743",
        "ref_texts": "77. Yang, X., Zhou, L., Jiang, H., Tang, Z., Wang, Y., Bao, H., Zhang, G.: Mobile3DRecon: Real-time monocular 3D reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics (2020)",
        "ref_ids": [
          "77"
        ],
        "1": "io/simplerecon 1 Introduction Generating 3D reconstructions of a scene is a challenging problem in computer vision which is useful for tasks such as robotic navigation, autonomous driving, content placement for augmented reality and historical preservation [47,77]."
      },
      "What's the Situation With Intelligent Mesh Generation: A Survey and Perspectives": {
        "authors": [
          "N Lei",
          "Z Li",
          "Z Xu",
          "Y Li",
          "X Gu"
        ],
        "url": "https://arxiv.org/pdf/2211.06009",
        "ref_texts": "[73] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang, \u201cMobile3drecon: real-time monocular 3d reconstruction on a mobile phone,\u201d IEEE TVCG , vol. 26, no. 12, pp. 3446\u2013",
        "ref_ids": [
          "73"
        ],
        "1": "[57] \u2713 \u2713 \u2713 U-Net Normal, depth maps DMC [58] \u2713 \u2713 \u2713 \u2713 \u2713 DMC Occupancy and vertex displacement CoMA [59] \u2713 \u2713\u2713 \u2713 Autoencoder Point position 3D-CFCN [60] \u2713 \u2713 \u2713 OctNet-based U-Net Truncated signed distance field MGN [11] \u2713 \u2713 \u2713 \u2713 \u2713 MLP Vertices position 3DN [35] \u2713 \u2713 \u2713 \u2713 \u2713 PointNet/VGG Vertices position TMN [12] \u2713 \u2713 \u2713 \u2713 ResNet/MLP Vertices position and errors ONet [61] \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 ResNet/PointNet Grid occupancy N3DMM [62] \u2713 \u2713\u2713 \u2713 Spiral-Conv GAN Vertices position PGAN [63] \u2713 \u2713\u2713 WGAN Geometry image HumanMeshNet [13] \u2713 \u2713 \u2713 \u2713 Resnet-18 Vertices position DISN [64] \u2713 \u2713 \u2713 \u2713 VGG-16 Signed distance field IM-Net [65] \u2713\u2713 \u2713\u2713 \u2713 IM-Net Signed distance field Scan2Mesh [66] \u2713 \u2713 \u2713 \u2713 3D-Conv GNN Mesh face DGP [67] \u2713 \u2713 \u2713 \u2713 MLP Local parametrization Mesh R-CNN [14] \u2713 \u2713 \u2713 \u2713 Mesh R-CNN Occupancy and point position DeepSDF [68] \u2713 \u2713 \u2713 MLP Signed distance field Pixel2mesh++ [15] \u2713 \u2713 \u2713 \u2713 VGG; GCN Vertex position PQ-Net [69] \u2713\u2713 \u2713 \u2713\u2713 Seq2Seq Autoencoder Signed distance field BCNet [16] \u2713 \u2713 \u2713 \u2713 ResNet; GAT; Spiral-Conv SMPL parameters; vertices position PolyGen [70] \u2713 \u2713\u2713 \u2713 \u2713 Transformer-based Predict vertices and faces sequentially DGTS [71] \u2713 \u2713 \u2713 \u2713 GCN Displacement vector per face Neural Subdivision [72] \u2713 \u2713 \u2713 \u2713 MLP Predict vertex position Mobile3drecon [73] \u2713 \u2713 \u2713 Res-UNet Depth map Sal [74] \u2713 \u2713 \u2713 MLP Unsigned distance field Pixel2mesh2 [17] \u2713 \u2713 \u2713 \u2713 GCN;G-Resnet Vertex position Voxel2mesh [40] \u2713 \u2713 \u2713 \u2713 GCN-3D Vertex position X-ray2shape [18] \u2713 \u2713 \u2713 \u2713 CNN;GCN Vertex position Liet al.",
        "2": "According to the different selected implicit functions, these IMG methods are divided into four categories: radial basis functions (RBFs) [52], occupancy fields [32], [54], [57], [58], [61], [73], [78], [84], [88], [91], [110], [119], [121], signed distance functions (SDFs) [39], [60], [64], [65], [69], [76], [77], [80], [87], [90], [92], [94]\u2013[97], [99], [101], [103], [107]\u2013[109], [113], [117], [120], and unsigned distance function (UDFs) [74], [86], [89], [100].",
        "3": "Currently, various occupancy-based methods with different network architectures and training strategies have been proposed to continuously improve the efficiency, robustness, and accuracy [32], [57], [73], [78], [84], [88], [91], [110], [119], [121].",
        "4": "83 Mobile3drecon [73] Real-time mesh generation Real-time dense mesh reconstruction Poorly maintained sharp features 3.",
        "5": "Existing methodologies [13], [28], [73], [90], [91], [93], [95], [110], [119], however, fall short of satisfying these criteria simultaneously, resulting in scene meshes that tend to be overly smooth and lacking in detail.",
        "6": "[73] X."
      },
      "Transformr: Pose-aware object substitution for composing alternate mixed realities": {
        "authors": [
          "M Kari",
          "T Grosse-Puppendahl"
        ],
        "url": "https://mkari.de/pubs/ismar2021-transformr.pdf",
        "ref_texts": "[63] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang. Mobile3drecon: Real-time monocular 3D reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics , 26(12):3446\u20133456, 2020. doi: 10.1109/TVCG.2020.3023634",
        "ref_ids": [
          "63"
        ],
        "1": "Index Terms: Human-centered computing\u2014Mixed / augmented reality\u2014;\u2014\n1 I NTRODUCTION Continuous advances in geometric scene understanding have contributed to the physical coherence of virtual objects in mixed reality scenes, for example through improvements in mesh reconstruction [63], occlusion shading [7], visual-inertial odometry [20,57], or light source estimation [59].",
        "2": "3347875\n[63] X."
      },
      "GR-PSN: learning to estimate surface normal and reconstruct photometric stereo images": {
        "authors": [
          "Y Ju",
          "B Shi",
          "Y Chen",
          "H Zhou",
          "J Dong"
        ],
        "url": "https://figshare.le.ac.uk/articles/journal_contribution/GR-PSN_Learning_to_Estimate_Surface_Normal_and_Reconstruct_Photometric_Stereo_Images/24463159/1/files/42980308.pdf",
        "ref_texts": "[3] Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, and Guofeng Zhang, \u201cMobile3drecon: real-time monocular 3d reconstruction on a mobile phone,\u201d IEEE Transactions on Visualization and Computer Graphics, vol. 26, no. 12, pp. 3446\u20133456, 2020.",
        "ref_ids": [
          "3"
        ],
        "1": "\u2726\n1 I NTRODUCTION RECOVERING the 3D shape of an object is a pivotal problem in many computer graphics and vision applications because it can further improve the understanding of images and scenes [1], [2], [3], [4]."
      },
      "MobiDepth: real-time depth estimation using on-device dual cameras": {
        "authors": [
          "Jinrui Zhang",
          "Huan Yang",
          "Ju Ren",
          "Deyu Zhang",
          "Bangwen He",
          "Ting Cao",
          "Yuanchun Li",
          "Yaoxue Zhang",
          "Yunxin Liu"
        ],
        "url": "https://dl.acm.org/doi/pdf/10.1145/3495243.3560517",
        "ref_texts": "[54] Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, and Guofeng Zhang. 2020. Mobile3DRecon: real-time monocular 3D reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics 26, 12 (2020), 3446\u20133456.",
        "ref_ids": [
          "54"
        ],
        "1": "3560517\n1 INTRODUCTION In recent years, the mobile industry and research community have significantly invested in augmented reality (AR) and virtual reality (VR) applications for mobile devices [8,11,20,37,53,54].",
        "2": "[54] propose a keyframe-based real-time surface mesh generation approach to reconstruct 3D objects from single RGB image."
      },
      "Mononeuralfusion: Online monocular neural 3d reconstruction with geometric priors": {
        "authors": [
          "ZX Zou",
          "SS Huang",
          "YP Cao",
          "TJ Mu",
          "Y Shan"
        ],
        "url": "https://arxiv.org/pdf/2209.15153",
        "ref_texts": "[18] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang, \u201cMobile3drecon: real-time monocular 3d reconstruction on a mobile phone,\u201d IEEE Transactions on Visualization and Computer Graphics , vol. 26, no. 12, pp. 3446\u20133456, 2020.[19] A. Gordon, H. Li, R. Jonschkowski, and A. Angelova, \u201cDepth from videos in the wild: Unsupervised monocular depth learning from unknown cameras,\u201d in IEEE ICCV , 2019, pp. 8977\u20138986.",
        "ref_ids": [
          "18",
          "19"
        ],
        "1": "With the progress of deep learning, some pioneering works [16], [17], [18] adopt the single-view depth estimation to monoc\u2022Zi-Xin Zou and Tai-Jiang Mu are with BNRist, the Department of Computer Science and Technology, Tsinghua University, Beijing, China.",
        "2": "However, given effective deep learning based monocular depth estimation approaches [19], [20], [21], it is still challenging to generate consistent depth estimation across different views, making it difficult to build coherent 3D reconstruction for large-scale VA/AR applications.",
        "3": "Mobile3DRecon [18] uses a multi-view semi-global matching method followed by a depth refinement post-processing for robust monocular depth estimation.",
        "4": "[18] X.",
        "5": "[19] A."
      },
      "FarfetchFusion: Towards Fully Mobile Live 3D Telepresence Platform": {
        "authors": [
          "Kyungjin Lee"
        ],
        "url": "https://scholar.archive.org/work/q4ljkmkzpjcgrc77uzr33txllu/access/wayback/https://dl.acm.org/doi/pdf/10.1145/3570361.3592525",
        "ref_texts": "[56] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang. Mobile3drecon: Real-time monocular 3d reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics , 26(12):3446\u20133456, 2020.",
        "ref_ids": [
          "56"
        ],
        "1": "Existing mobile 3D reconstruction systems that leverages TSDF-based volumetric fusion focus on reconstructing static 3D scenes [38,42,56].",
        "2": "[56] X."
      },
      "The robodepth challenge: Methods and advancements towards robust depth estimation": {
        "authors": [
          "L Kong",
          "Y Niu",
          "S Xie",
          "H Hu",
          "LX Ng"
        ],
        "url": "https://arxiv.org/pdf/2307.15061",
        "ref_texts": "[119] Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, and Guofeng Zhang. Mobile3drecon: real-time monocular 3d reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics (TVCG) , 26(12):3446\u20133456, 2020. 9",
        "ref_ids": [
          "119"
        ],
        "1": "1 Overview Depth estimation is a fundamental task in 3D vision with vital applications, such as autonomous driving [93], augmented reality [123], virtual reality [59], and 3D reconstruction [119]."
      },
      "Fmgs: Foundation model embedded 3d gaussian splatting for holistic 3d scene understanding": {
        "authors": [
          "X Zuo",
          "P Samangouei",
          "Y Zhou",
          "Y Di",
          "M Li"
        ],
        "url": "https://arxiv.org/pdf/2401.01970",
        "ref_texts": "[59] Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, and Guofeng Zhang. Mobile3drecon: realtime monocular 3d reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics , 26(12):3446\u2013",
        "ref_ids": [
          "59"
        ],
        "1": "To estimate the dense 3d voxel cells, probabilistic fusion methods were firstly [22] used and researchers also developed end-to-end learn-able methods [50], by using either depth sensors [22] or monocular camera systems [59]."
      },
      "Efficient view path planning for autonomous implicit reconstruction": {
        "authors": [
          "J Zeng",
          "Y Li",
          "Y Ran",
          "S Li",
          "F Gao",
          "L Li"
        ],
        "url": "https://arxiv.org/pdf/2209.13159"
      },
      "Real-time globally consistent 3D reconstruction with semantic priors": {
        "authors": [
          "SS Huang",
          "H Chen",
          "J Huang",
          "H Fu"
        ],
        "url": "https://shishenghuang.github.io/index/Papers/semanticfusion/paper.pdf",
        "ref_texts": "[4] Y. Zhang, W. Xu, Y. Tong, and K. Zhou, \u201cOnline structure analysis for real-time indoor scene reconstruction,\u201d ACM Trans. Graphics , vol. 34, no. 5, pp. 159:1\u2013159:13, 2015.",
        "ref_ids": [
          "4"
        ],
        "1": "Most of early works have focused on either 3D reconstruction [1], [2], [3], [4], [5], [6], [7], [8], [9] or 3D semantic segmentation [10], [11], [12], [13], [14], [15], [16] separately.",
        "2": "[4] Y."
      },
      "Coli-ba: Compact linearization based solver for bundle adjustment": {
        "authors": [
          "Zhichao Ye",
          "Guanglin Li",
          "Haomin Liu",
          "Zhaopeng Cui",
          "Hujun Bao",
          "Guofeng Zhang"
        ],
        "url": "http://www.cad.zju.edu.cn/home/gfzhang/papers/CoLi/CoLi.pdf",
        "ref_texts": "[45] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang. Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone. IEEE Transactions on Visualization and Computer Graphics, 26(12):3446\u20133456, 2020.",
        "ref_ids": [
          "45"
        ],
        "1": "Facing large scenes, the convergence becomes extremely slow, resulting in many offline reconstruction systems timecosting [3,35, 45].",
        "2": "[45] X."
      },
      "Robust real-time AUV self-localization based on stereo vision-inertia": {
        "authors": [
          "Y Wang",
          "D Gu",
          "X Ma",
          "J Wang"
        ],
        "url": "https://repository.essex.ac.uk/34811/1/Robust_Real_Time_AUV_Self_Localization_Based_on_Stereo_Vision_Inertia__final_version.pdf",
        "ref_texts": "[36] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang, \u201cMobile3DRecon: Real-time monocular 3D reconstruction on a mobile phone,\u201d IEEE Trans. Vis. Comput. Graphics , vol. 26, no. 12, pp. 3446\u2013",
        "ref_ids": [
          "36"
        ],
        "1": "In the literature, some kinds of visionbased methods have been applied to the localization of unmanned equipment [33], [34], [35], [36].",
        "2": "[36] X.",
        "3": ": DeepSLAM: A ROBUST MONOCULAR SLAM SYSTEM WITH UNSUPERVISED DL 3587\n[36] R."
      },
      "Time-Distributed Framework for 3D Reconstruction Integrating Fringe Projection with Deep Learning": {
        "authors": [
          "Hieu Nguyen",
          "Zhaoyang Wang"
        ],
        "url": "https://www.mdpi.com/1424-8220/23/16/7284/pdf",
        "ref_texts": "32. Yang, X.; Zhuo, L.; Jiang, H.; Tang, Z.; Wang, Y.; Bao, H.; Zhang, G. Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone. IEEE Trans. Vis. Comput. Graph. 2020 ,26, 3446\u20133456. [CrossRef]",
        "ref_ids": [
          "32"
        ]
      },
      "Multi-sensor fusion self-supervised deep odometry and depth estimation": {
        "authors": [
          "Yingcai Wan",
          "Qiankun Zhao",
          "Cheng Guo",
          "Chenlong Xu",
          "Lijing Fang"
        ],
        "url": "https://www.mdpi.com/2072-4292/14/5/1228/pdf",
        "ref_texts": "3. Yang, X.; Zhou, L.; Jiang, H.; Tang, Z.; Wang, Y.; Bao, H.; Zhang, G. Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone. IEEE Trans. Vis. Comput. Graph. 2020 ,26, 3446\u20133456. [CrossRef] [PubMed]",
        "ref_ids": [
          "3"
        ],
        "1": "Introduction Dense depth estimation from an RGB image is the fundamental issue for 3D scene reconstruction that is useful for computer vision applications, such as automatic driving [1], simultaneous localization and mapping (SLAM) [2], and 3D scene understanding [3]."
      },
      "SimpleMapping: Real-time visual-inertial dense mapping with deep multi-view stereo": {
        "authors": [
          "Y Xin",
          "X Zuo",
          "D Lu"
        ],
        "url": "https://arxiv.org/pdf/2306.08648",
        "ref_texts": "[61] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang. Mobile3DRecon: real-time monocular 3D reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics , 26(12):3446\u20133456, 2020.",
        "ref_ids": [
          "61"
        ],
        "1": "Mobile3DRecon [61] employs a multi-view semi-global matching method to recover a dense depth map, which is subsequently refined by a lightweight CNN-based single-view depth refinement neural network.",
        "2": "[61] X."
      },
      "Monocular Depth Estimation: Lightweight Convolutional and Matrix Capsule Feature-Fusion Network": {
        "authors": [
          "Yinchu Wang",
          "Haijiang Zhu"
        ],
        "url": "https://www.mdpi.com/1424-8220/22/17/6344/pdf",
        "ref_texts": "5. Yang, X.; Zhou, L.; Jiang, H.; Tang, Z.; Wang, Y.; Bao, H.; Zhang, G. Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone. IEEE Trans. Vis. Comput. Graph. 2020 ,26, 3446\u20133456. [CrossRef] [PubMed]",
        "ref_ids": [
          "5"
        ],
        "1": "Obtaining a depth image of a real-world scene through depth estimation provides data that can serve as the basis for many applications, such as robots [1], autonomous driving [2], SLAM [3], augmented reality [4], 3D reconstruction [5], and segmentation [6]."
      },
      "Attention-enhanced cross-modal localization between 360 images and point clouds": {
        "authors": [
          "Z Zhao",
          "H Yu",
          "C Lyv",
          "W Yang",
          "S Scherer"
        ],
        "url": "https://arxiv.org/pdf/2212.02757",
        "ref_texts": "[2] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang, \u201cMobile3DRecon: real-time monocular 3d reconstruction on a mobile phone,\u201d IEEE Transactions on Visualization and Computer Graphics , vol. 26, no. 12, pp. 3446\u20133456, 2020.",
        "ref_ids": [
          "2"
        ],
        "1": "I NTRODUCTION Locating the position of an image inthe point cloud map is of great importance for mobile robots and autonomous vehicles with numerous applications such as Simultaneous Localization and Mapping (SLAM) [1] and Virtual Reality [2].",
        "2": "[2] X."
      },
      "A Comprehensive Review of Vision-Based 3D Reconstruction Methods": {
        "authors": [
          "Linglong Zhou",
          "Guoxin Wu",
          "Yunbo Zuo",
          "Xuanyu Chen",
          "Hongle Hu"
        ],
        "url": "https://www.mdpi.com/1424-8220/24/7/2314/pdf",
        "ref_texts": "321. Yang, X.; Zhou, L.; Jiang, H.; Tang, Z.; Wang, Y.; Bao, H.; Zhang, G. Mobile3DRecon: Real-time monocular 3D reconstruction on a mobile phone. IEEE Trans. Vis. Comput. Graph. 2020 ,26, 3446\u20133456. [CrossRef] [PubMed] Disclaimer/Publisher\u2019s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.",
        "ref_ids": [
          "321"
        ],
        "1": "The popularity of mobile devices enables users to easily conduct image-based 3D scanning and 3D reconstruction [321]."
      },
      "Depth Completion with Multiple Balanced Bases and Confidence for Dense Monocular SLAM": {
        "authors": [
          "W Xie",
          "G Chu",
          "Q Qian",
          "Y Yu",
          "H Li",
          "D Chen"
        ],
        "url": "https://arxiv.org/pdf/2309.04145",
        "ref_texts": "[60] Xingbin Yang, Liyang Zhou, Hanqing Jiang, Zhongliang Tang, Yuanbo Wang, Hujun Bao, and Guofeng Zhang. Mobile3DRecon: real-time monocular 3d reconstruction on a mobile phone. IEEE Transactions on Visualization and Computer Graphics, 26(12):3446\u20133456, 2020.",
        "ref_ids": [
          "60"
        ],
        "1": "Our fusion method is based on an online incremental mesh generation method Mobile3DRecon [60]."
      },
      "Real-time hybrid mapping of populated indoor scenes using a low-cost monocular uav": {
        "authors": [
          "S Golodetz",
          "M Vankadari",
          "A Everitt"
        ],
        "url": "https://arxiv.org/pdf/2203.02453",
        "ref_texts": "[42] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang, \u201cMobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone,\u201d TVCG , 2020.",
        "ref_ids": [
          "42"
        ],
        "1": "We select the keyframe that maximises the following score (a simplified version of that in [42]): Sf(k) =I[\u2206f t(k)\u2265\u03c4tand\u2206f \u03b8(k)\u2264\u03c4\u03b8]e(\u2212(\u2206f t(k)\u2212\u03b4t)2/\u03c32 t)(3) In this, Idenotes the binary indicator function, fdenotes the frame,kdenotes a keyframe, \u2206f tand\u2206f \u03b8respectively denote the baseline (m) and angle (\u25e6) betweenfandk,\u03b4t= 0.",
        "2": "[42] X."
      },
      "Method for automated data collection for 3d reconstruction": {
        "authors": [
          "M Zaslavskiy",
          "R Shestopalov"
        ],
        "url": "https://fruct.org/publications/volume-32/fruct32/files/Zas.pdf"
      },
      "MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth Smooth Regularization": {
        "authors": [
          "P Zhu",
          "Y Zhuang",
          "B Chen",
          "L Li",
          "C Wu",
          "Z Liu"
        ],
        "url": "https://arxiv.org/pdf/2405.06241",
        "ref_texts": "[12] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y . Wang, H. Bao, and G. Zhang, \u201cMobile3drecon: Real-time monocular 3d reconstruction on a mobile phone,\u201d IEEE Transactions on Visualization and Computer Graphics , vol. 26, no. 12, pp. 3446\u20133456, 2020.",
        "ref_ids": [
          "12"
        ],
        "1": "Another study [11], [12] combines a real-time VO/SLAM system with a Multi-View Stereo (MVS) network for parallel tracking and dense depth estimation, and then the Truncated Signed Distance Function (TSDF) [13] is used to fuse depth maps and extract mesh.",
        "2": "[12] X."
      },
      "DINO-SD: Champion Solution for ICRA 2024 RoboDepth Challenge": {
        "authors": [
          "Y Mao",
          "M Li",
          "J Liu",
          "J Liu",
          "Z Qin",
          "C Chu",
          "J Xu"
        ],
        "url": "https://arxiv.org/pdf/2405.17102"
      },
      "The present and future of mixed reality in China": {
        "authors": [
          "G Zhang",
          "X Zhou",
          "F Tian",
          "H Zha",
          "Y Wang"
        ],
        "url": "https://dl.acm.org/doi/pdf/10.1145/3481619",
        "ref_texts": "25. Yang, X., Zhou, L., Jiang, H. Tang, Z., Wang, Y., Bao, H., Zhang, G. Mobile3DRecon: real-time monocular ",
        "ref_ids": [
          "25"
        ]
      },
      "Parallel Implementation of 3D Model Reconstruction of Monocular Video Frames in a Dynamic Environment.": {
        "authors": [
          "GM Fathy",
          "HA Hassan",
          "WM Sheta",
          "F Omara"
        ],
        "url": "https://inass.org/wp-content/uploads/2022/05/2022083153-2.pdf",
        "ref_texts": "[20] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang , \u201cMobile3drecon: real time monocular 3d reconstruction on a mobile phone \u201d, IEEE Transactions on Visual -Ization and Computer Graphics , Vol. 26, No. 12, pp. ",
        "ref_ids": [
          "20"
        ],
        "1": "Scene Methods Device Time \n(T/F) \n[19] Single Static object Monocular SLAM NVIDIA GeForce TITAN X without CUDA 21 ms \n[21] Single Dynamic object Markless \n3D human motion capture GeForce RTX 2070 without CUDA 40 ms \n[22] Single Dynamic object GCN network Nvidia GeForce RTX 2080Ti \n-without CUDA 23 ms \n[20] Full Static scene Online incremental mesh generation a single CPU thread third -party library OpenCV 2 57.",
        "2": "[20] X."
      },
      "A portable V-SLAM based solution for advanced visual 3D mobile mapping": {
        "authors": [
          "A Torresani"
        ],
        "url": "https://iris.unitn.it/bitstream/11572/362031/1/PhD_thesis_final.pdf",
        "ref_texts": "[100] X. Yang, L. Zhou, H. Jiang, Z. Tang, Y. Wang, H. Bao, and G. Zhang, \u201cMobile3drecon: real-time monocular 3d reconstruction on a mobile phone,\u201dIEEE Transactions on Visualization and Computer Graphics , vol. 26, no. 12, pp. 3446\u20133456, 2020.",
        "ref_ids": [
          "100"
        ],
        "1": "Despite in the latest years some works were able to achieve it [48, 100], important compromises on the image resolutions, maximum scene size and reconstruction accuracy are still necessary.",
        "2": "[100] X."
      },
      "\u0421\u0440\u0430\u0432\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432 \u0441\u0431\u043e\u0440\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u0442\u0440\u0435\u0445\u043c\u0435\u0440\u043d\u043e\u0439 \u0440\u0435\u043a\u043e\u043d\u0441\u0442\u0440\u0443\u043a\u0446\u0438\u0438": {
        "authors": [
          "\u0420\u041f \u0428\u0435\u0441\u0442\u043e\u043f\u0430\u043b\u043e\u0432",
          "\u041c\u041c \u0417\u0430\u0441\u043b\u0430\u0432\u0441\u043a\u0438\u0439"
        ],
        "url": "https://etu.ru/assets/files/Faculty-FKTI/MO/sbornik-2022-moevm.pdf#page=16",
        "ref_texts": ""
      },
      "On-Device 3D Foot Reconstruction for Digital Sizing": {
        "authors": [
          "N Hassan"
        ],
        "url": "https://tspace.library.utoronto.ca/bitstream/1807/129715/2/Hassan_Najah_202211_MAS_thesis.pdf",
        "ref_texts": ""
      },
      "Isovist computation of outdoor environment with semi-dense line SLAM and monocular camera": {
        "authors": [
          "T Le Jan",
          "M Servi\u00e8res",
          "T Leduc",
          "V Tourre"
        ],
        "url": "https://hal.science/hal-03368466/document",
        "ref_texts": ""
      }
    }
  }
]